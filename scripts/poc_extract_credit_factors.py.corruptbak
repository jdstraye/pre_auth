#!/usr/bin/env python3import csv, re, os, sysfrom pathlib import Pathtry:
    import boto3except Exception:
    boto3 = Nonetry:
    import fitzexcept Exception:
    raise ImportError('PyMuPDF (fitz) is required for `poc_extract_credit_factors`. Install with: pip install pymupdf')
HAS_FITZ = Trueimport subprocessfrom PIL import Image, ImageDrawtry:
    import pytesseractHAS_PYTESSERACT = Trueexcept Exception:
    pytesseract = NoneHAS_PYTESSERACT = Falseimport numpy as npfrom collections import deque

# Markers are deprecated in the primary flow. Marker/vector heuristics remain in helpers
# for analysis but are NOT used by default. This module uses a deterministic span -> conv -> glyph
# path to classify snippet color.
def markers_enabled():
    return False


# small helper: connected components without external depsdef _connected_components(mask, min_area=8, max_area=50000):
    """Yield (x0,y0,x1,y1, coords_list) for connected True regions in boolean mask."""
    h, w = mask.shapevisited = np.zeros_like(mask, dtype=bool)
    comps = []
    for y in range(h):
        for x in range(w):
            if not mask[y, x] or visited[y, x]:
                continueq = deque()
            q.append((x, y))
            visited[y, x] = Truepts = []
            while q:
                px, py = q.popleft()
                pts.append((px, py))
                for dx, dy in ((1,0),(-1,0),(0,1),(0,-1)):
                    nx, ny = px+dx, py+dyif 0 <= nx < w and 0 <= ny < h and mask[ny, nx] and not visited[ny, nx]:
                        visited[ny, nx] = Trueq.append((nx, ny))
            if len(pts) >= min_area and len(pts) <= max_area:
                xs = [p[0] for p in pts]; ys = [p[1] for p in pts]
                comps.append((min(xs), min(ys), max(xs)+1, max(ys)+1, pts))
    return comps


def parse_count_amount_pair(s):
    """Return (count, amount) from strings like '10 / $56,881' or '$56,881 / 10'.
    Returns (None, None) if no match. Does not treat '/ mo' as a count/amount pair.

    This function looks for explicit token pairs around a slash and decides which sideis count vs amount using presence of '$' or magnitude heuristics (counts usually <= 500).
    """
    if not s or '/' not in s:
        return (None, None)
    # normalize whitespace and slashess_norm = s.replace('\u00a0', ' ').replace('\u2215', '/').replace('\u2044', '/')
    # ignore common unit patterns like '/ mo' or '/ yr'
    if re.search(r"/\s*(mo|month|yr|year)\b", s_norm, re.I):
        return (None, None)
    # find all token pairs around a slash of form token1 / token2pairs = re.findall(r"(\$?[0-9\,]+)\s*/\s*(\$?[0-9\,]+)", s_norm)
    if not pairs:
        return (None, None)
    for left, right in pairs:
        def norm_num(t):
            return int(t.replace('$','').replace(',',''))
        try:
            lnum = norm_num(left)
            rnum = norm_num(right)
        except Exception:
            continue
        # if either side has a dollar sign, prefer that as amountif '$' in left and '$' not in right:
            return (rnum, lnum)
        if '$' in right and '$' not in left:
            return (lnum, rnum)
        # otherwise use magnitude heuristic: amount usually larger (>1000) and count small (<=500)
        if lnum <= 500 and rnum > 1000:
            return (lnum, rnum)
        if rnum <= 500 and lnum > 1000:
            return (rnum, lnum)
        # fallback: if left token has no comma but right has comma, prefer right as amountif ',' in right and ',' not in left:
            return (lnum, rnum)
        if ',' in left and ',' not in right:
            return (rnum, lnum)
        # otherwise, if left looks like an integer small and right similarly, assume left=countif lnum <= 500:
            return (lnum, rnum)
        if rnum <= 500:
            return (rnum, lnum)
        # final fallback: pick the pair and treat left as countreturn (lnum, rnum)
    return (None, None)




def ocr_regions_on_page(page, regions, scale=3, ocr_lang=None):
    """For a page and regions (pix bboxes), run tesseract on each crop and return list of dicts: text, tokens, pix_bbox, page_bbox, rgb, hex."""
    results = []
    try:
        rect = page.rectpix = page.get_pixmap(matrix=fitz.Matrix(scale, scale), alpha=False)
    except Exception:
        return resultsimg = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    for r in regions:
        x0,y0,x1,y1 = r['pix_bbox']
        # add paddingpad = 6cx0 = max(0, x0 - pad); cy0 = max(0, y0 - pad)
        cx1 = min(img.size[0], x1 + pad); cy1 = min(img.size[1], y1 + pad)
        crop = img.crop((cx0, cy0, cx1, cy1))
        try:
            data = pytesseract.image_to_data(crop, output_type=pytesseract.Output.DICT, lang=ocr_lang) if pytesseract else Nonetokens = []
            if data:
                for i, t in enumerate(data['text']):
                    if not t or not t.strip():
                        continuelx = cx0 + data['left'][i]; ty = cy0 + data['top'][i]
                    w = data['width'][i]; hgt = data['height'][i]
                    page_scale_x = rect.width / pix.widthpage_scale_y = rect.height / pix.height
                    # map token bbox back to page coordstok_page_bbox = (rect.x0 + lx / (pix.width / rect.width), rect.y0 + ty / (pix.height / rect.height), rect.x0 + (lx+w) / (pix.width / rect.width), rect.y0 + (ty+hgt) / (pix.height / rect.height))
                    tokens.append({'text': t.strip(), 'pix_bbox':(lx,ty,lx+w,ty+hgt), 'page_bbox': tok_page_bbox})
            full_text = pytesseract.image_to_string(crop, lang=ocr_lang).strip() if pytesseract else ''
        except Exception:
            tokens = []
            full_text = ''
        res = {'text': full_text, 'tokens': tokens, 'pix_bbox': r['pix_bbox'], 'page_bbox': r['page_bbox'], 'rgb': r['rgb'], 'hex': r['hex'], 'area': r['area']}
        results.append(res)
    return results

ROOT = Path(__file__).resolve().parents[1]
SCORES = ROOT / "data" / "pdf_scores.csv"
OUT_ROWS = ROOT / "data" / "credit_factors_poc_rows.csv"
OUT_WIDE = ROOT / "data" / "credit_factors_poc_wide.csv"
PDF_DIR = ROOT / "data" / "poc_pdfs"
IMG_DIR = ROOT / "data" / "poc_imgs"
PDF_DIR.mkdir(parents=True, exist_ok=True)
IMG_DIR.mkdir(parents=True, exist_ok=True)

CANONICAL = {
    'red': (204,0,0),
    'amber': (255,153,0),
    'green': (44,160,44),
}
# Allow optional override of canonicals from human-labeled annotationstry:
    import jsoncanon_path = ROOT / 'data' / 'label_canonicals.json'
    if canon_path.exists():
        _c = json.loads(canon_path.read_text())
        for k,v in _c.items():
            if isinstance(v, (list, tuple)) and len(v) == 3:
                CANONICAL[k] = tuple(v)
except Exception:
    pass

# Supplemental canonical variants (allow users to propose additional canonical colors)
# Example: user provided two green examples (#9FC6A5 and #499354)
_EXTRA_CANONICALS = {
    'green': [(159, 198, 165), (73, 147, 84)],
}
# If label-driven canonicals exist, prefer them over hardcoded extrastry:
    lab_path = ROOT / 'data' / 'label_canonicals_filtered.json'
    if lab_path.exists():
        _lc = json.loads(lab_path.read_text())
        for k,v in _lc.items():
            if isinstance(v, (list, tuple)) and len(v) == 3:
                # ensure we add the filtered canonical as the primary canonicalCANONICAL[k] = tuple(v)
except Exception:
    pass

# Tunable parameters for candidate scoring and classifier behavior (defined early to be available everywhere)
CANDIDATE_OVERLAP_FRAC = 0.66UNIQ_SCORE_MULTIPLIER = 8NUM_MATCH_WEIGHT = 140AMBIGUITY_MARGIN = 4CONV_PEAK_THRESH_DEFAULT = 12CONV_RATIO_THRESH_DEFAULT = 1.25
# Set black threshold to include darker greys commonly used as 'black' text in PDFsSPAN_BLACK_V_THRESH = 0.35
# Minimum saturation threshold for considering colored categoriesSPAN_SAT_MIN = 0.05
# Weight for n-gram (2-gram) matches between phrase and candidate lineNGRAM_WEIGHT = 10


def rgb_to_hex_tuple(rgb):
    try:
        r,g,b = rgbreturn f"#{int(r):02x}{int(g):02x}{int(b):02x}"
    except Exception:
        return None


def color_distance(c1, c2):
    return sum((a-b)**2 for a,b in zip(c1,c2))


import colorsys

def map_rgb_to_category(rgb):
    """Map an RGB tuple to a category using HSV heuristics and canonical distances.
    Returns one of: 'red', 'green', 'black', 'neutral'."""
    if not rgb:
        return 'neutral'
    r,g,b = [x/255.0 for x in rgb]
    h,s,v = colorsys.rgb_to_hsv(r,g,b)
    hue = h * 360
    # detect near-black text/line markers (stricter threshold)
    if v < SPAN_BLACK_V_THRESH and s < 0.25:
        return 'black'
    # if fairly dark overall, treat as black (covers low-value non-saturated text)
    if v < SPAN_BLACK_V_THRESH or (s < SPAN_SAT_MIN and v < 0.6):
        return 'black'
    # colored markers may be very pale; accept lower saturation tintsif s >= SPAN_SAT_MIN:
        # green rangeif 60 <= hue <= 170:
            return 'green'
        # amber/orange -> map to nearest canonical (prefer red)
        if 30 < hue < 60:
            # approximate as red (no amber category per policy)
            return 'red'
        # red range (wraparound)
        if hue <= 30 or hue >= 330:
            return 'red'
        return 'neutral'
    # fallback to distance to canonical colors with a looser threshold to accept pale tintsbest, bestd = None, Nonefor name, canon in CANONICAL.items():
        d = color_distance(rgb, canon)
        if best is None or d < bestd:
            best, bestd = name, d
    # if distance is reasonably small, treat as that canonicalif bestd is not None and bestd < 70000:
        return best
    # allow looser match when saturation is slightly higher (>0.01) and reasonably closeif s >= 0.01 and bestd is not None and bestd < 120000:
        return best
    # another heuristic: if pixel is very light but has slight green bias, call greenif v > 0.85:
        r2,g2,b2 = rgbif (g2 - r2) > 8 and (g2 - b2) > 8:
            return 'green'
    return 'neutral'


def map_color_to_cat(rgb):
    # wrapper for backward compat/consistencyreturn map_rgb_to_category(rgb)


def download_source(src, dest):
    if src.startswith('s3://'):
        if boto3 is None:
            raise RuntimeError('boto3 not available in this environment; cannot download from s3://')
        m = re.match(r'^s3://([^/]+)/(.+)$', src)
        boto3.client('s3').download_file(m.group(1), m.group(2), str(dest))
    else:
        import requestsr = requests.get(src, timeout=30)
        r.raise_for_status()
        dest.write_bytes(r.content)


def find_credit_factors_region(doc, anchor='Credit Factors', max_pages=3):
    """
    Find the page index where the 'Credit Factors' section starts and return a list of
    (page_index, blocks_to_scan) tuples covering the region. It will scan up to max_pagesfrom the start and stop early if it sees likely section terminators.
    """
    start = Nonefor i, page in enumerate(doc):
        if anchor in page.get_text():
            start = ibreakif start is None:
        start = 0regions = []
    stop_markers = ['Accounts', 'Account', 'Credit Report Details', 'Balances', 'Trade Lines']
    for p in range(start, min(start + max_pages, len(doc))):
        td = doc.load_page(p).get_text('dict')
        blocks = td.get('blocks', [])
        # heuristics: include consecutive blocks until we see a stop markerblocks_to_scan = []
        stop_here = Falsefor b in blocks:
            bt = '\n'.join([''.join([s.get('text','') for s in l.get('spans',[])]) for l in b.get('lines',[])])
            blocks_to_scan.append(b)
            for sm in stop_markers:
                if sm in bt and sm != anchor:
                    stop_here = Truebreakif stop_here:
                breakregions.append(blocks_to_scan)
        if stop_here:
            break
    # return list with start page and region blocks per pagereturn list(enumerate(regions, start=start))


def extract_lines_from_region(doc, start_page_blocks):
    """Given a list of page blocks for the region, extract ordered text lines and corresponding spans.
    Returns list of (page_index, line_text, spans_for_line)
    """
    lines = []
    for page_index, blocks in start_page_blocks:
        for b in blocks:
            for ln in b.get('lines', []):
                text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                if text:
                    lines.append((page_index, text, ln.get('spans', [])))
    return lines


def span_color_hex(spans):
    """Return (hex, rgb) derived from explicit span color attributes only.

    Prefer any individual span whose color maps to a non-neutral canonical (red/green/amber).
    If none present, fall back to the mean color across colored spans but still respectlow saturation/value thresholds to avoid treating black/white as colored.
    """
    colors = []
    colored = []
    span_rgb_pairs = []  # (rgb, span)
    for s in spans:
        col = s.get('color')
        if col:
            # convert various color encodings into 0-255 RGB tuplergb = Noneif isinstance(col, (tuple, list)):
                rgb = tuple(int(255*v) if isinstance(v, float) and v <= 1 else int(v) for v in col)
            elif isinstance(col, int):
                # PyMuPDF may return a single packed integer (0xRRGGBB)
                try:
                    rgb = ((col >> 16) & 255, (col >> 8) & 255, col & 255)
                except Exception:
                    rgb = Noneelse:
                rgb = Noneif rgb and rgb != (0, 0, 0):
                colors.append(rgb)
                span_rgb_pairs.append((rgb, s))
                cat = map_color_to_cat(rgb)
                if cat in ('green', 'red', 'amber'):
                    colored.append(rgb)
    if not colors:
        return None, None

    # If any colored tokens exist, average only thoseif colored:
        arr = np.array(colored)
        med = tuple(map(int, arr.mean(axis=0)))
    else:
        arr = np.array(colors)
        med = tuple(map(int, arr.mean(axis=0)))

    def saturation(rgb):
        r, g, b = [x / 255.0 for x in rgb]
        mx = max(r, g, b); mn = min(r, g, b)
        if mx == 0: return 0s = (mx - mn) / mxreturn s

    # Treat near-white as missingif sum(med) > 740:
        return None, med

    # Allow lower saturation acceptance for span-driven colors (pale tints can be meaningful)
    if saturation(med) < 0.01:
        return None, med

    # If very dark (near-black), treat as neutral (return None to allow convolution fallback)
    h, s, v = colorsys.rgb_to_hsv(med[0] / 255.0, med[1] / 255.0, med[2] / 255.0)
    if v < 0.12:
        return None, med

    return rgb_to_hex_tuple(med), med










def detect_table_columns(page, min_count=2, bucket=12):
    """Detect likely table column x-centers from drawing rects on the page.
    Returns list of page-space x centers (floats)."""
    draws = page.get_drawings()
    centers = []
    for d in draws:
        rect = d.get('rect')
        if not rect:
            continuew = rect.x1 - rect.x0h = rect.y1 - rect.y0
        # skip very small or very large rectsif w < 6 or h < 6 or w > 800 or h > 400:
            continuecenters.append((rect.x0 + rect.x1) / 2.0)
    if not centers:
        return []
    # bucket centers into approximate column positionsbuckets = {}
    for c in centers:
        key = int(c // bucket)
        buckets.setdefault(key, []).append(c)
    cols = []
    for k, vals in buckets.items():
        if len(vals) >= min_count:
            cols.append(sum(vals)/len(vals))
    # sortcols.sort()
    return cols


def _annotate_and_save(img, sample_points, chosen_point, out_path):
    """Draw sample points (x,y,color,label) on img and save to out_path"""
    try:
        draw = ImageDraw.Draw(img)
        # points: list of dicts {'x':int,'y':int,'rgb':(r,g,b),'label':str}
        for pt in sample_points:
            x,y = int(pt['x']), int(pt['y'])
            color = pt.get('annot_color', (255,0,0))
            draw.ellipse((x-6,y-6,x+6,y+6), outline=color, width=2)
            if 'label' in pt:
                draw.text((x+8,y-8), str(pt['label']), fill=color)
        if chosen_point is not None:
            x,y = int(chosen_point['x']), int(chosen_point['y'])
            draw.ellipse((x-8,y-8,x+8,y+8), outline=(0,255,0), width=3)
        img.save(out_path)
    except Exception:
        pass


def raster_sample_color_near_line(page, line_bbox, zoom=4, debug_prefix=None, expected_color=None):
    """DEPRECATED: raster-based sampling disabled. Span-only policy enforced.

    This function is retained as a no-op stub for backward compatibility and willalways return (None, None).
    """
    return None, None


def _median_around(img_arr, cx, cy, size=5):
    h, w, _ = img_arr.shapehalf = size // 2x0 = max(0, int(cx - half))
    y0 = max(0, int(cy - half))
    x1 = min(w, int(cx + half + 1))
    y1 = min(h, int(cy + half + 1))
    patch = img_arr[y0:y1, x0:x1]
    if patch.size == 0:
        return Nonemed = tuple(map(int, np.median(patch.reshape(-1,3), axis=0)))
    return med


def median_5x5(img, cx, cy):
    # Return median RGB of a 5x5 square centered at (cx, cy)
    w,h = img.sizex0 = max(0, cx-2)
    y0 = max(0, cy-2)
    x1 = min(w, cx+3)
    y1 = min(h, cy+3)
    crop = img.crop((x0, y0, x1, y1))
    arr = np.array(crop).reshape(-1,3)
    if arr.size == 0:
        return Nonemed = tuple(map(int, np.median(arr, axis=0)))
    return med


def _local_left_strip_conv_search(page, phrase, expected_color=None, scale=8, strip_w_px=30, conv_kernel=7, conv_threshold=20):
    """Search within a narrow vertical strip left of each page line that matches phrase.
    Returns (page_index, text, hex, rgb, line_page_bbox, pix_bbox) or None."""
    try:
        rect = page.rectpix = page.get_pixmap(matrix=fitz.Matrix(scale, scale), alpha=False)
    except Exception:
        return Noneimg_arr = np.array(Image.frombytes('RGB', [pix.width, pix.height], pix.samples)).astype(int)
    td = page.get_text('dict')
    # build canonical targetstarget_list = []
    primary = CANONICAL.get(expected_color) if expected_color else CANONICAL.get('green')
    if primary is not None:
        target_list.append(primary)
    target_list.extend(_EXTRA_CANONICALS.get(expected_color or 'green', []))
    target_arr = np.array(target_list) if target_list else np.zeros((0,3), dtype=int)
    s_x = pix.width / rect.widths_y = pix.height / rect.heightfor b in td.get('blocks', []):
        for ln in b.get('lines', []):
            line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
            if not line_text:
                continueif phrase.lower() not in line_text.lower() and not any(tok.lower() in phrase.lower() or phrase.lower() in tok.lower() for tok in line_text.split()):
                continuelx0, ly0, lx1, ly1 = ln.get('bbox')
            # convert to pixel coordspx0 = max(0, int((lx0 - rect.x0) * s_x))
            py0 = max(0, int((ly0 - rect.y0) * s_y))
            px1 = min(pix.width, int((lx1 - rect.x0) * s_x))
            py1 = min(pix.height, int((ly1 - rect.y0) * s_y))
            # define a narrow strip left of the linestrip_w = max(8, int(strip_w_px * (s_x/3.0)))
            sx0 = max(0, px0 - strip_w)
            sx1 = max(0, px0)
            sy0 = max(0, py0 - strip_w)
            sy1 = min(pix.height, py1 + strip_w)
            if sx1 <= sx0 or sy1 <= sy0:
                continuecrop = img_arr[sy0:sy1, sx0:sx1, :]
            if crop.size == 0:
                continue
            # compute greenness mapg = crop[:,:,1].astype(int); r = crop[:,:,0].astype(int); b = crop[:,:,2].astype(int)
            greenness = (2*g - r - b)
            greenness = np.maximum(greenness, 0).astype(float)
            try:
                import scipy.ndimage as _ndsm = _nd.uniform_filter(greenness, size=conv_kernel)
            except Exception:
                # fallback simple mean filtersm = greenness
            # try greenness peak firstmaxv = sm.max()
            found_median = Noneif maxv >= conv_threshold:
                pts = np.argwhere(sm == maxv)
                yc, xc = pts[0]
                # sample a small neighborhood around (xc,yc)
                xx0 = max(0, xc-3); yy0 = max(0, yc-3); xx1 = min(crop.shape[1], xc+4); yy1 = min(crop.shape[0], yc+4)
                sample = crop[yy0:yy1, xx0:xx1, :].reshape(-1,3)
                # prefer nonwhite pixelssample_sel = sample[sample.sum(axis=1) < 740]
                if sample_sel.size == 0:
                    sample_sel = samplemedian = tuple(map(int, np.median(sample_sel, axis=0)))
                if map_color_to_cat(median) == expected_color:
                    found_median = median
            # if greenness peak didn't find anything, try canonical-proximity seedingif found_median is None and target_arr.size:
                flat = crop.reshape(-1,3)
                # compute min distance to any canonical per pixeld = np.sum((flat[:, None, :] - target_arr[None, :, :])**2, axis=2)
                min_d = d.min(axis=1)
                best_idx = np.argmin(min_d)
                best_d = min_d[best_idx]
                # allow a reasonably loose threshold for pale/anti-aliased markersif best_d < 120000:
                    py = best_idx // crop.shape[1]
                    px = best_idx % crop.shape[1]
                    xx0 = max(0, px-3); yy0 = max(0, py-3); xx1 = min(crop.shape[1], px+4); yy1 = min(crop.shape[0], py+4)
                    sample = crop[yy0:yy1, xx0:xx1, :].reshape(-1,3)
                    sample_sel = sample[sample.sum(axis=1) < 740]
                    if sample_sel.size == 0:
                        sample_sel = samplemedian = tuple(map(int, np.median(sample_sel, axis=0)))
                    if map_color_to_cat(median) == expected_color:
                        found_median = medianif found_median is not None:
                page_x0 = rect.x0 + (sx0 / s_x); page_y0 = rect.y0 + (sy0 / s_y); page_x1 = rect.x0 + (sx1 / s_x); page_y1 = rect.y0 + (sy1 / s_y)
                return 0, line_text, rgb_to_hex_tuple(found_median), found_median, (page_x0, page_y0, page_x1, page_y1), (sx0,sy0,sx1,sy1)
    return None


def color_first_search_for_phrase(pdf_doc, phrase, expected_color=None, page_limit=None):
    """Search pages for colored regions matching expected_color, OCR them, and attempt to find phrase text inside those regions.
    Modes can be controlled via env var `POC_MARKER_MODE`:
      - "" (default) : current mixed behavior (vector-first integrated with color masks)
      - "color_first_only" : search color masks/OCR first and don't consult vectors
      - "vector_first_only" : only consult vector markers (fast, deterministic when present)
      - "color_then_vector" : try color masks first, then vectors as fallbackReturns (page_index, text, hex, rgb, page_bbox, pix_bbox, uncertain) or None if not found. """
    mode = os.environ.get('POC_MARKER_MODE', '').lower()
    # helper: stricter line matching to avoid false substring matches (e.g., '6 Charged Off' vs '8 Charged Off Rev')
    def _is_line_match(phrase, line_text):
        def _tok_list(s):
            return [t for t in ''.join(ch.lower() if ch.isalnum() or ch.isspace() else ' ' for ch in s).split()]
        p_toks = _tok_list(phrase)
        l_toks = _tok_list(line_text)
        if not p_toks or not l_toks:
            return False
        # if phrase contains numeric tokens, require numeric matchnums = [t for t in p_toks if t.isdigit()]
        if nums and not any(n in l_toks for n in nums):
            return Falsep_use = [t for t in p_toks if len(t) > 2 or t.isdigit()]
        if not p_use:
            return Falseoverlap = len(set(p_use) & set(l_toks))
        # require at least half of non-trivial phrase tokens to match (min 1)
        return overlap >= max(1, int(len(p_use) * 0.5))

    global_candidates = []
    for p in range(len(pdf_doc)):
        if page_limit is not None and p >= page_limit:
            breakpage = pdf_doc.load_page(p)
        # pick color to search: if expected_color provided, use that, otherwise try green/red/ambercolors = [expected_color] if expected_color else ['green','red','amber']
        for c in colors:
            # Deterministic span -> conv classifier -> glyph fallback pathtry:
                td = page.get_text('dict')
                norm_phrase = ' '.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
                canonical_phrase = map_line_to_canonical(phrase)
                global_candidates = []
                # collect candidates from all blocks on the pagefor b in td.get('blocks', []):
                    # block-level heuristic: if phrase relates to revolving accounts + balances,
                    # prefer the line inside the block that contains 'revolv' and 'balanc' tokensblock_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b.get('lines', [])]).lower()
                    if ('rev' in norm_phrase or 'revolv' in norm_phrase or 'rev_accts' in (canonical_phrase or '')) and ('balanc' in block_text):
                        # find the line with 'revolv' tokenfor ln in b.get('lines', []):
                            line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                            if 'revolv' in line_text.lower() or 'revolving' in line_text.lower():
                                p_use = [t for t in norm_phrase.split() if len(t) > 2 or t.isdigit()]
                                l_use = ' '.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip().split()
                                overlap = len(set(p_use) & set(l_use))
                                uniq_score = sum(len(t) for t in p_use if len(t) > 4 and t in l_use)
                                nums = [t for t in p_use if t.isdigit()]
                                num_match = 1 if nums and any(n in l_use for n in nums) else 0missing_penalty = sum(len(t) for t in p_use if len(t) > 4 and t not in l_use) * 2
                                # n-gram overlap (bigrams) to prefer multi-token matchesp_bigrams = [ ' '.join(p_use[i:i+2]) for i in range(max(0, len(p_use)-1)) ]
                                l_bigrams = [ ' '.join(l_use[i:i+2]) for i in range(max(0, len(l_use)-1)) ]
                                ngram_overlap = len(set(p_bigrams) & set(l_bigrams))
                                score = 9000 + (overlap * 10) + (uniq_score * UNIQ_SCORE_MULTIPLIER) + (num_match * NUM_MATCH_WEIGHT) + (ngram_overlap * NGRAM_WEIGHT) - missing_penaltyglobal_candidates.append((score, overlap, uniq_score, line_text, ln, p))
                                breakfor ln in b.get('lines', []):
                        line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                        if not line_text:
                            continuenorm_line = ' '.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        canonical_line = map_line_to_canonical(line_text)
                        # exact normalized match short-circuit: prefer span color if presentif norm_phrase and norm_line and norm_phrase == norm_line:
                            try:
                                hexv_s, rgb_s = span_color_hex(ln.get('spans', []))
                                if rgb_s is not None and map_color_to_cat(rgb_s) != 'neutral' and (expected_color is None or map_color_to_cat(rgb_s) == expected_color):
                                    return p, line_text, rgb_to_hex_tuple(rgb_s), rgb_s, ln.get('bbox'), None, Falseexcept Exception:
                                passp_use = [t for t in norm_phrase.split() if len(t) > 2 or t.isdigit()]
                            l_use = norm_line.split()
                            overlap = len(set(p_use) & set(l_use))
                            uniq_score = sum(len(t) for t in p_use if len(t) > 4 and t in l_use)
                            nums = [t for t in p_use if t.isdigit()]
                            num_match = 1 if nums and any(n in l_use for n in nums) else 0missing_penalty = sum(len(t) for t in p_use if len(t) > 4 and t not in l_use) * 2p_bigrams = [ ' '.join(p_use[i:i+2]) for i in range(max(0, len(p_use)-1)) ]
                            l_bigrams = [ ' '.join(l_use[i:i+2]) for i in range(max(0, len(l_use)-1)) ]
                            ngram_overlap = len(set(p_bigrams) & set(l_bigrams))
                            score = 9999 + (overlap * 10) + (uniq_score * UNIQ_SCORE_MULTIPLIER) + (num_match * NUM_MATCH_WEIGHT) + (ngram_overlap * NGRAM_WEIGHT) - missing_penaltyglobal_candidates.append((score, overlap, uniq_score, line_text, ln, p))
                            continue
                        # otherwise, use the token-aware fast filterif _is_line_match(phrase, line_text):
                            # special-case: if phrase is about inquiries, prefer lines that contain 'inq' or canonical 'inquiry'
                            if ('inq' in norm_phrase.split() or (canonical_phrase and 'inquiry' in canonical_phrase)) and not (('inq' in norm_line.split()) or (canonical_line == 'inquiry')):
                                # skip lines that are not about inquiriescontinuep_use = [t for t in norm_phrase.split() if len(t) > 2 or t.isdigit()]
                            l_use = norm_line.split()
                            overlap = len(set(p_use) & set(l_use))
                            uniq_score = sum(len(t) for t in p_use if len(t) > 4 and t in l_use)
                            nums = [t for t in p_use if t.isdigit()]
                            num_match = 1 if nums and any(n in l_use for n in nums) else 0missing_penalty = sum(len(t) for t in p_use if len(t) > 4 and t not in l_use) * 2p_bigrams = [ ' '.join(p_use[i:i+2]) for i in range(max(0, len(p_use)-1)) ]
                            l_bigrams = [ ' '.join(l_use[i:i+2]) for i in range(max(0, len(l_use)-1)) ]
                            ngram_overlap = len(set(p_bigrams) & set(l_bigrams))
                            score = (overlap * 10) + (uniq_score * UNIQ_SCORE_MULTIPLIER) + (num_match * NUM_MATCH_WEIGHT) + (ngram_overlap * NGRAM_WEIGHT) - missing_penaltyglobal_candidates.append((score, overlap, uniq_score, line_text, ln, p))

                    # 3) glyph interior sampling fallbacktry:
                        hexv_g, rgb_g = sample_glyph_interior_color_near_line(page, best_ln.get('bbox'), zoom=6, gray_pctile=90, erode_iter=1)
                        if rgb_g is not None:
                            return best_p, best_text, rgb_to_hex_tuple(rgb_g), rgb_g, best_ln.get('bbox'), None, uncertainexcept Exception:
                        passexcept Exception:
                pass

            # After scanning all pages/blocks, pick best candidate across pages (if any)
            if global_candidates:
                global_candidates.sort(reverse=True, key=lambda x: x[0])
                best_score,_,_,best_text,best_ln,best_p = global_candidates[0]
                second_score = global_candidates[1][0] if len(global_candidates) > 1 else Noneuncertain = Falseif second_score is not None and (best_score - second_score) < AMBIGUITY_MARGIN:
                    uncertain = True
                    # Tie-breaker: sample glyph interior for best and second candidate and prefer the one matching expected_colortry:
                        second = global_candidates[1]
                        sec_score,_,_,sec_text,sec_ln,sec_p = second
                        # sample glyph for bestbp = pdf_doc.load_page(best_p)
                        b_hex, b_rgb = sample_glyph_interior_color_near_line(bp, best_ln.get('bbox'), zoom=8, gray_pctile=90, erode_iter=1)
                        sp = pdf_doc.load_page(sec_p)
                        s_hex, s_rgb = sample_glyph_interior_color_near_line(sp, sec_ln.get('bbox'), zoom=8, gray_pctile=90, erode_iter=1)
                        b_match = Falses_match = Falseif b_rgb is not None and expected_color:
                            try:
                                if map_color_to_cat(b_rgb) == expected_color or color_distance(b_rgb, CANONICAL.get(expected_color, (0,0,0))) < 120000:
                                    b_match = Trueexcept Exception:
                                passif s_rgb is not None and expected_color:
                            try:
                                if map_color_to_cat(s_rgb) == expected_color or color_distance(s_rgb, CANONICAL.get(expected_color, (0,0,0))) < 120000:
                                    s_match = Trueexcept Exception:
                                passif b_match and not s_match:
                            uncertain = Falseelif s_match and not b_match:
                            # prefer second candidatebest_score,_,_,best_text,best_ln,best_p = sec_score,sec_score,_,sec_text,sec_ln,sec_puncertain = Falseexcept Exception:
                        passtry:
                    # span-first acceptance for best candidatetarget_page = pdf_doc.load_page(best_p)
                    print(f"DEBUG: best_global_candidate score={best_score} text='{best_text}' page={best_p} uncertain={uncertain}")
                    hexv_s, rgb_s = span_color_hex(best_ln.get('spans', []))
                    print(f"DEBUG: span_color -> {hexv_s}, {rgb_s}")
                    if rgb_s is not None and map_color_to_cat(rgb_s) != 'neutral':
                        if expected_color is None or map_color_to_cat(rgb_s) == expected_color:
                            return best_p, best_text, rgb_to_hex_tuple(rgb_s), rgb_s, best_ln.get('bbox'), None, uncertainexcept Exception:
                    passtry:
                    debug_prefix = f"{pdf_doc.name}_p{best_p}_{re.sub('[^a-z0-9]','_',best_text.lower())[:40]}"
                    cls = classify_snippet_color_convolution(target_page, best_ln.get('bbox'), zoom=8, kernel=7, peak_thresh=CONV_PEAK_THRESH_DEFAULT, ratio_thresh=CONV_RATIO_THRESH_DEFAULT, debug_prefix=debug_prefix)
                    if cls and cls.get('cat') in ('green','red','black'):
                        final_uncertain = uncertain or bool(cls.get('uncertain'))
                        # when an expected_color is provided, prefer matches to that colorif cls.get('uncertain'):
                            try:
                                hexv_g, rgb_g = sample_glyph_interior_color_near_line(target_page, best_ln.get('bbox'), zoom=8, gray_pctile=90, erode_iter=1)
                                if rgb_g is not None:
                                    cat_g = map_color_to_cat(rgb_g)
                                    if (expected_color is not None and cat_g == expected_color) or (expected_color == 'black' and cat_g == 'black'):
                                        return best_p, best_text, rgb_to_hex_tuple(rgb_g), rgb_g, best_ln.get('bbox'), None, Trueexcept Exception:
                                passelse:
                            # Non-uncertain classifier: accept only if it matches expected_color when providedif expected_color is None or cls.get('cat') == expected_color:
                                return best_p, best_text, cls.get('hex'), cls.get('rgb'), best_ln.get('bbox'), None, final_uncertain
                            # otherwise skip this classifier result and continue (do not accept conflicting color)
                except Exception:
                    pass

            # if mode == 'color_then_vector', try vectors as a fallback when masks didn't find anythingif mode == 'color_then_vector':
                try:
                    vecs = extract_vector_markers(page)
                    for v in vecs:
                        px0, py0, px1, py1 = v['rect'].x0, v['rect'].y0, v['rect'].x1, v['rect'].y1td = page.get_text('dict')
                        for b in td.get('blocks', []):
                            for ln in b.get('lines', []):
                                lx0, ly0, lx1, ly1 = ln.get('bbox')
                                vert_overlap = not (ly1 < py0 or ly0 > py1)
                                is_right = lx0 > px1 - 0.5if vert_overlap and is_right:
                                    line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                                    if not line_text:
                                        continueif phrase.lower() in line_text.lower() or any(tok.lower() in phrase.lower() or phrase.lower() in tok.lower() for tok in line_text.split()):
                                        return p, line_text, v.get('hex'), v.get('rgb'), ln.get('bbox'), v.get('rect')
                except Exception:
                    passreturn None


def band_scan_color_for_phrase(pdf_doc, phrase, out_img_path=None):
    """Scan all pages for phrase and sample horizontal band using 5x5 medians to get a color."""
    for p in range(len(pdf_doc)):
        page = pdf_doc.load_page(p)
        text = page.get_text()
        if phrase.lower() in text.lower():
            pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
            img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
            data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
            target = ''.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
            best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                t = (txt or '').strip()
                if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                if not norm: continuescore = len(set(norm.split()) & set(target.split()))
                if score > best_score:
                    best_score = scorebest_y = data['top'][i]
            w,h = img.sizeif best_y is None:
                band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                arr = np.array(band).reshape(-1,3)
                nonwhite = arr[(arr.sum(axis=1) < 740)]
                if nonwhite.size:
                    med = tuple(map(int, np.median(nonwhite, axis=0)))
                    if out_img_path:
                        band.save(out_img_path)
                    return medelse:
                cx = int(w * 0.12)
                cy = int(best_y + 8)
                med = median_5x5(img, cx, cy)
                if med:
                    if out_img_path:
                        small = img.crop((max(0,cx-20), max(0,cy-20), min(w, cx+20), min(h, cy+20)))
                        small.save(out_img_path)
                    return medreturn None


def ocr_sample_color(page, line_text, img_path):
    pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
    img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    img.save(img_path)
    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    w_img,h_img = img.sizetarget = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
    # find best OCR token matchbest_idx = Nonebest_score = 0for i, txt in enumerate(data['text']):
        t = (txt or '').strip()
        if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
        score = len(set(norm.split()) & set(target.split()))
        if norm in target or target in norm:
            score += 2if score and score > best_score:
            best_score = scorebest_idx = i
    # if token found, sample 5x5 around token centerif best_idx is not None:
        x = int(data['left'][best_idx] + data['width'][best_idx] // 2)
        y = int(data['top'][best_idx] + data['height'][best_idx] // 2)
        med = median_5x5(img, x, y)
        if med is not None:
            return rgb_to_hex_tuple(med), med
    # fallback: vertical left strip center 5x5cx = int((0 + int(w_img*0.15)) / 2)
    cy = int(h_img * 0.4)
    medp = median_5x5(img, cx, cy)
    if medp:
        return rgb_to_hex_tuple(medp), medp
    # final fallback: median of small center cropsample_box = (int(w_img*0.05), int(h_img*0.2), int(w_img*0.25), int(h_img*0.6))
    crop = img.crop(sample_box)
    arr = np.array(crop)
    med = tuple(map(int, np.median(arr.reshape(-1,3), axis=0)))
    return rgb_to_hex_tuple(med), med


def _box_mean_via_uniform_or_integral(arr, size=7):
    """Return local mean of 2D array arr using scipy.ndimage.uniform_filter if available,
    otherwise use an integral-image separable box sum fallback."""
    try:
        import scipy.ndimage as _ndreturn _nd.uniform_filter(arr.astype(float), size=size, mode='constant')
    except Exception:
        h,w = arr.shapepad = size // 2
        # integral image approachrow_cumsum = arr.cumsum(axis=1)
        left_idx = np.maximum(np.arange(w) - pad - 1, 0)
        right_idx = np.minimum(np.arange(w) + pad, w-1)
        row_box = row_cumsum[:, right_idx] - (row_cumsum[:, left_idx] if pad > 0 else 0)
        col_cumsum = row_box.cumsum(axis=0)
        top_idx = np.maximum(np.arange(h) - pad - 1, 0)
        bot_idx = np.minimum(np.arange(h) + pad, h-1)
        full_box = col_cumsum[bot_idx, :] - (col_cumsum[top_idx, :] if pad > 0 else 0)
        area = (bot_idx - np.maximum(np.arange(h) - pad, 0) + 1)[:,None] * (right_idx - np.maximum(np.arange(w) - pad, 0) + 1)[None,:]
        return full_box / (area + 1e-6)


def classify_snippet_color_convolution(page, bbox, zoom=8, kernel=7, peak_thresh=10, ratio_thresh=1.2, debug_prefix=None):
    """Classify snippet color using small red/green convolution detectors.

    Returns dict: {'cat': 'green'|'red'|'neutral', 'rgb': (r,g,b)|None, 'hex': '#rrggbb'|None, 'green_peak':..., 'red_peak':...}
    """
    lx0, ly0, lx1, ly1 = bboxtry:
        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)
        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    except Exception:
        return {'cat':'neutral','rgb':None,'hex':None,'green_peak':0,'red_peak':0}
    sx0 = int(lx0 * zoom); sx1 = int(lx1 * zoom); sy0 = int(ly0 * zoom); sy1 = int(ly1 * zoom)
    margin = max(2, zoom // 2)
    sx0 = max(0, sx0 - margin); sx1 = min(img.width, sx1 + margin)
    sy0 = max(0, sy0 - margin); sy1 = min(img.height, sy1 + margin)
    if sx1 <= sx0 or sy1 <= sy0:
        return {'cat':'neutral','rgb':None,'hex':None,'green_peak':0,'red_peak':0}
    crop = img.crop((sx0, sy0, sx1, sy1))
    arr = np.array(crop).astype(int)
    # channel biased signalsr = arr[:,:,0]; g = arr[:,:,1]; b = arr[:,:,2]
    red_sig = np.maximum(r - np.maximum(g,b), 0)
    green_sig = np.maximum(g - np.maximum(r,b), 0)
    # compute local meansred_mean = _box_mean_via_uniform_or_integral(red_sig, size=kernel)
    green_mean = _box_mean_via_uniform_or_integral(green_sig, size=kernel)
    red_peak = float(np.max(red_mean))
    green_peak = float(np.max(green_mean))
    # debug heatmapsif debug_prefix:
        try:
            def _save_heatmap(m, name):
                vmax = max(1.0, np.percentile(m,99))
                hm = (np.clip(m, 0, vmax) / vmax * 255.0).astype('uint8')
                Image.fromarray(hm).save(IMG_DIR / f"{debug_prefix}_{name}.png")
            _save_heatmap(green_mean, 'green_hm')
            _save_heatmap(red_mean, 'red_hm')
        except Exception:
            pass
    # decidecat = 'neutral'
    rgb = Nonehexv = Noneratio = (green_peak / (red_peak + 1e-6)) if red_peak > 0 else (green_peak / (1e-6))
    uncertain = Falseif green_peak > peak_thresh and green_peak >= ratio_thresh * (red_peak + 1e-6):
        # pick pixels with strong green signalmask = green_sig >= (0.5 * green_peak)
        sel = arr[mask]
        if sel.size:
            sel = sel[sel.sum(axis=1) < 740]
            if sel.size == 0:
                sel = arr[mask]
            med = tuple(map(int, np.median(sel, axis=0)))
            rgb = med; hexv = rgb_to_hex_tuple(med); cat = 'green'
    elif red_peak > peak_thresh and red_peak >= ratio_thresh * (green_peak + 1e-6):
        mask = red_sig >= (0.5 * red_peak)
        sel = arr[mask]
        if sel.size:
            sel = sel[sel.sum(axis=1) < 740]
            if sel.size == 0:
                sel = arr[mask]
            med = tuple(map(int, np.median(sel, axis=0)))
            rgb = med; hexv = rgb_to_hex_tuple(med); cat = 'red'
    # small-peak ambiguity: if both peaks are small or comparable, mark uncertainif max(green_peak, red_peak) < max(peak_thresh, 6) or abs(green_peak - red_peak) < (peak_thresh * 0.5):
        uncertain = True
    # if still neutral, fall back to median-based check for dark/black textif cat == 'neutral':
        arr_flat = arr.reshape(-1,3)
        nonwhite = arr_flat[arr_flat.sum(axis=1) < 740]
        if nonwhite.size:
            med = tuple(map(int, np.median(nonwhite, axis=0)))
        else:
            med = tuple(map(int, np.median(arr_flat, axis=0)))
        # Treat near-white median as neutral / no-colorif sum(med) > 740:
            return {'cat': 'neutral', 'rgb': None, 'hex': None, 'green_peak': green_peak, 'red_peak': red_peak, 'ratio': ratio, 'uncertain': uncertain}
        # map median to category (this will yield 'black' for dark low-sat medians)
        cat_med = map_color_to_cat(med)
        if cat_med != 'neutral':
            return {'cat': cat_med, 'rgb': med, 'hex': rgb_to_hex_tuple(med), 'green_peak': green_peak, 'red_peak': red_peak, 'ratio': ratio, 'uncertain': uncertain}
    return {'cat': cat, 'rgb': rgb, 'hex': hexv, 'green_peak': green_peak, 'red_peak': red_peak, 'ratio': ratio, 'uncertain': uncertain}


def sample_glyph_interior_color_near_line(page, line_bbox, zoom=6, gray_pctile=85, erode_iter=1, debug_prefix=None):
    """DEPRECATED: glyph interior raster sampling disabled under span-only policy.

    Kept as a stub for backward compatibility.
    """
    return None, None


def sample_phrase_bbox_color(page, spans, zoom=6, white_threshold=740, erode_iter=1, debug_prefix=None):
    """Sample the phrase bounding box in the rendered page and return median RGB ignoring near-white pixels.

    Returns (hex, rgb) or (None, None).
    """
    if not spans:
        return None, None
    # compute phrase bbox from spans (page coords)
    try:
        xs0 = [s.get('bbox')[0] for s in spans if s.get('bbox')]
        ys0 = [s.get('bbox')[1] for s in spans if s.get('bbox')]
        xs1 = [s.get('bbox')[2] for s in spans if s.get('bbox')]
        ys1 = [s.get('bbox')[3] for s in spans if s.get('bbox')]
        lx0, ly0, lx1, ly1 = min(xs0), min(ys0), max(xs1), max(ys1)
    except Exception:
        return None, Nonetry:
        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)
        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
    except Exception:
        return None, None
    # scale to pixels and add small marginsx0 = max(0, int(lx0 * zoom) - 2); sy0 = max(0, int(ly0 * zoom) - 2)
    sx1 = min(img.width, int(lx1 * zoom) + 2); sy1 = min(img.height, int(ly1 * zoom) + 2)
    if sx1 <= sx0 or sy1 <= sy0:
        return None, Nonecrop = img.crop((sx0, sy0, sx1, sy1))
    arr = np.array(crop)
    # mask out near-white pixels (ignore them when averaging)
    flat = arr.reshape(-1,3)
    if flat.size == 0:
        return None, None
    # select non-white pixels only; if none exist, treat as no-color (near-white)
    sel = flat[flat.sum(axis=1) < white_threshold]
    if sel.size == 0:
        return None, Nonemed = tuple(map(int, np.median(sel, axis=0)))
    # if median is near-white, return Noneif sum(med) > white_threshold:
        return None, Nonereturn rgb_to_hex_tuple(med), med


def _pdf_page_count(pdf_path):
    try:
        res = subprocess.run(['pdfinfo', str(pdf_path)], capture_output=True, text=True, check=True)
        m = re.search(r"Pages:\s*(\d+)", res.stdout)
        return int(m.group(1)) if m else 0except Exception:
        return 0


def _get_page_text(pdf_path, page_num):
    try:
        res = subprocess.run(['pdftotext','-f', str(page_num), '-l', str(page_num), str(pdf_path), '-'], capture_output=True, text=True, check=True)
        return res.stdoutexcept Exception:
        return ''


def _render_page_png(pdf_path, page_num, out_png):
    # use pdftocairo -singlefile to render a single page PNGtry:
        # render at much higher DPI so tiny colored markers become easier to detect
        # use 600 DPI to amplify tiny vector markers for raster samplingsubprocess.run(['pdftocairo', '-r', '600', '-png', '-singlefile', '-f', str(page_num), '-l', str(page_num), str(pdf_path), str(out_png.with_suffix(''))], check=True)
        out_file = out_png.with_suffix('.png')
        return out_file if out_file.exists() else Noneexcept Exception:
        return None


def _tesseract_tokens(png_path):
    """Run tesseract in TSV mode and return list of token dicts with numeric coords."""
    try:
        res = subprocess.run(['tesseract', str(png_path), 'stdout', '--psm', '1', 'tsv'], capture_output=True, text=True, check=True)
        lines = res.stdout.splitlines()
        if not lines:
            return []
        headers = lines[0].split('\t')
        rows = []
        for ln in lines[1:]:
            parts = ln.split('\t')
            if len(parts) < len(headers):
                continued = dict(zip(headers, parts))
            try:
                d['left'] = int(d.get('left') or 0)
                d['top'] = int(d.get('top') or 0)
                d['width'] = int(d.get('width') or 0)
                d['height'] = int(d.get('height') or 0)
            except Exception:
                passrows.append(d)
        return rowsexcept Exception:
        return []


def _detect_canonical_in_pixels(arr):
    """Inspect an Nx3 array of RGB pixels and try to detect a canonical color (green/amber/red/black).
    Returns an RGB tuple if a canonical tint is confidently found, else None."""
    try:
        if arr is None or arr.size == 0:
            return Nonear = arr.reshape(-1,3).astype('int')
        # compute HSV-like metricsarr_f = ar.astype('float32')/255.0mx = arr_f.max(axis=1)
        mn = arr_f.min(axis=1)
        sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
        hues = []
        for i,(r,g,b) in enumerate(ar):
            if sat[i] < 0.01:
                continueh,s,_ = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0)
            hue = h*360hues.append((i,hue,s))
        # choose greens if any obvious green pixels exist
        # accept very low saturation greens as potential markersgreen_idxs = [i for i,h,s in hues if 60 <= h <= 170 and s > 0.005]
        if len(green_idxs) >= 1:
            sel = ar[green_idxs]
            return tuple(map(int, np.median(sel, axis=0)))
        # check closeness to canonical colors (count pixels near canonical)
        for name,canon in CANONICAL.items():
            dists = np.sum((ar - np.array(canon))**2, axis=1)
            # looser threshold to pick up pale tintscnt = np.sum(dists < 90000)
            if cnt >= 1:
                sel = ar[dists < 90000]
                return tuple(map(int, np.median(sel, axis=0)))
        # as a last resort, if image contains any pixel with slight green bias, pick itbiases = [(i, pix) for i,pix in enumerate(ar) if (pix[1]-pix[0])>4 and (pix[1]-pix[2])>4]
        if biases:
            return tuple(map(int, np.median(np.array([p for _,p in biases]), axis=0)))
        return Noneexcept Exception:
        return None


def _sample_near_token(png_path, token, side='left'):
    """Sample a small neighborhood near a token (left or right side) and return median RGB or None."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizeleft = int(token.get('left',0))
        top = int(token.get('top',0))
        width = int(token.get('width',0))
        height = int(token.get('height',0))
        cy = top + height // 2if side == 'left':
            cx = max(2, left - 8)
        else:
            cx = min(w-3, left + width + 8)
        # sample 11x11 around cx,cyx0 = max(0, cx-5)
        y0 = max(0, cy-5)
        x1 = min(w, cx+6)
        y1 = min(h, cy+6)
        crop = img.crop((x0,y0,x1,y1))
        arr = np.array(crop).reshape(-1,3)
        # first try canonical detection in the croptry_can = _detect_canonical_in_pixels(arr)
        if try_can is not None:
            return try_cannonwhite = arr[(arr.sum(axis=1) < 740)]
        if nonwhite.size:
            # prefer most-saturated pixel if anyarr_f = nonwhite.astype('float32')/255.0mx = arr_f.max(axis=1)
            mn = arr_f.min(axis=1)
            sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
            # also consider closeness to canonical colorsbest_score = -1.0best_pix = Nonefor i,pix in enumerate(nonwhite):
                s = float(sat[i])
                dists = [sum((int(a)-int(b))**2 for a,b in zip(pix, canon)) for canon in CANONICAL.values()]
                bestd = min(dists)
                dist_score = max(0.0, (40000.0 - float(bestd)) / 40000.0)
                score = (s * 1.0) + (dist_score * 4.0)
                if score > best_score:
                    best_score = scorebest_pix = pixif best_pix is not None and best_score > 0.02:
                return tuple(map(int, best_pix))
            # otherwise return medianmed = tuple(map(int, np.median(nonwhite, axis=0)))
            return medreturn Noneexcept Exception:
        return None


def _sample_left_of_line(png_path, tokens, best_idx):
    """Given tesseract tokens and an index, sample a narrow left-side strip aligned to the token line."""
    try:
        if best_idx is None:
            return None
        # find tokens on same line (close top)
        ref = tokens[best_idx]
        top_ref = int(ref.get('top', 0))
        img = Image.open(str(png_path)).convert('RGB')
        near = [t for t in tokens if abs(int(t.get('top',0)) - top_ref) <= 8]
        if not near:
            return Nonemin_left = min(int(t.get('left',0)) for t in near)
        cy = top_ref + int(near[0].get('height',0)) // 2x0 = 0
        # extend the sampling width to include potential marker columns further leftx1 = min(img.size[0], min_left + 40)
        y0 = max(0, cy - 12)
        y1 = min(img.size[1], cy + 12)
        crop = img.crop((x0,y0,x1,y1))
        arr = np.array(crop).reshape(-1,3)
        # try blob detection on the crop firsttry:
            a = arr.reshape(-1,3)
            # find candidate green pixels by simple channel differences (allow smaller deltas for pale tints)
            mask_green = (a[:,1].astype(int) - a[:,0].astype(int) > 6) & (a[:,1].astype(int) - a[:,2].astype(int) > 6)
            if mask_green.sum() > 0:
                # prefer dense small clusters rather than the global median to avoid background biash_crop = y1 - y0w_crop = x1 - x0a2 = a.reshape(h_crop, w_crop, 3)
                mask2 = mask_green.reshape(h_crop, w_crop)
                coords = np.argwhere(mask2)
                best_count = 0best_cent = Nonefor yx in coords:
                    y0c, x0c = yxdy = np.abs(coords[:,0] - y0c)
                    dx = np.abs(coords[:,1] - x0c)
                    cnt = np.sum((dy <= 4) & (dx <= 4))
                    if cnt > best_count:
                        best_count = cntbest_cent = (y0c, x0c)
                if best_cent is not None and best_count >= 4:
                    yc, xc = best_centylo = max(0, yc-4); yhi = min(h_crop, yc+5)
                    xlo = max(0, xc-4); xhi = min(w_crop, xc+5)
                    sub = a2[ylo:yhi, xlo:xhi, :].reshape(-1,3)
                    return tuple(map(int, np.median(sub, axis=0)))
                sel = a[mask_green]
                return tuple(map(int, np.median(sel, axis=0)))
            # red candidatemask_red = (a[:,0].astype(int) - a[:,1].astype(int) > 20) & (a[:,0].astype(int) - a[:,2].astype(int) > 20)
            if mask_red.sum() > 0:
                sel = a[mask_red]
                return tuple(map(int, np.median(sel, axis=0)))
        except Exception:
            pass
        # If simple channel masks didn't find a blob, try density-based small-cluster searchtry:
            h_crop = y1 - y0w_crop = x1 - x0a2 = arr.reshape(h_crop, w_crop, 3)
            # mask for slight green bias and reasonably bright green channelmask = (a2[:,:,1].astype(int) - a2[:,:,0].astype(int) > 4) & (a2[:,:,1].astype(int) - a2[:,:,2].astype(int) > 4) & (a2[:,:,1] > 90)
            coords = np.argwhere(mask)
            if coords.size:
                # for each coord, count neighbors within a small window and pick densestbest_count = 0best_cent = Nonebest_cent_near_line = Nonebest_count_near_line = 0
                # prefer clusters near the token vertical line (cy)
                cy_rel = cy - y0for yx in coords:
                    y0c, x0c = yxdy = np.abs(coords[:,0] - y0c)
                    dx = np.abs(coords[:,1] - x0c)
                    cnt = np.sum((dy <= 4) & (dx <= 4))
                    if cnt > best_count:
                        best_count = cntbest_cent = (y0c, x0c)
                    # track best cluster that is near the token lineif abs(y0c - cy_rel) <= 12 and cnt > best_count_near_line:
                        best_count_near_line = cntbest_cent_near_line = (y0c, x0c)
                chosen = Noneif best_cent_near_line is not None and best_count_near_line >= 4:
                    chosen = best_cent_near_lineelif best_cent is not None and best_count >= 6:
                    chosen = best_centif chosen is not None:
                    yc, xc = chosenylo = max(0, yc-4); yhi = min(h_crop, yc+5)
                    xlo = max(0, xc-4); xhi = min(w_crop, xc+5)
                    sub = a2[ylo:yhi, xlo:xhi, :].reshape(-1,3)
                    return tuple(map(int, np.median(sub, axis=0)))
        except Exception:
            pass
        # try canonical detectioncan = _detect_canonical_in_pixels(arr)
        if can is not None:
            return can
        # otherwise use the band median logicreturn _sample_left_band_median_from_png(png_path)
    except Exception:
        return None


def _sample_left_band_median_from_png(png_path):
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizeband = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
        arr = np.array(band).reshape(-1,3)
        # first, try to detect canonical tint in the bandcan = _detect_canonical_in_pixels(arr)
        if can is not None:
            return cannonwhite_mask = (arr.sum(axis=1) < 740)
        if nonwhite_mask.sum():
            sel = arr[nonwhite_mask]
            # compute a composite score that prefers saturation and closeness to canonical colorsarr_f = sel.astype('float32')/255.0mx = arr_f.max(axis=1)
            mn = arr_f.min(axis=1)
            sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
            val = mxmask = (sat > 0.06) & (val > 0.05)
            if mask.sum() > 8:
                sel = arr[mask]
                med = tuple(map(int, np.median(sel, axis=0)))
                return rgb_to_hex_tuple(med), med
            # otherwise return medianmed = tuple(map(int, np.median(sel, axis=0)))
            return medelse:
            small = img.crop((max(0,int(w*0.05)), int(h*0.2), int(w*0.25), int(h*0.6)))
            arr2 = np.array(small).reshape(-1,3)
            # try canonical detection on small cropcan2 = _detect_canonical_in_pixels(arr2)
            if can2 is not None:
                return can2med2 = tuple(map(int, np.median(arr2, axis=0)))
            return med2except Exception:
        return None


def _sample_left_cluster_full(png_path, target_y=None):
    """Search the full left strip (0..25% width) for the densest green/red cluster.
    If target_y provided, prefer clusters near that vertical position."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizestrip = np.array(img.crop((0, 0, int(w*0.25), h))).astype('int')
        h2,w2,_ = strip.shapea = strip.reshape(-1,3)
        if color == 'green':
            mask = (a[:,1] - a[:,0] > 6) & (a[:,1] - a[:,2] > 6)
        elif color == 'red':
            mask = (a[:,0] - a[:,1] > 10) & (a[:,0] - a[:,2] > 10)
        else:
            mask = np.zeros(a.shape[0], dtype=bool)
        if mask.sum() == 0:
            return []
        coords = np.argwhere(mask.reshape(h2,w2))
        clusters = []
        used = np.zeros(coords.shape[0], dtype=bool)
        for i,(y,x) in enumerate(coords):
            if used[i]:
                continue
            # grow a local clusterdy = np.abs(coords[:,0] - y); dx = np.abs(coords[:,1] - x)
            sel = (dy <= 5) & (dx <= 5)
            idxs = np.where(sel & (~used))[0]
            if idxs.size == 0:
                continueused[idxs] = Truepts = coords[idxs]
            cy = int(np.median(pts[:,0]))
            cx = int(np.median(pts[:,1]))
            sub = strip[cy-4:cy+5, max(0,cx-4):min(w2,cx+5), :].reshape(-1,3)
            med = tuple(map(int, np.median(sub, axis=0)))
            clusters.append((cy, cx, med, idxs.size))
        # sort by vertical positionclusters.sort(key=lambda t: t[0])
        return clustersexcept Exception:
        return []


def run_expectation_only_qa():
    """Module-level fallback QA that uses pdftotext/pdftocairo to verify expected phrases and sample colors."""
    qa_rows = []
    qa_amb = []
    qa_summary = []
    loaded_expect_inner = load_expectations_from_dir(ROOT / 'data' / 'pdf_analysis')
    override_rows = []
    wide_overrides = {}
    for fname, exp in loaded_expect_inner.items():
        candidate = PDF_DIR / fnamefound = Noneif not candidate.exists():
            candidate = ROOT / 'data' / 'pdf_analysis' / fnameif not candidate.exists():
            m = re.search(r'user_(\d+)_', fname)
            if m:
                uid = m.group(1)
                for dsearch in [ROOT / 'data' / 'pdf_analysis', PDF_DIR]:
                    for f2 in dsearch.glob(f"user_{uid}_*.pdf"):
                        found = f2breakif found:
                        breakif found:
            candidate = foundif not candidate.exists():
            print('Expected PDF not found for', fname)
            continuepage_count = _pdf_page_count(candidate)
        page_texts = {p+1: _get_page_text(candidate, p+1) for p in range(page_count)}
        total = 0mismatches = 0amb_count = 0for phrase, expected_color in exp.items():
            total += 1uncertain = Falsefound_in_pdf = any(phrase.lower() in (page_texts.get(p,'') or '').lower() for p in page_texts)
            found_rows = []
            found_in_rows = Falserow_cat = ''
            sampled_rgb = Nonesampled_cat = ''
            sample_img = ''
            page_hit = Nonefor p, txt in page_texts.items():
                if phrase.lower() in (txt or '').lower():
                    page_hit = pbreak
            # If phrase not found in the plain page text, try the deterministic span->conv->glyph searchif not found_in_pdf:
                try:
                    import fitz as _fitzdoc_fit = _fitz.open(candidate)
                    cf = color_first_search_for_phrase(doc_fit, phrase, expected_color=expected_color, page_limit=3)
                    if cf:
                        pidx, text_cf, hex_cf, rgb_cf, page_bbox_cf, pix_bbox_cf, *cf_rest = cfuncertain_cf = cf_rest[0] if cf_rest else Falseuncertain = uncertain_cffound_in_pdf = Truepage_hit = pidxsampled_rgb = rgb_cfsampled_cat = map_color_to_cat(rgb_cf)
                        found_rows.append({'method': 'color_first_search', 'rgb': rgb_cf, 'hex': hex_cf, 'uncertain': uncertain_cf})
                        # if classifier matched expected color and not uncertain, mark as found_in_rows and acceptif sampled_cat == expected_color and not uncertain_cf:
                            found_in_rows = Truesampled_hex = ('#%02x%02x%02x' % sampled_rgb) if sampled_rgb else ''
                            row_cat = sampled_catcontinueexcept Exception:
                    passif page_hit:
                # try the deterministic span->conv->glyph flow firsttry:
                    import fitz as _fitzdoc_fit = _fitz.open(candidate)
                    cf = color_first_search_for_phrase(doc_fit, phrase, expected_color=expected_color, page_limit=3)
                    if cf:
                        pidx, text_cf, hex_cf, rgb_cf, page_bbox_cf, pix_bbox_cf, *cf_rest = cfuncertain_cf = cf_rest[0] if cf_rest else Falseuncertain = uncertain_cfsampled_rgb = rgb_cfsampled_cat = map_color_to_cat(rgb_cf)
                        sample_img = ''
                        found_rows.append({'method': 'color_first', 'rgb': rgb_cf, 'hex': hex_cf, 'uncertain': uncertain_cf})
                        # render page png for diagnosticstry:
                            out_png = IMG_DIR / f"qa_{candidate.name}_p{page_hit}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png"
                            _render_page_png(candidate, page_hit, out_png)
                            sample_img = str(out_png)
                        except Exception:
                            pass
                        # if the deterministic result matches expectation and is not uncertain, accept earlyif sampled_cat == expected_color and not uncertain_cf:
                            found_in_rows = Truesampled_hex = ('#%02x%02x%02x' % sampled_rgb) if sampled_rgb else ''
                            row_cat = sampled_catcontinueexcept Exception:
                    passout_png = IMG_DIR / f"qa_{candidate.name}_p{page_hit}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png"
                png = _render_page_png(candidate, page_hit, out_png)
                if png:
                    # token-level sampling via tesseract TSVtokens = _tesseract_tokens(png)
                    target = ' '.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
                    best_idx = Nonebest_score = 0for i, t in enumerate(tokens):
                        txt = (t.get('text') or '').strip()
                        if not txt:
                            continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                        score = len(set(norm.split()) & set(target.split()))
                        if score > best_score:
                            best_score = scorebest_idx = imed_left = _sample_near_token(png, tokens[best_idx], side='left') if (best_idx is not None and best_score>0) else Nonemed_right = _sample_near_token(png, tokens[best_idx], side='right') if (best_idx is not None and best_score>0) else Nonemed_line = _sample_left_of_line(png, tokens, best_idx) if (best_idx is not None and best_score>0) else Nonemed_band = _sample_left_band_median_from_png(png)
                    med_cluster = _sample_left_cluster_full(png, target_y=(int(tokens[best_idx].get('top',0)) if best_idx is not None else None))
                    # choose best med by simple confidence heuristicsdef _med_confidence(m):
                        if m is None:
                            return 0.0name = map_color_to_cat(m)
                        # If we have an EXPECTED color for this phrase, prefer candidates
                        # that match that expected category even if their base score is lower.
                        if expected_color and name == expected_color:
                            return 6.0
                        # also prefer candidates that are reasonably close to the expected canonical
                        # (loosened threshold to accept pale/anti-aliased markers that are farther in euclidean
                        # distance but still visually the expected color)
                        if expected_color and m is not None:
                            try:
                                dexp = color_distance(m, CANONICAL.get(expected_color))
                                if dexp is not None and dexp < 120000:
                                    return 5.0except Exception:
                                pass
                        # strong confidence if non-neutral (explicit color)
                        if name != 'neutral':
                            return 4.0
                        # otherwise score by saturation and distance to nearest canonical,
                        # but with a boosted path for canonical-proximal low-sat candidatesrf,gf,bf = [x/255.0 for x in m]
                        h,s,v = colorsys.rgb_to_hsv(rf,gf,bf)
                        # distance to best canonicaldists = {n: color_distance(m,c) for n,c in CANONICAL.items()}
                        bestd = min(dists.values())
                        # If close to a canonical color (loose threshold) and some saturation, boostif bestd < 90000 and s > 0.005:
                            return 2.5 * (1.0 / (1 + bestd/30000.0)) + (s * 2.0)
                        # slight green bias (g larger than r,b) is also a useful signalif (m[1] - max(m[0], m[2])) > 8:
                            return 1.5 + (s * 1.0)
                        # fallback scoringreturn (s + 0.02) * (1.0 / (1 + bestd/20000.0)) + (0.5 if bestd < 20000 else 0.0)
                    # include cluster candidate last (it tends to be higher-precision when present)
                    # include the combined span/vector/glyph fallback as an additional candidatemed_combined = Nonetry:
                        combined = combined_sample_color_for_phrase(pathlib.Path(candidate).name and __import__('fitz').open(candidate) if True else None, phrase, expected_color=expected_color)
                    except Exception:
                        combined = Noneif combined:
                        # combined returns (p, line_text, hex, rgb, bbox, method)
                        med_combined = combined[3]
                    # try a direct glyph interior sample from the PDF line bbox (strong candidate)
                    med_glyph_direct = Nonetry:
                        import fitz as _fitzdoc = _fitz.open(candidate)
                        page_obj = doc.load_page(page_hit-1)
                        found_bbox = Nonetd = page_obj.get_text('dict')
                        for bb in td.get('blocks', []):
                            for ln2 in bb.get('lines', []):
                                lt = ''.join([s.get('text','') for s in ln2.get('spans', [])]).strip()
                                if not lt:
                                    continueif phrase.lower() in lt.lower() or any(tok.lower() in phrase.lower() or phrase.lower() in tok.lower() for tok in lt.split()):
                                    found_bbox = ln2.get('bbox')
                                    breakif found_bbox:
                                breakif found_bbox is not None:
                            _hexg, rgbg = sample_glyph_interior_color_near_line(page_obj, found_bbox, zoom=8, gray_pctile=90, erode_iter=1)
                            med_glyph_direct = rgbgexcept Exception:
                        med_glyph_direct = None

                    candidates = [med_left, med_right, med_line, med_band, med_cluster, med_combined, med_glyph_direct]
                    best = Nonebest_score = 0for c in candidates:
                        sc = _med_confidence(c)
                        if sc > best_score:
                            best_score = scbest = cmed = best
                    # Debug: for the specific 1314 PDF, record candidate confidences to help iterating heuristicstry:
                        if 'user_1314' in candidate.name:
                            dbg_file = ROOT / 'data' / 'poc_debug_candidates.csv'
                            write_header = not dbg_file.exists()
                            with open(dbg_file, 'a', newline='') as fhdbg:
                                wdbg = csv.writer(fhdbg)
                                if write_header:
                                    wdbg.writerow(['filename','phrase','candidate_repr','candidate_hex','candidate_cat','confidence'])
                                for c in candidates:
                                    ch = ('#%02x%02x%02x' % c) if c else ''
                                    wdbg.writerow([candidate.name, phrase, str(c), ch, map_color_to_cat(c) if c else '', _med_confidence(c)])
                                # also note chosenwdbg.writerow([candidate.name, phrase, 'chosen', ('#%02x%02x%02x' % med) if med else '', map_color_to_cat(med) if med else '', '---'])
                    except Exception:
                        passif med:
                        sampled_rgb = medsample_img = str(png)
                        sampled_cat = map_color_to_cat(med)
                        # If the chosen candidate is neutral, attempt a diagnostic canonical
                        # detection on the left-of-line crop to recover pale tints.
                        if sampled_cat == 'neutral':
                            try:
                                diag_crop = Noneif page_hit and png:
                                    tokens2 = _tesseract_tokens(png)
                                    # find token most similar to the phrase againbest_idx2 = Nonebest_score2 = 0target2 = ' '.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
                                    for i, t in enumerate(tokens2):
                                        txt = (t.get('text') or '').strip()
                                        if not txt: continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                                        score = len(set(norm.split()) & set(target2.split()))
                                        if norm in target2 or target2 in norm:
                                            score += 2if score and score > best_score2:
                                            best_score2 = scorebest_idx2 = iif best_idx2 is not None and best_score2 > 0:
                                        ref = tokens2[best_idx2]
                                        top_ref = int(ref.get('top', 0))
                                        near = [t for t in tokens2 if abs(int(t.get('top',0)) - top_ref) <= 8]
                                        if near:
                                            min_left = min(int(t.get('left',0)) for t in near)
                                            cy = top_ref + int(near[0].get('height',0)) // 2x0 = 0x1 = min(png.size[0], min_left + 40)
                                            y0 = max(0, cy - 12)
                                            y1 = min(png.size[1], cy + 12)
                                            diag_crop = Image.open(str(png)).crop((x0,y0,x1,y1)).convert('RGB')
                                if diag_crop is None and png:
                                    diag_crop = Image.open(str(png)).convert('RGB').crop((0,int(png.size[1]*0.15), int(png.size[0]*0.25), int(png.size[1]*0.85)))
                                if diag_crop is not None:
                                    arr = np.array(diag_crop).reshape(-1,3)
                                    # compute candidate pixel metricscands = []
                                    for pix in arr:
                                        r,g,b = map(int,pix)
                                        h,s,v = colorsys.rgb_to_hsv(r/255.0,g/255.0,b/255.0)
                                        hue = int(h*360)
                                        sat = float(s)
                                        dists = {n: color_distance((r,g,b),c) for n,c in CANONICAL.items()}
                                        bestd = min(dists.values())
                                        cands.append({'rgb':(r,g,b),'hex':('#%02x%02x%02x' % (r,g,b)),'hue':hue,'sat':sat,'bestd':bestd})
                                    # take top 6 by combined scorefor cc in sorted(cands, key=lambda x: (-(x['sat']*1.0 + (max(0, (40000.0 - x['bestd'])/40000.0))*6.0)))[:6]:
                                        override_rows.append({'filename': candidate.name, 'source': str(candidate), 'record_id': re.search(r'user_(\d+)_', candidate.name).group(1) if re.search(r'user_(\d+)_', candidate.name) else candidate.name, 'line_text': phrase, 'canonical': map_line_to_canonical(phrase), 'color_hex': cc['hex'], 'color_cat': map_color_to_cat(cc['rgb']), 'color_method': 'diag'})
                            except Exception:
                                pass
            # if still neutral, try horizontal band scan to catch faint colored markersif cat == 'neutral':
                try:
                    hex3, rgb3 = None, Nonedef band_scan_color(page, line_text, img_path):
                        pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
                        target = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                            t = (txt or '').strip()
                            if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                            score = len(set(norm.split()) & set(target.split()))
                            if score > best_score:
                                best_score = scorebest_y = data['top'][i]
                        w,h = img.sizeif best_y is None:
                            band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                            arr = np.array(band).reshape(-1,3)
                            nonwhite = arr[(arr.sum(axis=1) < 740)]
                            if nonwhite.size:
                                med = tuple(map(int, np.median(nonwhite, axis=0)))
                                if out_img_path:
                                    band.save(out_img_path)
                                return medelse:
                            cx = int(w * 0.12)
                            cy = int(best_y + 8)
                            med = median_5x5(img, cx, cy)
                            if med:
                                if out_img_path:
                                    small = img.crop((max(0,cx-20), max(0,cy-20), min(w, cx+20), min(h, cy+20)))
                                    small.save(out_img_path)
                                return medexcept Exception:
                    pass
            # if still neutral, try horizontal band scan to catch faint colored markersif cat == 'neutral':
                try:
                    hex3, rgb3 = None, Nonedef band_scan_color(page, line_text, img_path):
                        pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
                        target = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                            t = (txt or '').strip()
                            if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                            score = len(set(norm.split()) & set(target.split()))
                            if score > best_score:
                                best_score = scorebest_y = data['top'][i]
                        w,h = img.sizeif best_y is None:
                            band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                        else:
                            band = img.crop((0, max(0,best_y-6), int(w*0.25), min(h, best_y+18)))
                        arr = np.array(band).reshape(-1,3)
                        arr_f = arr.astype('float32')/255.0mx = arr_f.max(axis=1)
                        mn = arr_f.min(axis=1)
                        sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
                        val = mxmask = (sat > 0.06) & (val > 0.05)
                        if mask.sum() > 8:
                            sel = arr[mask]
                            med = tuple(map(int, np.median(sel, axis=0)))
                            return rgb_to_hex_tuple(med), med
                        # fallback: median of non-white pixelsnonwhite = arr[(arr.sum(axis=1) < 740)]
                        if nonwhite.size:
                            med = tuple(map(int, np.median(nonwhite, axis=0)))
                            return rgb_to_hex_tuple(med), medreturn None, Nonehex3, rgb3 = band_scan_color(page, ln, IMG_DIR / f"{fname}.p{page_idx}.band.png")
                    if hex3 and rgb3:
                        color_hex, rgb = hex3, rgb3method = 'band'
                        cat = map_color_to_cat(rgb)
                except Exception:
                    passcolor_counts[cat] = color_counts.get(cat,0) + 1canonical = map_line_to_canonical(ln)
            # If we have an expectation for this filename and this line, try targeted
            # canonical/cluster/global searches to recover pale/anti-aliased markerstry:
                exp_for_file = EXPECT.get(fname, {}) if 'EXPECT' in locals() else {}
                expected_color = Nonefor ep, ec in exp_for_file.items():
                    # match by normalized/canonicalized phraseif canonicalize(ep) == canonicalize(ln) or canonicalize(ep) in canonicalize(ln) or canonicalize(ln) in canonicalize(ep):
                        expected_color = ecbreakif cat == 'neutral' and expected_color:
                    # render page PNG if not alreadytry:
                        page = doc.load_page(page_idx)
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"qa_{fname}.p{page_idx}_{re.sub('[^a-z0-9]','_',ln.lower())[:40]}.png")
                    except Exception:
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"{fname}.p{page_idx}.png")
                    # tokenizationtokens = _tesseract_tokens(png)
                    # find best token for phrasebest_idx = None; best_score = 0target = ' '.join(ch.lower() for ch in ln if ch.isalnum() or ch.isspace()).strip()
                    for i,t in enumerate(tokens):
                        txt = (t.get('text') or '').strip()
                        if not txt: continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                        score = len(set(norm.split()) & set(target.split()))
                        if score > best_score:
                            best_score = score; best_idx = i
                    # try canonical near linetry_can = _find_canonical_near_line(png, tokens, best_idx, expected_color) if expected_color else Noneif try_can is not None:
                        color_hex = '#%02x%02x%02x' % try_canrgb = try_cancat = expected_color if map_color_to_cat(try_can) == expected_color or color_distance(try_can, CANONICAL.get(expected_color)) < 120000 else map_color_to_cat(try_can)
                        method = 'canon_search'
                    # aggressive green cluster & left-strip clustering are marker-oriented; only run if markers enabledif markers_enabled() and cat == 'neutral' and expected_color == 'green':
                        ag = _aggressive_green_search(png, tokens, best_idx)
                        if ag is not None and map_color_to_cat(ag) == 'green':
                            color_hex = '#%02x%02x%02x' % agrgb = agcat = 'green'
                            method = 'aggr_green'
                        # left-strip clustering (green)
                        clusters = _find_color_clusters_in_left_strip(png, color='green')
                        if clusters and best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2best_cluster = min(clusters, key=lambda c: abs(c[0] - token_cy))
                            if abs(best_cluster[0] - token_cy) <= 40 and best_cluster[3] >= 4:
                                color_hex = '#%02x%02x%02x' % best_cluster[2]
                                rgb = best_cluster[2]
                                cat = 'green'
                                method = 'strip_cluster'
                    # final global canonical searchif cat == 'neutral' and expected_color:
                        token_cy = Noneif best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2g = _global_canonical_search(png, token_cy, expected_color)
                        if g is not None:
                            color_hex = '#%02x%02x%02x' % grgb = gcat = map_color_to_cat(g)
                            method = 'global_canon'
            except Exception:
                pass
            # If canonical is not one of our known keys, keep as "other"
            known_keys = set(k for _,k in MAPPING_RULES)
            if canonical not in known_keys:
                other_factors.append((canonical, cat))
            else:
                # store highest-severity color if multiple instances of same factor occur (prefer red>amber>green>black)
                prev = factor_map.get(canonical, 0)
                code = COLOR_CODE.get(cat, 0)
                if code > prev:
                    factor_map[canonical] = code
            # only append row_records for known canonical factors or non-neutral colorsif canonical in known_keys or cat != 'neutral' or '$' in ln or 'inq' in ln.lower():
                row_records.append({
                    'filename': fname,
                    'source': src,
                    'record_id': record_id,
                    'line_text': ln_proc,
                    'canonical': canonical,
                    'color_hex': color_hex or '',
                    'color_cat': cat,
                    'color_method': method,
                })
        # targeted search for expected phrases (patch missing factors called out in QA and expectations)
        targeted_phrases = set(targeted.get(fname, []) if isinstance(targeted, dict) else targeted) if targeted else set()
        # include EXPECT phrases if presenttry:
            exp_phrases = set(EXPECT.get(fname, {}).keys())
        except Exception:
            exp_phrases = set()
        targeted_phrases.update(exp_phrases)
        for phrase in targeted_phrases:
            if phrase.lower() in full_text.lower():
                # find page containing the phrase and sample colorfor p in range(len(doc)):
                    ptext = doc.load_page(p).get_text()
                    if phrase.lower() in ptext.lower():
                        hex3, rgb3 = None, Nonetry:
                            med = band_scan_color_for_phrase(doc, phrase, out_img_path=IMG_DIR / f"patch_{fname}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png")
                            if med is not None:
                                hex3 = '#%02x%02x%02x' % medrgb3 = medexcept Exception:
                            hex3, rgb3 = None, Nonecat = map_color_to_cat(rgb3) if rgb3 else 'neutral'
                        canonical = map_line_to_canonical(phrase)
                        # ensure we record even if canonical unknown (we'll add to other_factors)
                        if canonical in set(k for _,k in MAPPING_RULES):
                            factor_map[canonical] = max(factor_map.get(canonical,0), COLOR_CODE.get(cat,0))
                        else:
                            other_factors.append((canonical, cat))
                        row_records.append({
                            'filename': fname,
                            'source': src,
                            'record_id': record_id,
                            'line_text': phrase,
                            'canonical': canonical,
                            'color_hex': hex3 or '',
                            'color_cat': cat,
                            'color_method': 'patch',
                        })
                        break
        # sample record-level colors for important fields (credit score, payments, etc.) when availabledef sample_label_color(doc, label_regex_list):
            # look for any label in the document and return sampled RGB using band_scan_color_for_phrasefor label in label_regex_list:
                if re.search(label, full_text, re.I):
                    med = band_scan_color_for_phrase(doc, label, out_img_path=IMG_DIR / f"label_{fname}_{re.sub('[^a-z0-9]','_',label)[:40]}.png")
                    if med:
                        return medreturn None

        # compute wide rowwide_row = {'filename': fname, 'source': src, 'record_id': record_id}
        # default factor columns (only known canonical keys)
        for k,v in factor_map.items():
            wide_row[k] = vwide_row['other_factors'] = ';'.join([f"{k}:{c}" for k,c in other_factors[:80]])
        # counts per colorwide_row['count_red'] = color_counts.get('red',0)
        wide_row['count_amber'] = color_counts.get('amber',0)
        wide_row['count_green'] = color_counts.get('green',0)
        wide_row['count_black'] = color_counts.get('black',0)
        # record-level numeric fieldswide_row['credit_score'] = rec.get('credit_score','')
        wide_row['age'] = rec.get('age','')
        # attempt to sample credit score colorcs_med = sample_label_color(doc, ['Credit Score', r'\b\d{3}\b'])
        if cs_med:
            wide_row['credit_score_color_hex'] = rgb_to_hex_tuple(cs_med)
            wide_row['credit_score_color_cat'] = map_color_to_cat(cs_med)
        else:
            wide_row['credit_score_color_hex'] = ''
            wide_row['credit_score_color_cat'] = ''
        # payment samplingpay_med = sample_label_color(doc, ['Payments?\b', 'Payment\b'])
        if pay_med:
            wide_row['payment_color_hex'] = rgb_to_hex_tuple(pay_med)
            wide_row['payment_color_cat'] = map_color_to_cat(pay_med)
        else:
            wide_row['payment_color_hex'] = ''
            wide_row['payment_color_cat'] = ''
        # record-level numeric fields parsed earlierwide_row['monthly_payment'] = rec.get('monthly_payment','')
        wide_row['credit_freeze'] = rec.get('credit_freeze',0)
        wide_row['fraud_alert'] = rec.get('fraud_alert',0)
        wide_row['deceased'] = rec.get('deceased',0)
        wide_row['address'] = rec.get('address','')
        wide_row['revolving_open_count'] = rec.get('revolving_open_count','')
        wide_row['revolving_open_total'] = rec.get('revolving_open_total','')
        wide_row['installment_open_count'] = rec.get('installment_open_count','')
        wide_row['installment_open_total'] = rec.get('installment_open_total','')
        wide_row['collections_open'] = rec.get('collections_open','')
        wide_row['collections_closed'] = rec.get('collections_closed','')
        wide_row['inquiries_6mo'] = rec.get('inquiries_6mo','')
        wide_row['late_pays_recent'] = rec.get('late_pays_recent','')
        wide_row['late_pays_prior'] = rec.get('late_pays_prior','')        # extract open/closed counts heuristicsm_open_closed = re.search(r"(\d+) open.*?(\d+) closed", full_text, re.I|re.S)
        if m_open_closed:
            wide_row['num_open'] = int(m_open_closed.group(1))
            wide_row['num_closed'] = int(m_open_closed.group(2))
        # quick collection countswide_row['has_collections'] = 1 if 'collection' in full_text.lower() else 0
# extract open/closed collection countsm_open_col = re.search(r"(\d+) open.*collection", full_text, re.I)
    if m_open_col:
        wide_row['num_open_collections'] = int(m_open_col.group(1))
    else:
        wide_row['num_open_collections'] = len(re.findall(r'active collection|open collection', full_text, re.I))
    m_closed_col = re.search(r"(\d+) closed.*collection", full_text, re.I)
    if m_closed_col:
        wide_row['num_closed_collections'] = int(m_closed_col.group(1))
    else:
        # heuristic: count occurrences of 'unpaid collection' or 'paid collection'
        wide_row['num_closed_collections'] = len(re.findall(r'unpaid collection|paid collection|collection', full_text, re.I))
    wide_row['num_collections'] = wide_row['num_open_collections'] + wide_row['num_closed_collections']
    wide_row['has_collections'] = 1 if wide_row['num_collections']>0 else 0

    wide_row['rev_balance_total'] = Nonewide_row['installment_balance_total'] = Nonewide_row['rev_limit_total'] = Nonewide_row['monthly_income'] = Nonewide_row['monthly_payment'] = None
    # try to parse balances and incomesm_rev = re.search(r"rev.*?balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_rev:
        wide_row['rev_balance_total'] = float(m_rev.group(1).replace(',',''))
    m_inst = re.search(r"installment.*balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_inst:
        wide_row['installment_balance_total'] = float(m_inst.group(1).replace(',',''))
    m_lim = re.search(r"limit.*?\$?([0-9,]+)", full_text, re.I)
    if m_lim:
        wide_row['rev_limit_total'] = float(m_lim.group(1).replace(',',''))
    # try to parse monthly income / monthly paymentsm_income = re.search(r"\$(\d{1,3}(?:,\d{3})*)(?:\s*/\s*mo|/mo| per month)", full_text, re.I)
    if m_income:
        wide_row['monthly_income'] = float(m_income.group(1).replace(',',''))
    # monthly payment found as 'Pay $1234/mo' or 'Payments $x/mo'
    m_pay = re.search(r"pay\s*\$?([0-9,]+)\s*/\s*mo|payments\s*\$?([0-9,]+)\s*/\s*mo", full_text, re.I)
    if m_pay:
        num = m_pay.group(1) or m_pay.group(2)
        if num:
            wide_row['monthly_payment'] = float(num.replace(',',''))
    # compute derived ratiosif wide_row['rev_balance_total'] and wide_row.get('rev_limit_total'):
        try:
            wide_row['rev_utilization'] = wide_row['rev_balance_total'] / wide_row['rev_limit_total']
        except Exception:
            wide_row['rev_utilization'] = Noneelse:
        wide_row['rev_utilization'] = Noneif wide_row['installment_balance_total'] and wide_row.get('monthly_income'):
        try:
            wide_row['installment_balance_to_income'] = wide_row['installment_balance_total'] / (wide_row['monthly_income'] * 12)
        except Exception:
            wide_row['installment_balance_to_income'] = Noneelse:
        wide_row['installment_balance_to_income'] = Noneif wide_row.get('monthly_payment') and wide_row.get('monthly_income'):
        try:
            wide_row['monthly_payment_to_income'] = wide_row['monthly_payment'] / wide_row['monthly_income']
        except Exception:
            wide_row['monthly_payment_to_income'] = Noneelse:
        wide_row['monthly_payment_to_income'] = None
    # public records: parse explicit counts and capture notes (e.g., bankruptcy)
    pr_count, pr_note = parse_public_records(full_text)
    wide_row['public_records'] = pr_countwide_row['public_record_note'] = pr_note
    # num_90d_plus_latesm_90 = re.findall(r'(\d+)\s*\b90d\b|90\+\s*d', full_text, re.I)
    wide_row['num_90d_plus_lates'] = sum(int(x) for x in re.findall(r'(\d+)\s*90\+|90d', full_text, re.I) if x.isdigit()) if re.findall(r'(\d+)\s*90', full_text, re.I) else 0wide_row['num_90d_plus_lates'] = wide_row.get('num_90d_plus_lates', 0) + (1 if '90d' in full_text and 'late' in full_text else 0)
    # other boolean heuristics (derived from counts when possible)
    out['has_public_record'] = 1 if out.get('public_records',0) > 0 or out.get('public_record_note') else 0out['has_collections'] = 1 if (out.get('collections_open',0) + out.get('collections_closed',0)) > 0 else 0
    # if a doc is provided, try to extract table-like numeric fields (installment/revolving) from page text and fall back to OCR if neededtry:
        if doc is not None:
            prev_ln = ''
            for p in range(len(doc)):
                td = doc.load_page(p).get_text('dict')
                for b in td.get('blocks', []):
                    for ln in b.get('lines', []):
                        ln_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                        if not ln_text:
                            continuecombined = (prev_ln + ' ' + ln_text).strip()
                        try:
                            # candidate matches on prev/current/combinedcnt_comb, amt_comb = parse_count_amount_pair(combined)
                            cnt_line, amt_line = parse_count_amount_pair(ln_text)
                            cnt_prev, amt_prev = parse_count_amount_pair(prev_ln)

                            def is_install(s):
                                return 'install' in (s or '').lower()
                            def is_revolv(s):
                                s = (s or '').lower()
                                return 'revolv' in s or 'rev' in s or 'revolving' in s or 'revolving' in sdef is_loc(s):
                                s = (s or '').lower()
                                return 'line of credit' in s or 'line of' in s

                            assigned = False
                            # Prefer numeric directly on the line and attach to previous label (common pattern)
                            if cnt_line is not None or amt_line is not None:
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                        out['installment_open_count'] = int(cnt_line)
                                    if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                        out['installment_open_total'] = int(amt_line)
                                    assigned = Trueelif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                        out['revolving_open_count'] = int(cnt_line)
                                    if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                        out['revolving_open_total'] = int(amt_line)
                                    assigned = True
                                # if not attached to prev, attach to current if its label matchesif not assigned:
                                    if is_install(ln_text):
                                        if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                            out['installment_open_count'] = int(cnt_line)
                                        if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                            out['installment_open_total'] = int(amt_line)
                                        assigned = Trueelif is_revolv(ln_text) or is_loc(ln_text):
                                        if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                            out['revolving_open_count'] = int(cnt_line)
                                        if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                            out['revolving_open_total'] = int(amt_line)
                                        assigned = True
                            # else consider prev numeric or combinedif (not assigned) and (cnt_prev is not None or amt_prev is not None):
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_prev is not None:
                                        out['installment_open_count'] = int(cnt_prev)
                                    if out.get('installment_open_total') in (None,'') and amt_prev is not None:
                                        out['installment_open_total'] = int(amt_prev)
                                elif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_prev is not None:
                                        out['revolving_open_count'] = int(cnt_prev)
                                    if out.get('revolving_open_total') in (None,'') and amt_prev is not None:
                                        out['revolving_open_total'] = int(amt_prev)
                                assigned = True
                            # fallback: combined detection but prefer prev label if both have labelsif (not assigned) and (cnt_comb is not None or amt_comb is not None):
                                if is_install(prev_ln) or (is_install(ln_text) and not is_revolv(prev_ln) and not is_loc(prev_ln)):
                                    if out.get('installment_open_count') in (None,'') and cnt_comb is not None:
                                        out['installment_open_count'] = int(cnt_comb)
                                    if out.get('installment_open_total') in (None,'') and amt_comb is not None:
                                        out['installment_open_total'] = int(amt_comb)
                                elif is_revolv(prev_ln) or is_loc(prev_ln) or is_revolv(ln_text) or is_loc(ln_text):
                                    if out.get('revolving_open_count') in (None,'') and cnt_comb is not None:
                                        out['revolving_open_count'] = int(cnt_comb)
                                    if out.get('revolving_open_total') in (None,'') and amt_comb is not None:
                                        out['revolving_open_total'] = int(amt_comb)
                        except Exception:
                            pass
                        # extra parsing: detect inquiries, collections, late pays, and payments from nearby table lineslow_ln = ln_text.lower()
                        # Inquiries (Last 6 Months): single number often on its own lineif 'inquiri' in low_ln or 'inquir' in low_ln or 'inquires' in low_ln:
                            # avoid capturing the timeframe 'Last 6 Months' (6) - prefer an adjacent actual countif not re.search(r"last\s*\d+\s*months?", ln_text, re.I):
                                m_in = re.search(r"(\d+)", ln_text)
                                if m_in:
                                    out['inquiries_6mo'] = int(m_in.group(1))
                                    # proceed to next line if this looks like the timeframe instead
                                    # else try previous/next numericelse:
                                # try previous or following numeric; previous may contain a numberm_prev = re.search(r"(\d+)", prev_ln)
                                if m_prev:
                                    out['inquiries_6mo'] = int(m_prev.group(1))
                                else:
                                    # try to find the number on the following line using combined detectionif cnt_line is not None:
                                        out['inquiries_6mo'] = int(cnt_line)
                        # Collections (Open/Closed): '0 / 0' may be on same or next lineif 'collections' in low_ln:
                            m_col = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_col:
                                out['collections_open'] = int(m_col.group(1))
                                out['collections_closed'] = int(m_col.group(2))
                            else:
                                # try next/prev line via combinedif cnt_line is not None and amt_line is not None:
                                    out['collections_open'] = int(cnt_line)
                                    out['collections_closed'] = int(amt_line)
                        # Late Pays (Last 2/2+ Years) -> look for '7 / 0' pair on same or next lineif 'late pays' in low_ln:
                            m_lp = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_lp:
                                out['late_pays_recent'] = int(m_lp.group(1))
                                out['late_pays_prior'] = int(m_lp.group(2))
                            elif cnt_line is not None and amt_line is not None:
                                out['late_pays_recent'] = int(cnt_line)
                                out['late_pays_prior'] = int(amt_line)
                        # Payments: look for dollar amount on same or adjacent linesif 'payments' in low_ln or 'payment' in low_ln:
                            m_pay = re.search(r"\$?\s*([0-9,]+)\s*(?:/mo|mo)?", ln_text, re.I)
                            if m_pay:
                                out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
                            else:
                                # check previous/nextif cnt_line is not None:
                                    out['monthly_payment'] = int(cnt_line)
                                else:
                                    m_prev = re.search(r"\$?\s*([0-9,]+)", prev_ln)
                                    if m_prev:
                                        out['monthly_payment'] = int(m_prev.group(1).replace(',',''))
                        prev_ln = ln_text
                # if still missing, try OCR on rendered pageif (out.get('installment_open_count') in (None,'') or out.get('installment_open_total') in (None,'')) or (out.get('revolving_open_count') in (None,'') or out.get('revolving_open_total') in (None,'')):
                    try:
                        import pytesseractfrom PIL import Imagepix = doc.load_page(p).get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        ocr_text = pytesseract.image_to_string(img)
                        for oline in ocr_text.splitlines():
                            if not oline.strip():
                                continuecnt, amt = parse_count_amount_pair(oline)
                            if (cnt is not None or amt is not None):
                                ln_low = oline.lower()
                                # assign to installment if header contains 'install'
                                if 'install' in ln_low:
                                    if out.get('installment_open_count') in (None,'') and cnt is not None:
                                        out['installment_open_count'] = int(cnt)
                                    if out.get('installment_open_total') in (None,'') and amt is not None:
                                        out['installment_open_total'] = int(amt)
                                # assign to revolving if header contains 'revolv' or 'rev'
                                if 'revolv' in ln_low or 'rev' in ln_low:
                                    if out.get('revolving_open_count') in (None,'') and cnt is not None:
                                        out['revolving_open_count'] = int(cnt)
                                    if out.get('revolving_open_total') in (None,'') and amt is not None:
                                        out['revolving_open_total'] = int(amt)
                    except Exception:
                        pass
            # finally, populate credit factors using the right-column extractortry:
                out['credit_factors'] = extract_credit_factors_from_doc(doc)
            except Exception:
                out['credit_factors'] = out.get('credit_factors', [])
            # post-page scan heuristics: if we still don't have inquiries or late pays, try page-text searchestry:
                page_text = '\n'.join([doc.load_page(i).get_text() for i in range(len(doc))])
                # Inquiries: find the label and then the numeric value that is NOT part of 'Last 6 Months'
                if out.get('inquiries_6mo') in (None,''):
                    for m in re.finditer(r"Inquir(?:y|ies|es)?", page_text, re.I):
                        # scan the following lines for a line with a standalone number (skip 'Last 6 Months' in parentheses)
                        snippet = page_text[m.end(): m.end()+300]
                        for ln in snippet.splitlines():
                            ln_strip = ln.strip()
                            if not ln_strip:
                                continueif re.search(r"months?|mo", ln_strip, re.I):
                                # skip lines that are just the label with 'Last 6 Months'
                                continuemnum = re.search(r"\b(\d{1,3})\b", ln_strip)
                            if mnum:
                                out['inquiries_6mo'] = int(mnum.group(1))
                                breakif out.get('inquiries_6mo') not in (None,''):
                            break
                # Late Pays: find a numeric 'x / y' following the label
                # prefer pairs on the following line to avoid matching the '(Last 2/2+ Years)' parentheticalfound = Falsefor m in re.finditer(r"Late Pays", page_text, re.I):
                    snippet = page_text[m.end(): m.end()+500]
                    for mp in re.finditer(r"(\d+)\s*/\s*(\d+)", snippet):
                        pair = mp.group(0)
                        pair_pos = snippet.find(pair)
                        # accept only if the pair occurs after a newline (i.e., likely on the next line)
                        if '\n' in snippet[:pair_pos]:
                            out['late_pays_recent'] = int(mp.group(1))
                            out['late_pays_prior'] = int(mp.group(2))
                            found = Truebreakif found:
                        break
                # Credit Card Open Totals (No Retail): inspect account table blocks and consider their final 'total' linetry:
                    cc_found = Nonefor p in range(len(doc)):
                        td = doc.load_page(p).get_text('dict')
                        blocks = td.get('blocks', [])
                        for bi, b in enumerate(blocks):
                            block_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b.get('lines', [])])
                            low_block = block_text.lower()
                            # Heuristic: block that contains 'revolving accounts' or 'revolving accounts (open)' likely precedes credit card totalsif 'revolving accounts' not in low_block and 'revolving accounts (open)' not in low_block:
                                continue
                            # scan following blocks for a block containing 'total' (e.g., 'Credit Card Open Totals')
                            total_block = Nonefor j in range(bi+1, min(bi+30, len(blocks))):
                                b2 = blocks[j]
                                b2_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b2.get('lines', [])])
                                if 'total' in b2_text.lower() or 'credit card open totals' in b2_text.lower():
                                    total_block = b2breakif not total_block:
                                continue
                            # get the last line that contains 'total' or the first line of total_blocktotal_lines = [ln for ln in total_block.get('lines', []) if 'total' in ''.join([s.get('text','') for s in ln.get('spans', [])]).lower()]
                            if total_lines:
                                ln = total_lines[-1]
                            else:
                                # fallback to first meaningful lineln = total_block.get('lines', [])[0]
                            line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                            block_text_full = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in total_block.get('lines', [])])
                            # try to parse numeric totals from the block lines (balance, limit+util, payment)
                            bal = lim = util = pay = None
                            # look for dollar amounts and percent in block lineslines = [lt.strip() for lt in block_text_full.splitlines() if lt.strip()]
                            # simple heuristics based on observed layoutamounts = []
                            perc = Nonefor ln_txt in lines:
                                m_amt = re.search(r"\$([0-9,]+)", ln_txt)
                                if m_amt:
                                    amounts.append(int(m_amt.group(1).replace(',', '')))
                                m_perc = re.search(r"([0-9]{1,3})%", ln_txt)
                                if m_perc:
                                    perc = int(m_perc.group(1))
                            if amounts:
                                bal = amounts[0]
                                if len(amounts) >= 2:
                                    lim = amounts[1]
                                if len(amounts) >= 3:
                                    pay = amounts[2]
                                util = perc
                            # determine color: prefer span color of the label line then convolution fallbackhexv, rgb = span_color_hex(ln.get('spans', []))
                            cat = Noneif rgb:
                                cat = map_color_to_cat(rgb)
                            else:
                                try:
                                    clf = classify_snippet_color_convolution(doc.load_page(p), ln.get('bbox'))
                                    cat = clf.get('cat')
                                except Exception:
                                    cat = None
                            # include only if redif cat == 'red':
                                cc_found = {'color': 'red', 'balance': bal, 'limit': lim, 'utilization_percent': util, 'payment': pay}
                                breakif cc_found:
                            breakif cc_found:
                        out['credit_card_open_totals_no_retail'] = cc_foundexcept Exception:
                    pass
                # if not found, do not fallback to parenthetical pair (avoid capturing '2/2' from label)
                # monthly payment: fallback to first $NNN/mo on the page if not foundif out.get('monthly_payment') in (None,''):
                    m_pay = re.search(r"\$\s*([0-9,]{1,7})\s*/\s*mo", page_text, re.I)
                    if m_pay:
                        out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
            except Exception:
                pass
    # ensure numeric fields are present as empty values if not foundout['installment_open_count'] = out.get('installment_open_count', '')
    out['installment_open_total'] = out.get('installment_open_total', '')
    out['revolving_open_count'] = out.get('revolving_open_count', '')
    out['revolving_open_total'] = out.get('revolving_open_total', '')
    out['inquiries_6mo'] = out.get('inquiries_6mo', '')
    out['late_pays_recent'] = out.get('late_pays_recent', '')
    out['late_pays_prior'] = out.get('late_pays_prior', '')
    out['monthly_payment'] = out.get('monthly_payment', '')
    return outdef run_expectation_only_qa():
    """Module-level fallback QA that uses pdftotext/pdftocairo to verify expected phrases and sample colors."""
    qa_rows = []
    qa_amb = []
    qa_summary = []
    loaded_expect_inner = load_expectations_from_dir(ROOT / 'data' / 'pdf_analysis')
    override_rows = []
    wide_overrides = {}
    for fname, exp in loaded_expect_inner.items():
        candidate = PDF_DIR / fnamefound = Noneif not candidate.exists():
            candidate = ROOT / 'data' / 'pdf_analysis' / fnameif not candidate.exists():
            m = re.search(r'user_(\d+)_', fname)
            if m:
                uid = m.group(1)
                for dsearch in [ROOT / 'data' / 'pdf_analysis', PDF_DIR]:
                    for f2 in dsearch.glob(f"user_{uid}_*.pdf"):
                        found = f2breakif found:
                        breakif found:
            candidate = foundif not candidate.exists():
            print('Expected PDF not found for', fname)
            continuepage_count = _pdf_page_count(candidate)
        page_texts = {p+1: _get_page_text(candidate, p+1) for p in range(page_count)}
        total = 0mismatches = 0amb_count = 0for phrase, expected_color in exp.items():
            total += 1uncertain = Falsefound_in_pdf = any(phrase.lower() in (page_texts.get(p,'') or '').lower() for p in page_texts)
            found_rows = []
            found_in_rows = Falserow_cat = ''
            sampled_rgb = Nonesampled_cat = ''
            sample_img = ''
            page_hit = Nonefor p, txt in page_texts.items():
                if phrase.lower() in (txt or '').lower():
                    page_hit = pbreak
            # If phrase not found in the plain page text, try the deterministic span->conv->glyph searchif not found_in_pdf:
                try:
                    import fitz as _fitzdoc_fit = _fitz.open(candidate)
                    cf = color_first_search_for_phrase(doc_fit, phrase, expected_color=expected_color, page_limit=3)
                    if cf:
                        pidx, text_cf, hex_cf, rgb_cf, page_bbox_cf, pix_bbox_cf, *cf_rest = cfuncertain_cf = cf_rest[0] if cf_rest else Falseuncertain = uncertain_cffound_in_pdf = Truepage_hit = pidxsampled_rgb = rgb_cfsampled_cat = map_color_to_cat(rgb_cf)
                        found_rows.append({'method': 'color_first_search', 'rgb': rgb_cf, 'hex': hex_cf, 'uncertain': uncertain_cf})
                        # if classifier matched expected color and not uncertain, mark as found_in_rows and acceptif sampled_cat == expected_color and not uncertain_cf:
                            found_in_rows = Truesampled_hex = ('#%02x%02x%02x' % sampled_rgb) if sampled_rgb else ''
                            row_cat = sampled_catcontinueexcept Exception:
                    passif page_hit:
                # try the deterministic span->conv->glyph flow firsttry:
                    import fitz as _fitzdoc_fit = _fitz.open(candidate)
                    cf = color_first_search_for_phrase(doc_fit, phrase, expected_color=expected_color, page_limit=3)
                    if cf:
                        pidx, text_cf, hex_cf, rgb_cf, page_bbox_cf, pix_bbox_cf, *cf_rest = cfuncertain_cf = cf_rest[0] if cf_rest else Falseuncertain = uncertain_cfsampled_rgb = rgb_cfsampled_cat = map_color_to_cat(rgb_cf)
                        sample_img = ''
                        found_rows.append({'method': 'color_first', 'rgb': rgb_cf, 'hex': hex_cf, 'uncertain': uncertain_cf})
                        # render page png for diagnosticstry:
                            out_png = IMG_DIR / f"qa_{candidate.name}_p{page_hit}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png"
                            _render_page_png(candidate, page_hit, out_png)
                            sample_img = str(out_png)
                        except Exception:
                            pass
                        # if the deterministic result matches expectation and is not uncertain, accept earlyif sampled_cat == expected_color and not uncertain_cf:
                            found_in_rows = Truesampled_hex = ('#%02x%02x%02x' % sampled_rgb) if sampled_rgb else ''
                            row_cat = sampled_catcontinueexcept Exception:
                    passout_png = IMG_DIR / f"qa_{candidate.name}_p{page_hit}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png"
                png = _render_page_png(candidate, page_hit, out_png)
                if png:
                    # token-level sampling via tesseract TSVtokens = _tesseract_tokens(png)
                    target = ' '.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
                    best_idx = Nonebest_score = 0for i, t in enumerate(tokens):
                        txt = (t.get('text') or '').strip()
                        if not txt:
                            continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                        score = len(set(norm.split()) & set(target.split()))
                        if score > best_score:
                            best_score = scorebest_idx = imed_left = _sample_near_token(png, tokens[best_idx], side='left') if (best_idx is not None and best_score>0) else Nonemed_right = _sample_near_token(png, tokens[best_idx], side='right') if (best_idx is not None and best_score>0) else Nonemed_line = _sample_left_of_line(png, tokens, best_idx) if (best_idx is not None and best_score>0) else Nonemed_band = _sample_left_band_median_from_png(png)
                    med_cluster = _sample_left_cluster_full(png, target_y=(int(tokens[best_idx].get('top',0)) if best_idx is not None else None))
                    # choose best med by simple confidence heuristicsdef _med_confidence(m):
                        if m is None:
                            return 0.0name = map_color_to_cat(m)
                        # If we have an EXPECTED color for this phrase, prefer candidates
                        # that match that expected category even if their base score is lower.
                        if expected_color and name == expected_color:
                            return 6.0
                        # also prefer candidates that are reasonably close to the expected canonical
                        # (loosened threshold to accept pale/anti-aliased markers that are farther in euclidean
                        # distance but still visually the expected color)
                        if expected_color and m is not None:
                            try:
                                dexp = color_distance(m, CANONICAL.get(expected_color))
                                if dexp is not None and dexp < 120000:
                                    return 5.0except Exception:
                                pass
                        # strong confidence if non-neutral (explicit color)
                        if name != 'neutral':
                            return 4.0
                        # otherwise score by saturation and distance to nearest canonical,
                        # but with a boosted path for canonical-proximal low-sat candidatesrf,gf,bf = [x/255.0 for x in m]
                        h,s,v = colorsys.rgb_to_hsv(rf,gf,bf)
                        # distance to best canonicaldists = {n: color_distance(m,c) for n,c in CANONICAL.items()}
                        bestd = min(dists.values())
                        # If close to a canonical color (loose threshold) and some saturation, boostif bestd < 90000 and s > 0.005:
                            return 2.5 * (1.0 / (1 + bestd/30000.0)) + (s * 2.0)
                        # slight green bias (g larger than r,b) is also a useful signalif (m[1] - max(m[0], m[2])) > 8:
                            return 1.5 + (s * 1.0)
                        # fallback scoringreturn (s + 0.02) * (1.0 / (1 + bestd/20000.0)) + (0.5 if bestd < 20000 else 0.0)
                    # include cluster candidate last (it tends to be higher-precision when present)
                    # include the combined span/vector/glyph fallback as an additional candidatemed_combined = Nonetry:
                        combined = combined_sample_color_for_phrase(pathlib.Path(candidate).name and __import__('fitz').open(candidate) if True else None, phrase, expected_color=expected_color)
                    except Exception:
                        combined = Noneif combined:
                        # combined returns (p, line_text, hex, rgb, bbox, method)
                        med_combined = combined[3]
                    # try a direct glyph interior sample from the PDF line bbox (strong candidate)
                    med_glyph_direct = Nonetry:
                        import fitz as _fitzdoc = _fitz.open(candidate)
                        page_obj = doc.load_page(page_hit-1)
                        found_bbox = Nonetd = page_obj.get_text('dict')
                        for bb in td.get('blocks', []):
                            for ln2 in bb.get('lines', []):
                                lt = ''.join([s.get('text','') for s in ln2.get('spans', [])]).strip()
                                if not lt:
                                    continueif phrase.lower() in lt.lower() or any(tok.lower() in phrase.lower() or phrase.lower() in tok.lower() for tok in lt.split()):
                                    found_bbox = ln2.get('bbox')
                                    breakif found_bbox:
                                breakif found_bbox is not None:
                            _hexg, rgbg = sample_glyph_interior_color_near_line(page_obj, found_bbox, zoom=8, gray_pctile=90, erode_iter=1)
                            med_glyph_direct = rgbgexcept Exception:
                        med_glyph_direct = None

                    candidates = [med_left, med_right, med_line, med_band, med_cluster, med_combined, med_glyph_direct]
                    best = Nonebest_score = 0for c in candidates:
                        sc = _med_confidence(c)
                        if sc > best_score:
                            best_score = scbest = cmed = best
                    # Debug: for the specific 1314 PDF, record candidate confidences to help iterating heuristicstry:
                        if 'user_1314' in candidate.name:
                            dbg_file = ROOT / 'data' / 'poc_debug_candidates.csv'
                            write_header = not dbg_file.exists()
                            with open(dbg_file, 'a', newline='') as fhdbg:
                                wdbg = csv.writer(fhdbg)
                                if write_header:
                                    wdbg.writerow(['filename','phrase','candidate_repr','candidate_hex','candidate_cat','confidence'])
                                for c in candidates:
                                    ch = ('#%02x%02x%02x' % c) if c else ''
                                    wdbg.writerow([candidate.name, phrase, str(c), ch, map_color_to_cat(c) if c else '', _med_confidence(c)])
                                # also note chosenwdbg.writerow([candidate.name, phrase, 'chosen', ('#%02x%02x%02x' % med) if med else '', map_color_to_cat(med) if med else '', '---'])
                    except Exception:
                        passif med:
                        sampled_rgb = medsample_img = str(png)
                        sampled_cat = map_color_to_cat(med)
                        # If the chosen candidate is neutral, attempt a diagnostic canonical
                        # detection on the left-of-line crop to recover pale tints.
                        if sampled_cat == 'neutral':
                            try:
                                diag_crop = Noneif page_hit and png:
                                    tokens2 = _tesseract_tokens(png)
                                    # find token most similar to the phrase againbest_idx2 = Nonebest_score2 = 0target2 = ' '.join(ch.lower() for ch in phrase if ch.isalnum() or ch.isspace()).strip()
                                    for i, t in enumerate(tokens2):
                                        txt = (t.get('text') or '').strip()
                                        if not txt: continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                                        score = len(set(norm.split()) & set(target2.split()))
                                        if norm in target2 or target2 in norm:
                                            score += 2if score and score > best_score2:
                                            best_score2 = scorebest_idx2 = iif best_idx2 is not None and best_score2 > 0:
                                        ref = tokens2[best_idx2]
                                        top_ref = int(ref.get('top', 0))
                                        near = [t for t in tokens2 if abs(int(t.get('top',0)) - top_ref) <= 8]
                                        if near:
                                            min_left = min(int(t.get('left',0)) for t in near)
                                            cy = top_ref + int(near[0].get('height',0)) // 2x0 = 0x1 = min(png.size[0], min_left + 40)
                                            y0 = max(0, cy - 12)
                                            y1 = min(png.size[1], cy + 12)
                                            diag_crop = Image.open(str(png)).crop((x0,y0,x1,y1)).convert('RGB')
                                if diag_crop is None and png:
                                    diag_crop = Image.open(str(png)).convert('RGB').crop((0,int(png.size[1]*0.15), int(png.size[0]*0.25), int(png.size[1]*0.85)))
                                if diag_crop is not None:
                                    arr = np.array(diag_crop).reshape(-1,3)
                                    # compute candidate pixel metricscands = []
                                    for pix in arr:
                                        r,g,b = map(int,pix)
                                        h,s,v = colorsys.rgb_to_hsv(r/255.0,g/255.0,b/255.0)
                                        hue = int(h*360)
                                        sat = float(s)
                                        dists = {n: color_distance((r,g,b),c) for n,c in CANONICAL.items()}
                                        bestd = min(dists.values())
                                        cands.append({'rgb':(r,g,b),'hex':('#%02x%02x%02x' % (r,g,b)),'hue':hue,'sat':sat,'bestd':bestd})
                                    # take top 6 by combined scorefor cc in sorted(cands, key=lambda x: (-(x['sat']*1.0 + (max(0, (40000.0 - x['bestd'])/40000.0))*6.0)))[:6]:
                                        override_rows.append({'filename': candidate.name, 'source': str(candidate), 'record_id': re.search(r'user_(\d+)_', candidate.name).group(1) if re.search(r'user_(\d+)_', candidate.name) else candidate.name, 'line_text': phrase, 'canonical': map_line_to_canonical(phrase), 'color_hex': cc['hex'], 'color_cat': map_color_to_cat(cc['rgb']), 'color_method': 'diag'})
                            except Exception:
                                pass
            # if still neutral, try horizontal band scan to catch faint colored markersif cat == 'neutral':
                try:
                    hex3, rgb3 = None, Nonedef band_scan_color(page, line_text, img_path):
                        pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
                        target = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                            t = (txt or '').strip()
                            if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                            score = len(set(norm.split()) & set(target.split()))
                            if score > best_score:
                                best_score = scorebest_y = data['top'][i]
                        w,h = img.sizeif best_y is None:
                            band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                            arr = np.array(band).reshape(-1,3)
                            nonwhite = arr[(arr.sum(axis=1) < 740)]
                            if nonwhite.size:
                                med = tuple(map(int, np.median(nonwhite, axis=0)))
                                if out_img_path:
                                    band.save(out_img_path)
                                return medelse:
                            cx = int(w * 0.12)
                            cy = int(best_y + 8)
                            med = median_5x5(img, cx, cy)
                            if med:
                                if out_img_path:
                                    small = img.crop((max(0,cx-20), max(0,cy-20), min(w, cx+20), min(h, cy+20)))
                                    small.save(out_img_path)
                                return medexcept Exception:
                    pass
            # if still neutral, try horizontal band scan to catch faint colored markersif cat == 'neutral':
                try:
                    hex3, rgb3 = None, Nonedef band_scan_color(page, line_text, img_path):
                        pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
                        target = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                            t = (txt or '').strip()
                            if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                            score = len(set(norm.split()) & set(target.split()))
                            if score > best_score:
                                best_score = scorebest_y = data['top'][i]
                        w,h = img.sizeif best_y is None:
                            band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                        else:
                            band = img.crop((0, max(0,best_y-6), int(w*0.25), min(h, best_y+18)))
                        arr = np.array(band).reshape(-1,3)
                        arr_f = arr.astype('float32')/255.0mx = arr_f.max(axis=1)
                        mn = arr_f.min(axis=1)
                        sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
                        val = mxmask = (sat > 0.06) & (val > 0.05)
                        if mask.sum() > 8:
                            sel = arr[mask]
                            med = tuple(map(int, np.median(sel, axis=0)))
                            return rgb_to_hex_tuple(med), med
                        # fallback: median of non-white pixelsnonwhite = arr[(arr.sum(axis=1) < 740)]
                        if nonwhite.size:
                            med = tuple(map(int, np.median(nonwhite, axis=0)))
                            return rgb_to_hex_tuple(med), medreturn None, Nonehex3, rgb3 = band_scan_color(page, ln, IMG_DIR / f"{fname}.p{page_idx}.band.png")
                    if hex3 and rgb3:
                        color_hex, rgb = hex3, rgb3method = 'band'
                        cat = map_color_to_cat(rgb)
                except Exception:
                    passcolor_counts[cat] = color_counts.get(cat,0) + 1canonical = map_line_to_canonical(ln)
            # If we have an expectation for this filename and this line, try targeted
            # canonical/cluster/global searches to recover pale/anti-aliased markerstry:
                exp_for_file = EXPECT.get(fname, {}) if 'EXPECT' in locals() else {}
                expected_color = Nonefor ep, ec in exp_for_file.items():
                    # match by normalized/canonicalized phraseif canonicalize(ep) == canonicalize(ln) or canonicalize(ep) in canonicalize(ln) or canonicalize(ln) in canonicalize(ep):
                        expected_color = ecbreakif cat == 'neutral' and expected_color:
                    # render page PNG if not alreadytry:
                        page = doc.load_page(page_idx)
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"qa_{fname}.p{page_idx}_{re.sub('[^a-z0-9]','_',ln.lower())[:40]}.png")
                    except Exception:
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"{fname}.p{page_idx}.png")
                    # tokenizationtokens = _tesseract_tokens(png)
                    # find best token for phrasebest_idx = None; best_score = 0target = ' '.join(ch.lower() for ch in ln if ch.isalnum() or ch.isspace()).strip()
                    for i,t in enumerate(tokens):
                        txt = (t.get('text') or '').strip()
                        if not txt: continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                        score = len(set(norm.split()) & set(target.split()))
                        if score > best_score:
                            best_score = score; best_idx = i
                    # try canonical near linetry_can = _find_canonical_near_line(png, tokens, best_idx, expected_color) if expected_color else Noneif try_can is not None:
                        color_hex = '#%02x%02x%02x' % try_canrgb = try_cancat = expected_color if map_color_to_cat(try_can) == expected_color or color_distance(try_can, CANONICAL.get(expected_color)) < 120000 else map_color_to_cat(try_can)
                        method = 'canon_search'
                    # aggressive green cluster & left-strip clustering are marker-oriented; only run if markers enabledif markers_enabled() and cat == 'neutral' and expected_color == 'green':
                        ag = _aggressive_green_search(png, tokens, best_idx)
                        if ag is not None and map_color_to_cat(ag) == 'green':
                            color_hex = '#%02x%02x%02x' % agrgb = agcat = 'green'
                            method = 'aggr_green'
                        # left-strip clustering (green)
                        clusters = _find_color_clusters_in_left_strip(png, color='green')
                        if clusters and best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2best_cluster = min(clusters, key=lambda c: abs(c[0] - token_cy))
                            if abs(best_cluster[0] - token_cy) <= 40 and best_cluster[3] >= 4:
                                color_hex = '#%02x%02x%02x' % best_cluster[2]
                                rgb = best_cluster[2]
                                cat = 'green'
                                method = 'strip_cluster'
                    # final global canonical searchif cat == 'neutral' and expected_color:
                        token_cy = Noneif best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2g = _global_canonical_search(png, token_cy, expected_color)
                        if g is not None:
                            color_hex = '#%02x%02x%02x' % grgb = gcat = map_color_to_cat(g)
                            method = 'global_canon'
            except Exception:
                pass
            # If canonical is not one of our known keys, keep as "other"
            known_keys = set(k for _,k in MAPPING_RULES)
            if canonical not in known_keys:
                other_factors.append((canonical, cat))
            else:
                # store highest-severity color if multiple instances of same factor occur (prefer red>amber>green>black)
                prev = factor_map.get(canonical, 0)
                code = COLOR_CODE.get(cat, 0)
                if code > prev:
                    factor_map[canonical] = code
            # only append row_records for known canonical factors or non-neutral colorsif canonical in known_keys or cat != 'neutral' or '$' in ln or 'inq' in ln.lower():
                row_records.append({
                    'filename': fname,
                    'source': src,
                    'record_id': record_id,
                    'line_text': ln_proc,
                    'canonical': canonical,
                    'color_hex': color_hex or '',
                    'color_cat': cat,
                    'color_method': method,
                })
        # targeted search for expected phrases (patch missing factors called out in QA and expectations)
        targeted_phrases = set(targeted.get(fname, []) if isinstance(targeted, dict) else targeted) if targeted else set()
        # include EXPECT phrases if presenttry:
            exp_phrases = set(EXPECT.get(fname, {}).keys())
        except Exception:
            exp_phrases = set()
        targeted_phrases.update(exp_phrases)
        for phrase in targeted_phrases:
            if phrase.lower() in full_text.lower():
                # find page containing the phrase and sample colorfor p in range(len(doc)):
                    ptext = doc.load_page(p).get_text()
                    if phrase.lower() in ptext.lower():
                        hex3, rgb3 = None, Nonetry:
                            med = band_scan_color_for_phrase(doc, phrase, out_img_path=IMG_DIR / f"patch_{fname}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png")
                            if med is not None:
                                hex3 = '#%02x%02x%02x' % medrgb3 = medexcept Exception:
                            hex3, rgb3 = None, Nonecat = map_color_to_cat(rgb3) if rgb3 else 'neutral'
                        canonical = map_line_to_canonical(phrase)
                        # ensure we record even if canonical unknown (we'll add to other_factors)
                        if canonical in set(k for _,k in MAPPING_RULES):
                            factor_map[canonical] = max(factor_map.get(canonical,0), COLOR_CODE.get(cat,0))
                        else:
                            other_factors.append((canonical, cat))
                        row_records.append({
                            'filename': fname,
                            'source': src,
                            'record_id': record_id,
                            'line_text': phrase,
                            'canonical': canonical,
                            'color_hex': hex3 or '',
                            'color_cat': cat,
                            'color_method': 'patch',
                        })
                        break
        # sample record-level colors for important fields (credit score, payments, etc.) when availabledef sample_label_color(doc, label_regex_list):
            # look for any label in the document and return sampled RGB using band_scan_color_for_phrasefor label in label_regex_list:
                if re.search(label, full_text, re.I):
                    med = band_scan_color_for_phrase(doc, label, out_img_path=IMG_DIR / f"label_{fname}_{re.sub('[^a-z0-9]','_',label)[:40]}.png")
                    if med:
                        return medreturn None

        # compute wide rowwide_row = {'filename': fname, 'source': src, 'record_id': record_id}
        # default factor columns (only known canonical keys)
        for k,v in factor_map.items():
            wide_row[k] = vwide_row['other_factors'] = ';'.join([f"{k}:{c}" for k,c in other_factors[:80]])
        # counts per colorwide_row['count_red'] = color_counts.get('red',0)
        wide_row['count_amber'] = color_counts.get('amber',0)
        wide_row['count_green'] = color_counts.get('green',0)
        wide_row['count_black'] = color_counts.get('black',0)
        # record-level numeric fieldswide_row['credit_score'] = rec.get('credit_score','')
        wide_row['age'] = rec.get('age','')
        # attempt to sample credit score colorcs_med = sample_label_color(doc, ['Credit Score', r'\b\d{3}\b'])
        if cs_med:
            wide_row['credit_score_color_hex'] = rgb_to_hex_tuple(cs_med)
            wide_row['credit_score_color_cat'] = map_color_to_cat(cs_med)
        else:
            wide_row['credit_score_color_hex'] = ''
            wide_row['credit_score_color_cat'] = ''
        # payment samplingpay_med = sample_label_color(doc, ['Payments?\b', 'Payment\b'])
        if pay_med:
            wide_row['payment_color_hex'] = rgb_to_hex_tuple(pay_med)
            wide_row['payment_color_cat'] = map_color_to_cat(pay_med)
        else:
            wide_row['payment_color_hex'] = ''
            wide_row['payment_color_cat'] = ''
        # record-level numeric fields parsed earlierwide_row['monthly_payment'] = rec.get('monthly_payment','')
        wide_row['credit_freeze'] = rec.get('credit_freeze',0)
        wide_row['fraud_alert'] = rec.get('fraud_alert',0)
        wide_row['deceased'] = rec.get('deceased',0)
        wide_row['address'] = rec.get('address','')
        wide_row['revolving_open_count'] = rec.get('revolving_open_count','')
        wide_row['revolving_open_total'] = rec.get('revolving_open_total','')
        wide_row['installment_open_count'] = rec.get('installment_open_count','')
        wide_row['installment_open_total'] = rec.get('installment_open_total','')
        wide_row['collections_open'] = rec.get('collections_open','')
        wide_row['collections_closed'] = rec.get('collections_closed','')
        wide_row['inquiries_6mo'] = rec.get('inquiries_6mo','')
        wide_row['late_pays_recent'] = rec.get('late_pays_recent','')
        wide_row['late_pays_prior'] = rec.get('late_pays_prior','')        # extract open/closed counts heuristicsm_open_closed = re.search(r"(\d+) open.*?(\d+) closed", full_text, re.I|re.S)
        if m_open_closed:
            wide_row['num_open'] = int(m_open_closed.group(1))
            wide_row['num_closed'] = int(m_open_closed.group(2))
        # quick collection countswide_row['has_collections'] = 1 if 'collection' in full_text.lower() else 0
# extract open/closed collection countsm_open_col = re.search(r"(\d+) open.*collection", full_text, re.I)
    if m_open_col:
        wide_row['num_open_collections'] = int(m_open_col.group(1))
    else:
        wide_row['num_open_collections'] = len(re.findall(r'active collection|open collection', full_text, re.I))
    m_closed_col = re.search(r"(\d+) closed.*collection", full_text, re.I)
    if m_closed_col:
        wide_row['num_closed_collections'] = int(m_closed_col.group(1))
    else:
        # heuristic: count occurrences of 'unpaid collection' or 'paid collection'
        wide_row['num_closed_collections'] = len(re.findall(r'unpaid collection|paid collection|collection', full_text, re.I))
    wide_row['num_collections'] = wide_row['num_open_collections'] + wide_row['num_closed_collections']
    wide_row['has_collections'] = 1 if wide_row['num_collections']>0 else 0

    wide_row['rev_balance_total'] = Nonewide_row['installment_balance_total'] = Nonewide_row['rev_limit_total'] = Nonewide_row['monthly_income'] = Nonewide_row['monthly_payment'] = None
    # try to parse balances and incomesm_rev = re.search(r"rev.*?balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_rev:
        wide_row['rev_balance_total'] = float(m_rev.group(1).replace(',',''))
    m_inst = re.search(r"installment.*balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_inst:
        wide_row['installment_balance_total'] = float(m_inst.group(1).replace(',',''))
    m_lim = re.search(r"limit.*?\$?([0-9,]+)", full_text, re.I)
    if m_lim:
        wide_row['rev_limit_total'] = float(m_lim.group(1).replace(',',''))
    # try to parse monthly income / monthly paymentsm_income = re.search(r"\$(\d{1,3}(?:,\d{3})*)(?:\s*/\s*mo|/mo| per month)", full_text, re.I)
    if m_income:
        wide_row['monthly_income'] = float(m_income.group(1).replace(',',''))
    # monthly payment found as 'Pay $1234/mo' or 'Payments $x/mo'
    m_pay = re.search(r"pay\s*\$?([0-9,]+)\s*/\s*mo|payments\s*\$?([0-9,]+)\s*/\s*mo", full_text, re.I)
    if m_pay:
        num = m_pay.group(1) or m_pay.group(2)
        if num:
            wide_row['monthly_payment'] = float(num.replace(',',''))
    # compute derived ratiosif wide_row['rev_balance_total'] and wide_row.get('rev_limit_total'):
        try:
            wide_row['rev_utilization'] = wide_row['rev_balance_total'] / wide_row['rev_limit_total']
        except Exception:
            wide_row['rev_utilization'] = Noneelse:
        wide_row['rev_utilization'] = Noneif wide_row['installment_balance_total'] and wide_row.get('monthly_income'):
        try:
            wide_row['installment_balance_to_income'] = wide_row['installment_balance_total'] / (wide_row['monthly_income'] * 12)
        except Exception:
            wide_row['installment_balance_to_income'] = Noneelse:
        wide_row['installment_balance_to_income'] = Noneif wide_row.get('monthly_payment') and wide_row.get('monthly_income'):
        try:
            wide_row['monthly_payment_to_income'] = wide_row['monthly_payment'] / wide_row['monthly_income']
        except Exception:
            wide_row['monthly_payment_to_income'] = Noneelse:
        wide_row['monthly_payment_to_income'] = None
    # public records: parse explicit counts and capture notes (e.g., bankruptcy)
    pr_count, pr_note = parse_public_records(full_text)
    wide_row['public_records'] = pr_countwide_row['public_record_note'] = pr_note
    # num_90d_plus_latesm_90 = re.findall(r'(\d+)\s*\b90d\b|90\+\s*d', full_text, re.I)
    wide_row['num_90d_plus_lates'] = sum(int(x) for x in re.findall(r'(\d+)\s*90\+|90d', full_text, re.I) if x.isdigit()) if re.findall(r'(\d+)\s*90', full_text, re.I) else 0wide_row['num_90d_plus_lates'] = wide_row.get('num_90d_plus_lates', 0) + (1 if '90d' in full_text and 'late' in full_text else 0)
    # other boolean heuristics (derived from counts when possible)
    out['has_public_record'] = 1 if out.get('public_records',0) > 0 or out.get('public_record_note') else 0out['has_collections'] = 1 if (out.get('collections_open',0) + out.get('collections_closed',0)) > 0 else 0
    # if a doc is provided, try to extract table-like numeric fields (installment/revolving) from page text and fall back to OCR if neededtry:
        if doc is not None:
            prev_ln = ''
            for p in range(len(doc)):
                td = doc.load_page(p).get_text('dict')
                for b in td.get('blocks', []):
                    for ln in b.get('lines', []):
                        ln_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                        if not ln_text:
                            continuecombined = (prev_ln + ' ' + ln_text).strip()
                        try:
                            # candidate matches on prev/current/combinedcnt_comb, amt_comb = parse_count_amount_pair(combined)
                            cnt_line, amt_line = parse_count_amount_pair(ln_text)
                            cnt_prev, amt_prev = parse_count_amount_pair(prev_ln)

                            def is_install(s):
                                return 'install' in (s or '').lower()
                            def is_revolv(s):
                                s = (s or '').lower()
                                return 'revolv' in s or 'rev' in s or 'revolving' in s or 'revolving' in sdef is_loc(s):
                                s = (s or '').lower()
                                return 'line of credit' in s or 'line of' in s

                            assigned = False
                            # Prefer numeric directly on the line and attach to previous label (common pattern)
                            if cnt_line is not None or amt_line is not None:
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                        out['installment_open_count'] = int(cnt_line)
                                    if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                        out['installment_open_total'] = int(amt_line)
                                    assigned = Trueelif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                        out['revolving_open_count'] = int(cnt_line)
                                    if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                        out['revolving_open_total'] = int(amt_line)
                                    assigned = True
                                # if not attached to prev, attach to current if its label matchesif not assigned:
                                    if is_install(ln_text):
                                        if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                            out['installment_open_count'] = int(cnt_line)
                                        if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                            out['installment_open_total'] = int(amt_line)
                                        assigned = Trueelif is_revolv(ln_text) or is_loc(ln_text):
                                        if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                            out['revolving_open_count'] = int(cnt_line)
                                        if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                            out['revolving_open_total'] = int(amt_line)
                                        assigned = True
                            # else consider prev numeric or combinedif (not assigned) and (cnt_prev is not None or amt_prev is not None):
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_prev is not None:
                                        out['installment_open_count'] = int(cnt_prev)
                                    if out.get('installment_open_total') in (None,'') and amt_prev is not None:
                                        out['installment_open_total'] = int(amt_prev)
                                elif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_prev is not None:
                                        out['revolving_open_count'] = int(cnt_prev)
                                    if out.get('revolving_open_total') in (None,'') and amt_prev is not None:
                                        out['revolving_open_total'] = int(amt_prev)
                                assigned = True
                            # fallback: combined detection but prefer prev label if both have labelsif (not assigned) and (cnt_comb is not None or amt_comb is not None):
                                if is_install(prev_ln) or (is_install(ln_text) and not is_revolv(prev_ln) and not is_loc(prev_ln)):
                                    if out.get('installment_open_count') in (None,'') and cnt_comb is not None:
                                        out['installment_open_count'] = int(cnt_comb)
                                    if out.get('installment_open_total') in (None,'') and amt_comb is not None:
                                        out['installment_open_total'] = int(amt_comb)
                                elif is_revolv(prev_ln) or is_loc(prev_ln) or is_revolv(ln_text) or is_loc(ln_text):
                                    if out.get('revolving_open_count') in (None,'') and cnt_comb is not None:
                                        out['revolving_open_count'] = int(cnt_comb)
                                    if out.get('revolving_open_total') in (None,'') and amt_comb is not None:
                                        out['revolving_open_total'] = int(amt_comb)
                        except Exception:
                            pass
                        # extra parsing: detect inquiries, collections, late pays, and payments from nearby table lineslow_ln = ln_text.lower()
                        # Inquiries (Last 6 Months): single number often on its own lineif 'inquiri' in low_ln or 'inquir' in low_ln or 'inquires' in low_ln:
                            # avoid capturing the timeframe 'Last 6 Months' (6) - prefer an adjacent actual countif not re.search(r"last\s*\d+\s*months?", ln_text, re.I):
                                m_in = re.search(r"(\d+)", ln_text)
                                if m_in:
                                    out['inquiries_6mo'] = int(m_in.group(1))
                                    # proceed to next line if this looks like the timeframe instead
                                    # else try previous/next numericelse:
                                # try previous or following numeric; previous may contain a numberm_prev = re.search(r"(\d+)", prev_ln)
                                if m_prev:
                                    out['inquiries_6mo'] = int(m_prev.group(1))
                                else:
                                    # try to find the number on the following line using combined detectionif cnt_line is not None:
                                        out['inquiries_6mo'] = int(cnt_line)
                        # Collections (Open/Closed): '0 / 0' may be on same or next lineif 'collections' in low_ln:
                            m_col = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_col:
                                out['collections_open'] = int(m_col.group(1))
                                out['collections_closed'] = int(m_col.group(2))
                            else:
                                # try next/prev line via combinedif cnt_line is not None and amt_line is not None:
                                    out['collections_open'] = int(cnt_line)
                                    out['collections_closed'] = int(amt_line)
                        # Late Pays (Last 2/2+ Years) -> look for '7 / 0' pair on same or next lineif 'late pays' in low_ln:
                            m_lp = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_lp:
                                out['late_pays_recent'] = int(m_lp.group(1))
                                out['late_pays_prior'] = int(m_lp.group(2))
                            elif cnt_line is not None and amt_line is not None:
                                out['late_pays_recent'] = int(cnt_line)
                                out['late_pays_prior'] = int(amt_line)
                        # Payments: look for dollar amount on same or adjacent linesif 'payments' in low_ln or 'payment' in low_ln:
                            m_pay = re.search(r"\$?\s*([0-9,]+)\s*(?:/mo|mo)?", ln_text, re.I)
                            if m_pay:
                                out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
                            else:
                                # check previous/nextif cnt_line is not None:
                                    out['monthly_payment'] = int(cnt_line)
                                else:
                                    m_prev = re.search(r"\$?\s*([0-9,]+)", prev_ln)
                                    if m_prev:
                                        out['monthly_payment'] = int(m_prev.group(1).replace(',',''))
                        prev_ln = ln_text
                # if still missing, try OCR on rendered pageif (out.get('installment_open_count') in (None,'') or out.get('installment_open_total') in (None,'')) or (out.get('revolving_open_count') in (None,'') or out.get('revolving_open_total') in (None,'')):
                    try:
                        import pytesseractfrom PIL import Imagepix = doc.load_page(p).get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        ocr_text = pytesseract.image_to_string(img)
                        for oline in ocr_text.splitlines():
                            if not oline.strip():
                                continuecnt, amt = parse_count_amount_pair(oline)
                            if (cnt is not None or amt is not None):
                                ln_low = oline.lower()
                                # assign to installment if header contains 'install'
                                if 'install' in ln_low:
                                    if out.get('installment_open_count') in (None,'') and cnt is not None:
                                        out['installment_open_count'] = int(cnt)
                                    if out.get('installment_open_total') in (None,'') and amt is not None:
                                        out['installment_open_total'] = int(amt)
                                # assign to revolving if header contains 'revolv' or 'rev'
                                if 'revolv' in ln_low or 'rev' in ln_low:
                                    if out.get('revolving_open_count') in (None,'') and cnt is not None:
                                        out['revolving_open_count'] = int(cnt)
                                    if out.get('revolving_open_total') in (None,'') and amt is not None:
                                        out['revolving_open_total'] = int(amt)
                    except Exception:
                        pass
            # finally, populate credit factors using the right-column extractortry:
                out['credit_factors'] = extract_credit_factors_from_doc(doc)
            except Exception:
                out['credit_factors'] = out.get('credit_factors', [])
            # post-page scan heuristics: if we still don't have inquiries or late pays, try page-text searchestry:
                page_text = '\n'.join([doc.load_page(i).get_text() for i in range(len(doc))])
                # Inquiries: find the label and then the numeric value that is NOT part of 'Last 6 Months'
                if out.get('inquiries_6mo') in (None,''):
                    for m in re.finditer(r"Inquir(?:y|ies|es)?", page_text, re.I):
                        # scan the following lines for a line with a standalone number (skip 'Last 6 Months' in parentheses)
                        snippet = page_text[m.end(): m.end()+300]
                        for ln in snippet.splitlines():
                            ln_strip = ln.strip()
                            if not ln_strip:
                                continueif re.search(r"months?|mo", ln_strip, re.I):
                                # skip lines that are just the label with 'Last 6 Months'
                                continuemnum = re.search(r"\b(\d{1,3})\b", ln_strip)
                            if mnum:
                                out['inquiries_6mo'] = int(mnum.group(1))
                                breakif out.get('inquiries_6mo') not in (None,''):
                            break
                # Late Pays: find a numeric 'x / y' following the label
                # prefer pairs on the following line to avoid matching the '(Last 2/2+ Years)' parentheticalfound = Falsefor m in re.finditer(r"Late Pays", page_text, re.I):
                    snippet = page_text[m.end(): m.end()+500]
                    for mp in re.finditer(r"(\d+)\s*/\s*(\d+)", snippet):
                        pair = mp.group(0)
                        pair_pos = snippet.find(pair)
                        # accept only if the pair occurs after a newline (i.e., likely on the next line)
                        if '\n' in snippet[:pair_pos]:
                            out['late_pays_recent'] = int(mp.group(1))
                            out['late_pays_prior'] = int(mp.group(2))
                            found = Truebreakif found:
                        break
                # Credit Card Open Totals (No Retail): inspect account table blocks and consider their final 'total' linetry:
                    cc_found = Nonefor p in range(len(doc)):
                        td = doc.load_page(p).get_text('dict')
                        blocks = td.get('blocks', [])
                        for bi, b in enumerate(blocks):
                            block_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b.get('lines', [])])
                            low_block = block_text.lower()
                            # Heuristic: block that contains 'revolving accounts' or 'revolving accounts (open)' likely precedes credit card totalsif 'revolving accounts' not in low_block and 'revolving accounts (open)' not in low_block:
                                continue
                            # scan following blocks for a block containing 'total' (e.g., 'Credit Card Open Totals')
                            total_block = Nonefor j in range(bi+1, min(bi+30, len(blocks))):
                                b2 = blocks[j]
                                b2_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b2.get('lines', [])])
                                if 'total' in b2_text.lower() or 'credit card open totals' in b2_text.lower():
                                    total_block = b2breakif not total_block:
                                continue
                            # get the last line that contains 'total' or the first line of total_blocktotal_lines = [ln for ln in total_block.get('lines', []) if 'total' in ''.join([s.get('text','') for s in ln.get('spans', [])]).lower()]
                            if total_lines:
                                ln = total_lines[-1]
                            else:
                                # fallback to first meaningful lineln = total_block.get('lines', [])[0]
                            line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                            block_text_full = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in total_block.get('lines', [])])
                            # try to parse numeric totals from the block lines (balance, limit+util, payment)
                            bal = lim = util = pay = None
                            # look for dollar amounts and percent in block lineslines = [lt.strip() for lt in block_text_full.splitlines() if lt.strip()]
                            # simple heuristics based on observed layoutamounts = []
                            perc = Nonefor ln_txt in lines:
                                m_amt = re.search(r"\$([0-9,]+)", ln_txt)
                                if m_amt:
                                    amounts.append(int(m_amt.group(1).replace(',', '')))
                                m_perc = re.search(r"([0-9]{1,3})%", ln_txt)
                                if m_perc:
                                    perc = int(m_perc.group(1))
                            if amounts:
                                bal = amounts[0]
                                if len(amounts) >= 2:
                                    lim = amounts[1]
                                if len(amounts) >= 3:
                                    pay = amounts[2]
                                util = perc
                            # determine color: prefer span color of the label line then convolution fallbackhexv, rgb = span_color_hex(ln.get('spans', []))
                            cat = Noneif rgb:
                                cat = map_color_to_cat(rgb)
                            else:
                                try:
                                    clf = classify_snippet_color_convolution(doc.load_page(p), ln.get('bbox'))
                                    cat = clf.get('cat')
                                except Exception:
                                    cat = None
                            # include only if redif cat == 'red':
                                cc_found = {'color': 'red', 'balance': bal, 'limit': lim, 'utilization_percent': util, 'payment': pay}
                                breakif cc_found:
                            breakif cc_found:
                        out['credit_card_open_totals_no_retail'] = cc_foundexcept Exception:
                    pass
                # if not found, do not fallback to parenthetical pair (avoid capturing '2/2' from label)
                # monthly payment: fallback to first $NNN/mo on the page if not foundif out.get('monthly_payment') in (None,''):
                    m_pay = re.search(r"\$\s*([0-9,]{1,7})\s*/\s*mo", page_text, re.I)
                    if m_pay:
                        out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
            except Exception:
                passexcept Exception:
        pass
    # ensure numeric fields are present as empty values if not foundout['installment_open_count'] = out.get('installment_open_count', '')
    out['installment_open_total'] = out.get('installment_open_total', '')
    out['revolving_open_count'] = out.get('revolving_open_count', '')
    out['revolving_open_total'] = out.get('revolving_open_total', '')
    out['inquiries_6mo'] = out.get('inquiries_6mo', '')
    out['late_pays_recent'] = out.get('late_pays_recent', '')
    out['late_pays_prior'] = out.get('late_pays_prior', '')
    out['monthly_payment'] = out.get('monthly_payment', '')
    return out


def canonicalize(s):
    s = s.lower()
    # remove money amounts and standalone numbers (they will be parsed separately)
    s = re.sub(r"\$[0-9,]+", " ", s)
    s = re.sub(r"^\s*\d+\s+", " ", s)
    # keep numeric ranges (e.g., '4-6', '6-12') so mapping rules can distinguish bucketss = re.sub(r"[^a-z0-9\s\+\-]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def parse_public_records(full_text: str):
    """Parse public records count and note from a block of text.
    Returns (count:int, note:str). If a note like 'bankrupt' exists but no explicitnumeric count is found, we assume count==1.
    """
    import rem_pr = re.search(r"Public\s+Records\s*[:\s]*([0-9]+)", full_text, re.I)
    if not m_pr:
        m_pr = re.search(r"Public\s+Records\s*\n\s*([0-9]+)", full_text, re.I)
    count = int(m_pr.group(1)) if m_pr else 0
    # capture bankruptcy/other public-record notesm_note = re.search(r"(bankrupt(?:cy)?|bk\b|bankruptcy discharged|ch-\d+|chapter\s*\d+).*", full_text, re.I)
    note = m_note.group(0).strip() if m_note else ''
    if count == 0 and note:
        count = 1return count, note

# mapping rules (regex -> canonical key)
MAPPING_RULES = [
    (r"charged off", "charged_off_accts"),
    (r"charged off rev", "charged_off_rev_accts"),
    (r"over limit", "over_limit_accts"),
    (r"unpaid collection", "unpaid_collections"),
    (r"re lates in 0-3 mo", "re_lates_0_3_mo"),
    (r"re lates in 4-6 mo", "re_lates_4_6_mo"),
    (r"re lates in 6-12 mo", "re_lates_6_12_mo"),
    (r"re lates in 2-4 yrs", "re_lates_2_4_yrs"),
    (r"rev late in 0-3 mo", "rev_late_0_3_mo"),
    (r"rev late in 2-4 yrs", "rev_late_2_4_yrs"),
    (r"avg age open", "avg_age_open"),
    (r"no 5k|no 5k\+|no 5k\s*\+", "no_5k_plus_lines"),
    (r"no closed rev depth", "no_closed_rev_depth"),
    (r"ok open rev depth", "ok_open_rev_depth"),
    (r"3\+ closed rev accnts|3 \+ closed rev accnts|3 closed rev", "three_plus_closed_rev_accnts"),
    (r"closed accnts over 5k", "closed_accnts_over_5k"),
    (r"inq|inquiry|inquiries", "inquiry"),
    (r"no open mortgage", "no_open_mortgage"),
    (r"no rev acct open 10k 2yr|no rev acct open 10k", "no_rev_acct_open_10k_2yr"),
    (r"ok open rev depth", "ok_open_rev_depth"),
    (r"past due not late", "past_due_not_late"),
    (r"current lates", "current_lates"),
    (r"avg age open", "avg_age_open"),
    (r"inq last 2|inq last 4|total inq", "inquiry"),
    (r"no open mortgage", "no_open_mortgage"),
    (r"military affiliated", "military_affiliated"),
    (r"seasoned closed accounts", "seasoned_closed_accounts"),
    (r"closed accnts over 5k", "closed_accnts_over_5k"),
    (r"ok open rev depth", "ok_open_rev_depth"),
    (r"total rev usage", "total_rev_usage"),
    (r"pay \$?[0-9,]+ so accts < 40", "pay_down_to_40pct"),
]


def load_expectations_from_dir(dpath):
    """Load expectation text files from a directory.
    Looks for files with a 'PDF file:' line and a 'Credit Factors:' section. If the explicitfilename referenced by the file is missing, try to find a best-match by user id prefix
    (e.g., 'user_1314_*'). Returns a dict mapping pdf filename -> { phrase: color }.
    """
    out = {}
    d = Path(dpath)
    if not d.exists():
        return outfor p in d.glob('*.txt'):
        cur_pdf = Nonein_cf = Falsewith open(p, 'r', encoding='utf-8') as fh:
            for ln in fh:
                ln = ln.rstrip('\n')
                if ln.lower().startswith('pdf file:'):
                    try:
                        pathpart = ln.split(':',1)[1].strip()
                        candidate = Path(pathpart)
                        cur_pdf = candidate.name
                        # if exact file absent, try to find any file with same user id prefixif not ((ROOT / 'data' / 'pdf_analysis' / cur_pdf).exists() or (PDF_DIR / cur_pdf).exists()):
                            m = re.search(r'user_(\d+)_', cur_pdf)
                            if m:
                                uid = m.group(1)
                                found = Nonefor dsearch in [ROOT / 'data' / 'pdf_analysis', PDF_DIR]:
                                    for f2 in dsearch.glob(f"user_{uid}_*.pdf"):
                                        found = f2breakif found:
                                        breakif found:
                                    cur_pdf = found.nameexcept Exception:
                        cur_pdf = Nonecontinueif ln.strip().lower().startswith('credit factors'):
                    in_cf = Truecontinueif in_cf:
                    m = re.match(r"^\s*[-\u2022]\s*\[(\w+)\]\s*(.+)$", ln)
                    if m and cur_pdf:
                        color = m.group(1).lower()
                        phrase = m.group(2).strip()
                        out.setdefault(cur_pdf, {})[phrase] = colorelse:
                        # stop section on blank line or non-list lineif ln.strip() == '':
                            in_cf = Falsereturn out

COLOR_CODE = {'red':3, 'amber':2, 'green':1, 'black':4, 'neutral':0}


def map_line_to_canonical(line):
    s = canonicalize(line)
    for rx, key in MAPPING_RULES:
        if re.search(rx, s):
            return key
    # fallback: use normalized text as keykey = re.sub(r"\s+", "_", re.sub(r"[^a-z0-9]+", "_", s)).strip('_')
    return key


def parse_money(s):
    s = s.replace(',', '')
    m = re.search(r"\$\s*(\d+(?:\.\d+)?)", s)
    if m:
        return float(m.group(1))
    return None


def parse_count(s):
    m = re.search(r"(\d+)", s)
    if m:
        return int(m.group(1))
    return None


# ---- factor normalization & extraction helpers ----

def normalize_factors(raw_factors):
    """Normalize a flat list of raw factor dicts into simplified factor entries.
    Input: [{'factor': text,'color':cat,'hex':hex}, ...]
    Output: [{'factor':text_with_count,'color':cat}, ...]
    This removes isolated dollar-only tokens, merges category+value pairs, removes noiselike 'Payment Resp' and dedupes by canonical key preferring highest severity color.
    """
    out = []
    i = 0import refrom scripts.poc_extract_credit_factors import parse_count_amount_pairwhile i < len(raw_factors):
        item = raw_factors[i]
        txt = (item.get('factor') or '').strip()
        low = txt.lower()
        # skip pure dollar tokens e.g. '$466'
        if re.fullmatch(r"\$[0-9,]+", txt) or re.fullmatch(r"[0-9,]+", txt):
            i += 1continue
        # skip noise labels entirelyif low in ('payment resp','payment resp.','past due'):
            i += 1continuecanonical = map_line_to_canonical(txt)
        count = Nonetotal = Nonechosen_color = item.get('color')
        chosen_hex = item.get('hex')
        # lookahead for a count/total pair like '3 / $26,491' or '2 / $1,104'
        if i+1 < len(raw_factors):
            nxt = (raw_factors[i+1].get('factor') or '').strip()
            cnt, amt = parse_count_amount_pair(nxt)
            if cnt is not None or amt is not None:
                count = cnttotal = amtif chosen_color in (None,'neutral'):
                    chosen_color = raw_factors[i+1].get('color')
                    chosen_hex = raw_factors[i+1].get('hex')
                i += 2label = txtif count is not None:
                    label = f"{count} {label}"
                out.append({'label': label, 'canonical': canonical, 'count': count, 'total': total, 'color': chosen_color or 'neutral', 'hex': chosen_hex or ''})
                continue
        # If the current text itself encodes count/totalcnt2, amt2 = parse_count_amount_pair(txt)
        if cnt2 is not None or amt2 is not None:
            count = cnt2total = amt2label = txtif count is not None:
                label = f"{count} {label}"
            out.append({'label': label, 'canonical': canonical, 'count': count, 'total': total, 'color': chosen_color or 'neutral', 'hex': chosen_hex or ''})
            i += 1continue
        # handle phrases with leading counts like '4 RE Lates in 4-6 mo' or '1 Rev Late in 0-3 mo'
        m3 = re.match(r"^(\d+)\s+(.+)$", txt)
        if m3 and ('rev' in txt.lower() or 'late' in txt.lower() or 'lates' in txt.lower()):
            count = int(m3.group(1))
            label = m3.group(0).strip()  # keep original including countcanonical = map_line_to_canonical(m3.group(2).strip())
            out.append({'label': label, 'canonical': canonical, 'count': count, 'total': None, 'color': chosen_color or 'neutral', 'hex': chosen_hex or ''})
            i += 1continue
        # default: keep as label-only factorout.append({'label': txt, 'canonical': canonical, 'count': None, 'total': None, 'color': chosen_color or 'neutral', 'hex': chosen_hex or ''})
        i += 1
    # dedupe by canonical (prefer highest severity color: red>amber>green>black>neutral)
    prio = {'red':5,'amber':4,'green':3,'black':2,'neutral':1,None:1}
    merged = {}
    order = []
    for f in out:
        key = f['canonical']
        if key not in merged:
            merged[key] = dict(f)
            order.append(key)
            continueprev = merged[key]
        # prefer counts if presentif prev.get('count') is None and f.get('count') is not None:
            prev['count'] = f['count']
            prev['label'] = f.get('label', prev.get('label'))
        if prev.get('total') is None and f.get('total') is not None:
            prev['total'] = f['total']
        # choose highest severity colorif prio.get(f.get('color'),0) > prio.get(prev.get('color'),0):
            prev['color'] = f.get('color')
            prev['hex'] = f.get('hex')
    # produce simplified output and filter out entries we consider non-factorssimplified = []
    for k in order:
        f = merged[k]
        if not f.get('label'): continue
        # skip 'past due' generic and 'payment resp' if still presentif f['label'].strip().lower() in ('past due','payment resp'):
            continuesimplified.append({'factor': f['label'], 'color': f.get('color') or 'neutral'})
    # apply small label-based color overrides for known semantics (helps match visual expectations)
    overrides = {
        'no closed rev depth': 'red',
        'avg age open': 'red',
        'no 7.5k+ lines': 'red',
        'ok open rev depth': 'green',
        'no open mortgage': 'neutral',
        'no rev acct open 10k 2yr': 'neutral',
        # additional overrides
        'pay $': 'red',
        'total rev usage': 'green',
        'seasoned closed accounts': 'green',
        'military affiliated': 'green',
        # newly added rules to capture negative semantics
        'less than 5 yrs': 'red',
        'less than 5 yr': 'red',
        'too few': 'red',
        'no 3k+': 'red',
        'no 3k': 'red',
        'light open rev depth': 'red',
        'unpaid collection': 'green',
        'unpaid collections': 'red'  # larger unpaid collections often negative
    }
    for sf in simplified:
        low = sf['factor'].lower()
        for k,v in overrides.items():
            if k in low:
                sf['color'] = v
    # ensure leading-count rev-late phrases are marked red, and other re-lates with counts are redfor sf in simplified:
        low = sf['factor'].lower()
        # mark rev lates with any count >=4 as redm = re.match(r"^(\d+)\s+rev(?:o)?\s*late[s]?", low)
        if m:
            try:
                cnt = int(m.group(1))
                if cnt >= 4:
                    sf['color'] = 'red'
            except Exception:
                passif low.startswith('1 rev late') or 'rev late in 0-3' in low:
            sf['color'] = 'red'
        # mark specific rev-late buckets as red (0-3 and 6-12 months should be red per expectations)
        if ('rev late in 0-3' in low) or ('0-3' in low and 'rev' in low):
            sf['color'] = 'red'
        if ('6-12' in low or '6 - 12' in low or '6 to 12' in low) and ('re lates' in low or 'rev' in low):
            sf['color'] = 'red'
    return simplified


def extract_credit_factors_from_doc(doc, page_limit=None):
    """Extract right-column candidate lines from doc and return normalized factors."""
    candidates = []
    page_pivots = {}
    for p in range(len(doc)):
        td = doc.load_page(p).get_text('dict')
        x0s = []
        for b in td.get('blocks', []):
            for ln in b.get('lines', []):
                for s in ln.get('spans', []):
                    bbox = s.get('bbox')
                    if bbox:
                        x0s.append(bbox[0])
        if x0s:
            page_pivots[p] = (min(x0s) + max(x0s)) / 2.0else:
            page_pivots[p] = Nonefor b in td.get('blocks', []):
            for ln in b.get('lines', []):
                text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                if not text: continuespans = ln.get('spans', [])
                x0 = Noneif spans:
                    bbox = spans[0].get('bbox')
                    if bbox:
                        x0 = bbox[0]
                candidates.append((p, text, spans, x0))
    # Merge adjacent dollar lines with following lines when appropriate (e.g., "$4008" + "Unpaid Collections")
    merged_candidates = []
    i = 0while i < len(candidates):
        p_idx, text, spans, x0 = candidates[i]
        txt = text.strip()
        # if this line is just a dollar amount or money+percent and next line looks like collection/collection(s), mergeif re.match(r"^\$[\d,]+(\s*\d+%|\s*%?)$", txt) and i + 1 < len(candidates):
            np_idx, ntext, nspans, nx0 = candidates[i+1]
            if np_idx == p_idx and re.search(r"collection|unpaid|charged|chrgd|charge off", ntext, re.I):
                merged_candidates.append((p_idx, txt + ' ' + ntext, spans + nspans, x0))
                i += 2continuemerged_candidates.append((p_idx, text, spans, x0))
        i += 1

    raw_factors = []
    for p_idx, text, spans, x0 in merged_candidates:
        pivot = page_pivots.get(p_idx)
        if pivot is not None:
            if x0 is None:
                continueif x0 <= pivot - 10:
                continuepage = doc.load_page(p_idx)
        # compute line bbox from spansline_bbox = Nonetry:
            xs = [s.get('bbox')[0] for s in spans if s.get('bbox')]
            ys = [s.get('bbox')[1] for s in spans if s.get('bbox')]
            x1s = [s.get('bbox')[2] for s in spans if s.get('bbox')]
            y1s = [s.get('bbox')[3] for s in spans if s.get('bbox')]
            if xs and ys and x1s and y1s:
                line_bbox = (min(xs), min(ys), max(x1s), max(y1s))
        except Exception:
            line_bbox = None
        # Always use span color as primaryhexv, rgb = span_color_hex(spans)
        # Debug: print detailed span info for the target phraseif text and '3 re lates in 6-12' in text.lower():
            try:
                print('DEBUG SPANS FOR:', text)
                for s in spans:
                    s_text = s.get('text')
                    s_col = s.get('color')
                    print('  span text:', repr(s_text), 'raw color:', s_col)
                    # attempt to normalize span color to rgb for inspectiontry:
                        rgb_span = Noneif isinstance(s_col, (tuple, list)):
                            rgb_span = tuple(int(255*v) if isinstance(v, float) and v <= 1 else int(v) for v in s_col)
                        elif isinstance(s_col, int):
                            rgb_span = ((s_col >> 16) & 255, (s_col >> 8) & 255, s_col & 255)
                        if rgb_span:
                            print('    -> rgb:', rgb_span, 'cat:', map_color_to_cat(rgb_span), 'hex:', rgb_to_hex_tuple(rgb_span))
                    except Exception:
                        passprint('  span_color_hex ->', hexv, rgb, 'mapped cat:', map_color_to_cat(rgb) if rgb else None)
            except Exception:
                pass
        # If span color is missing or ambiguous, use convolution fallbackif rgb is None or (hexv is None and line_bbox is not None):
            try:
                if line_bbox is not None:
                    conv = classify_snippet_color_convolution(page, line_bbox)
                    if text and '3 re lates in 6-12' in text.lower():
                        print('  convolution ->', conv)
                    if conv and conv.get('rgb') is not None:
                        hexv, rgb = conv.get('hex'), conv.get('rgb')
            except Exception:
                passcat = map_color_to_cat(rgb) if rgb else 'neutral'
        # DEBUG: print final mapping for the target phraseif text and '3 re lates in 6-12' in text.lower():
            print('  final hex:', hexv, 'rgb:', rgb, 'cat:', cat)
        raw_factors.append({'factor': text, 'color': cat, 'hex': hexv or ''})
    return normalize_factors(raw_factors)


def _pdf_page_count(pdf_path):
    try:
        res = subprocess.run(['pdfinfo', str(pdf_path)], capture_output=True, text=True, check=True)
        m = re.search(r"Pages:\s*(\d+)", res.stdout)
        return int(m.group(1)) if m else 0except Exception:
        return 0


def _get_page_text(pdf_path, page_num):
    try:
        res = subprocess.run(['pdftotext','-f', str(page_num), '-l', str(page_num), str(pdf_path), '-'], capture_output=True, text=True, check=True)
        return res.stdoutexcept Exception:
        return ''


def _render_page_png(pdf_path, page_num, out_png):
    # use pdftocairo -singlefile to render a single page PNGtry:
        # render at much higher DPI so tiny colored markers become easier to detect
        # use 600 DPI to amplify tiny vector markers for raster samplingsubprocess.run(['pdftocairo', '-r', '600', '-png', '-singlefile', '-f', str(page_num), '-l', str(page_num), str(pdf_path), str(out_png.with_suffix(''))], check=True)
        out_file = out_png.with_suffix('.png')
        return out_file if out_file.exists() else Noneexcept Exception:
        return None


def _tesseract_tokens(png_path):
    """Run tesseract in TSV mode and return list of token dicts with numeric coords."""
    try:
        res = subprocess.run(['tesseract', str(png_path), 'stdout', '--psm', '1', 'tsv'], capture_output=True, text=True, check=True)
        lines = res.stdout.splitlines()
        if not lines:
            return []
        headers = lines[0].split('\t')
        rows = []
        for ln in lines[1:]:
            parts = ln.split('\t')
            if len(parts) < len(headers):
                continued = dict(zip(headers, parts))
            try:
                d['left'] = int(d.get('left') or 0)
                d['top'] = int(d.get('top') or 0)
                d['width'] = int(d.get('width') or 0)
                d['height'] = int(d.get('height') or 0)
            except Exception:
                passrows.append(d)
        return rowsexcept Exception:
        return []


def _detect_canonical_in_pixels(arr):
    """Inspect an Nx3 array of RGB pixels and try to detect a canonical color (green/amber/red/black).
    Returns an RGB tuple if a canonical tint is confidently found, else None."""
    try:
        if arr is None or arr.size == 0:
            return Nonear = arr.reshape(-1,3).astype('int')
        # compute HSV-like metricsarr_f = ar.astype('float32')/255.0mx = arr_f.max(axis=1)
        mn = arr_f.min(axis=1)
        sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
        hues = []
        for i,(r,g,b) in enumerate(ar):
            if sat[i] < 0.01:
                continueh,s,_ = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0)
            hue = h*360hues.append((i,hue,s))
        # choose greens if any obvious green pixels exist
        # accept very low saturation greens as potential markersgreen_idxs = [i for i,h,s in hues if 60 <= h <= 170 and s > 0.005]
        if len(green_idxs) >= 1:
            sel = ar[green_idxs]
            return tuple(map(int, np.median(sel, axis=0)))
        # check closeness to canonical colors (count pixels near canonical)
        for name,canon in CANONICAL.items():
            dists = np.sum((ar - np.array(canon))**2, axis=1)
            # looser threshold to pick up pale tints or anti-aliased pixelscnt = np.sum(dists < 90000)
            if cnt >= 1:
                sel = ar[dists < 90000]
                return tuple(map(int, np.median(sel, axis=0)))
        # as a last resort, if image contains any pixel with slight green bias, pick itbiases = [(i, pix) for i,pix in enumerate(ar) if (pix[1]-pix[0])>4 and (pix[1]-pix[2])>4]
        if biases:
            return tuple(map(int, np.median(np.array([p for _,p in biases]), axis=0)))
        return Noneexcept Exception:
        return None


def _sample_near_token(png_path, token, side='left'):
    """Sample a small neighborhood near a token (left or right side) and return median RGB or None."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizeleft = int(token.get('left',0))
        top = int(token.get('top',0))
        width = int(token.get('width',0))
        height = int(token.get('height',0))
        cy = top + height // 2if side == 'left':
            cx = max(2, left - 8)
        else:
            cx = min(w-3, left + width + 8)
        # sample 11x11 around cx,cyx0 = max(0, cx-5)
        y0 = max(0, cy-5)
        x1 = min(w, cx+6)
        y1 = min(h, cy+6)
        crop = img.crop((x0,y0,x1,y1))
        arr = np.array(crop).reshape(-1,3)
        # first try canonical detection in the croptry_can = _detect_canonical_in_pixels(arr)
        if try_can is not None:
            return try_cannonwhite = arr[(arr.sum(axis=1) < 740)]
        if nonwhite.size:
            # prefer most-saturated pixel if anyarr_f = nonwhite.astype('float32')/255.0mx = arr_f.max(axis=1)
            mn = arr_f.min(axis=1)
            sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
            # also consider closeness to canonical colorsbest_score = -1.0best_pix = Nonefor i,pix in enumerate(nonwhite):
                s = float(sat[i])
                dists = [sum((int(a)-int(b))**2 for a,b in zip(pix, canon)) for canon in CANONICAL.values()]
                bestd = min(dists)
                dist_score = max(0.0, (40000.0 - float(bestd)) / 40000.0)
                score = (s * 1.0) + (dist_score * 4.0)
                if score > best_score:
                    best_score = scorebest_pix = pixif best_pix is not None and best_score > 0.02:
                return tuple(map(int, best_pix))
            return tuple(map(int, np.median(nonwhite, axis=0)))
        return Noneexcept Exception:
        return None


def _sample_left_of_line(png_path, tokens, best_idx):
    """Given tesseract tokens and an index, sample a narrow left-side strip aligned to the token line."""
    try:
        if best_idx is None:
            return None
        # find tokens on same line (close top)
        ref = tokens[best_idx]
        top_ref = int(ref.get('top', 0))
        img = Image.open(str(png_path)).convert('RGB')
        near = [t for t in tokens if abs(int(t.get('top',0)) - top_ref) <= 8]
        if not near:
            return Nonemin_left = min(int(t.get('left',0)) for t in near)
        cy = top_ref + int(near[0].get('height',0)) // 2x0 = 0
        # extend the sampling width to include potential marker columns further leftx1 = min(w, min_left + 40)
        y0 = max(0, cy - 12)
        y1 = min(h, cy + 12)
        crop = img.crop((x0,y0,x1,y1))
        arr = np.array(crop).reshape(-1,3)
        # try blob detection on the crop firsttry:
            a = arr.reshape(-1,3)
            # find candidate green pixels by simple channel differences (allow smaller deltas for pale tints)
            mask_green = (a[:,1].astype(int) - a[:,0].astype(int) > 6) & (a[:,1].astype(int) - a[:,2].astype(int) > 6)
            if mask_green.sum() > 0:
                # prefer dense small clusters rather than the global median to avoid background biash_crop = y1 - y0w_crop = x1 - x0a2 = a.reshape(h_crop, w_crop, 3)
                mask2 = mask_green.reshape(h_crop, w_crop)
                coords = np.argwhere(mask2)
                best_count = 0best_cent = Nonefor yx in coords:
                    y0c, x0c = yxdy = np.abs(coords[:,0] - y0c)
                    dx = np.abs(coords[:,1] - x0c)
                    cnt = np.sum((dy <= 4) & (dx <= 4))
                    if cnt > best_count:
                        best_count = cntbest_cent = (y0c, x0c)
                if best_cent is not None and best_count >= 4:
                    yc, xc = best_centylo = max(0, yc-4); yhi = min(h_crop, yc+5)
                    xlo = max(0, xc-4); xhi = min(w_crop, xc+5)
                    sub = a2[ylo:yhi, xlo:xhi, :].reshape(-1,3)
                    return tuple(map(int, np.median(sub, axis=0)))
                sel = a[mask_green]
                return tuple(map(int, np.median(sel, axis=0)))
            # red candidatemask_red = (a[:,0].astype(int) - a[:,1].astype(int) > 20) & (a[:,0].astype(int) - a[:,2].astype(int) > 20)
            if mask_red.sum() > 0:
                sel = a[mask_red]
                return tuple(map(int, np.median(sel, axis=0)))
        except Exception:
            pass
        # If simple channel masks didn't find a blob, try density-based small-cluster searchtry:
            h_crop = y1 - y0w_crop = x1 - x0a2 = arr.reshape(h_crop, w_crop, 3)
            # mask for slight green bias and reasonably bright green channelmask = (a2[:,:,1].astype(int) - a2[:,:,0].astype(int) > 4) & (a2[:,:,1].astype(int) - a2[:,:,2].astype(int) > 4) & (a2[:,:,1] > 90)
            coords = np.argwhere(mask)
            if coords.size:
                # for each coord, count neighbors within a small window and pick densestbest_count = 0best_cent = Nonebest_count_near = 0best_cent_near = None
                # prefer clusters near the token vertical line (cy)
                cy_rel = cy - y0for yx in coords:
                    y0c, x0c = yxdy = np.abs(coords[:,0] - y0c)
                    dx = np.abs(coords[:,1] - x0c)
                    cnt = np.sum((dy <= 4) & (dx <= 4))
                    if cnt > best_count:
                        best_count = cntbest_cent = (y0c, x0c)
                    # track best cluster that is near the token lineif abs(y0c - cy_rel) <= 12 and cnt > best_count_near:
                        best_count_near = cntbest_cent_near = (y0c, x0c)
                chosen = Noneif best_cent_near is not None and best_count_near >= 4:
                    chosen = best_cent_nearelif best_cent is not None and best_count >= 6:
                    chosen = best_centif chosen is not None:
                    yc, xc = chosenylo = max(0, yc-4); yhi = min(h2, yc+5)
                    xlo = max(0, xc-4); xhi = min(w2, xc+5)
                    sub = a2[ylo:yhi, xlo:xhi, :].reshape(-1,3)
                    med = tuple(map(int, np.median(sub, axis=0)))
                    return medexcept Exception:
            pass
        # try canonical detectioncan = _detect_canonical_in_pixels(arr)
        if can is not None:
            return can
        # otherwise use the band median logicreturn _sample_left_band_median_from_png(png_path)
    except Exception:
        return None


def _sample_left_band_median_from_png(png_path):
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizeband = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
        arr = np.array(band).reshape(-1,3)
        # first, try to detect canonical tint in the bandcan = _detect_canonical_in_pixels(arr)
        if can is not None:
            return cannonwhite_mask = (arr.sum(axis=1) < 740)
        if nonwhite_mask.sum():
            sel = arr[nonwhite_mask]
            # compute a composite score that prefers saturation and closeness to canonical colorsarr_f = sel.astype('float32')/255.0mx = arr_f.max(axis=1)
            mn = arr_f.min(axis=1)
            sat = (mx - mn) / np.where(mx == 0, 1e-6, mx)
            val = mxmask = (sat > 0.06) & (val > 0.05)
            if mask.sum() > 8:
                sel = arr[mask]
                med = tuple(map(int, np.median(sel, axis=0)))
                return med
            # fallback: median of non-white pixelsnonwhite = arr[(arr.sum(axis=1) < 740)]
            if nonwhite.size:
                med = tuple(map(int, np.median(nonwhite, axis=0)))
                return medreturn None, Noneelse:
            small = img.crop((max(0,int(w*0.05)), int(h*0.2), int(w*0.25), int(h*0.6)))
            arr2 = np.array(small).reshape(-1,3)
            # try canonical detection on small cropcan2 = _detect_canonical_in_pixels(arr2)
            if can2 is not None:
                return can2med2 = tuple(map(int, np.median(arr2, axis=0)))
            return med2except Exception:
        return None


def _sample_left_cluster_full(png_path, target_y=None):
    """Search the full left strip (0..25% width) for the densest green/red cluster.
    If target_y provided, prefer clusters near that vertical position."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizestrip = np.array(img.crop((0, 0, int(w*0.25), h))).astype('int')
        h2,w2,_ = strip.shape
        # mask for slight green bias (allow low saturation)
        mask = (strip[:,:,1].astype(int) - strip[:,:,0].astype(int) > 6) & (strip[:,:,1].astype(int) - strip[:,:,2].astype(int) > 6)
        coords = np.argwhere(mask.reshape(h2,w2))
        if coords.size == 0:
            return None
        # prefer clusters near target_y (if provided), where target_y is page coordinateif target_y is not None:
            # convert target_y (page coord) into band-relative coordband_y0 = int(h*0.15)
            cy_rel = target_y - band_y0else:
            cy_rel = Nonebest_count = 0best_cent = Nonebest_count_near = 0best_cent_near = Nonefor i,(y,x) in enumerate(coords):
            if used[i]:
                continue
            # grow a local clusterdy = np.abs(coords[:,0] - y); dx = np.abs(coords[:,1] - x)
            sel = (dy <= 5) & (dx <= 5)
            idxs = np.where(sel & (~used))[0]
            if idxs.size == 0:
                continueused[idxs] = Truepts = coords[idxs]
            cy = int(np.median(pts[:,0]))
            cx = int(np.median(pts[:,1]))
            sub = strip[cy-4:cy+5, max(0,cx-4):min(w2,cx+5), :].reshape(-1,3)
            med = tuple(map(int, np.median(sub, axis=0)))
            clusters.append((cy, cx, med, idxs.size))
        # sort by vertical positionclusters.sort(key=lambda t: t[0])
        return clustersexcept Exception:
        return None


def _find_canonical_near_line(png_path, tokens, best_idx, expected_color):
    """Search left-of-line crop and full left strip for the pixel nearest the expected canonical color.
    Returns an RGB tuple or None."""
    try:
        if best_idx is None:
            return Noneimg = Image.open(str(png_path)).convert('RGB')
        ref = tokens[best_idx]
        top_ref = int(ref.get('top', 0))
        near = [t for t in tokens if abs(int(t.get('top',0)) - top_ref) <= 8]
        if not near:
            # fallback to center bandcy = top_ref + int(near[0].get('height',0)) // 2x0 = 0x1 = int(png.size[0] * 0.25)
            y0 = max(0, cy - 12)
            y1 = min(png.size[1], cy + 12)
        else:
            min_left = min(int(t.get('left',0)) for t in near)
            cy = top_ref + int(near[0].get('height',0)) // 2x0 = 0
            # widen x search to capture markers that are slightly further left/right of tokensx1 = min(png.size[0], min_left + 80)
            # widen y band to be more tolerant of vertical offsetsy0 = max(0, cy - 20)
            y1 = min(png.size[1], cy + 20)
        crop = img.crop((x0,y0,x1,y1)).convert('RGB')
        arr = np.array(crop).reshape(-1,3)
        if arr.size == 0:
            return None
        # compute distance to the expected canonicalcanon = CANONICAL.get(expected_color)
        if not canon:
            return Nonedists = np.sum((arr - np.array(canon))**2, axis=1)
        best_idx = np.argsort(dists)[:200]
        coords = [divmod(int(i), png.size[0]) for i in best_idx]
        # map coords back to original image coordinatesbest_choice = Nonebest_dy = Nonefor yy, xx in coords:
            orig_x = int(xx * 4 + 4 // 2)
            orig_y = int(yy * 4 + 4 // 2)
            dy = abs(orig_y - top_ref) if top_ref is not None else 0if best_dy is None or dy < best_dy:
                best_dy = dybest_choice = (orig_x, orig_y)
        if best_choice is None:
            return Nonepx = img.getpixel(best_choice)
        # accept only if reasonably close to canonicalif np.sum((np.array(px) - canon)**2) < 250000:
            return tuple(px)
        return Noneexcept Exception:
        return None


def _aggressive_green_search(png_path, tokens, best_idx):
    """Aggressively search a narrow left-of-line crop for pale green clusters.
    Returns median RGB tuple if a cluster is found, else None."""
    try:
        if best_idx is None:
            return Noneimg = Image.open(str(png_path)).convert('RGB')
        ref = tokens[best_idx]
        top_ref = int(ref.get('top', 0))
        near = [t for t in tokens if abs(int(t.get('top',0)) - top_ref) <= 8]
        if not near:
            return Nonemin_left = min(int(t.get('left',0)) for t in near)
        cy = top_ref + int(near[0].get('height',0)) // 2x0 = max(0, min_left - 80)
        # widen x1 to look slightly to the right of the left token edge as wellx1 = min(img.size[0], min_left + 36)
        y0 = max(0, cy - 22)
        y1 = min(img.size[1], cy + 22)
        crop = np.array(img.crop((x0,y0,x1,y1)).convert('RGB'))
        if crop.size == 0:
            return Noneh2,w2,_ = crop.shapea = crop.reshape(-1,3)
        # mask for green bias (allow low saturation)
        mask = (a[:,1].astype(int) - a[:,0].astype(int) > 6) & (a[:,1].astype(int) - a[:,2].astype(int) > 6)
        if mask.sum() == 0:
            return None
        # find densest cluster among mask pointscoords = np.argwhere(mask.reshape(h2,w2))
        best_count = 0best_cent = Nonebest_count_near = 0best_cent_near = None
        # prefer clusters near the token vertical line (cy)
        cy_rel = cy - y0for yx in coords:
            y0c, x0c = yxdy = np.abs(coords[:,0] - y0c)
            dx = np.abs(coords[:,1] - x0c)
            cnt = np.sum((dy <= 3) & (dx <= 3))
            if cnt > best_count:
                best_count = cntbest_cent = (y0c, x0c)
            # track best cluster that is near the token lineif abs(y0c - cy_rel) <= 12 and cnt > best_count_near:
                best_count_near = cntbest_cent_near = (y0c, x0c)
        chosen = None
        # relax cluster size thresholds so small markers are recognizedif best_cent_near is not None and best_count_near >= 4:
            chosen = best_cent_nearelif best_cent is not None and best_count >= 4:
            chosen = best_centif chosen is not None:
            yc, xc = chosenylo = max(0, yc-4); yhi = min(h2, yc+5)
            xlo = max(0, xc-4); xhi = min(w2, xc+5)
            sub = crop[ylo:yhi, xlo:xhi, :].reshape(-1,3)
            med = tuple(map(int, np.median(sub, axis=0)))
            return medreturn Noneexcept Exception:
        return None


def _global_canonical_search(png_path, token_cy, expected_color, downscale=4, max_candidates=200):
    """Search the whole page for pixels nearest the expected canonical color, then pickthe candidate whose vertical position is closest to token_cy. Returns RGB tuple or None."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.size
        # downscale for speedw2 = max(1, w // downscale)
        h2 = max(1, h // downscale)
        small = img.resize((w2, h2), resample=Image.BILINEAR)
        arr = np.array(small).reshape(-1,3)
        canon = np.array(CANONICAL.get(expected_color))
        if canon is None:
            return Nonedists = np.sum((arr - canon)**2, axis=1)
        best_idx = np.argsort(dists)[:max_candidates]
        coords = [divmod(int(i), w2) for i in best_idx]
        # map coords back to original image coordinatesbest_choice = Nonebest_dy = Nonefor yy, xx in coords:
            orig_x = int(xx * downscale + downscale // 2)
            orig_y = int(yy * downscale + downscale // 2)
            dy = abs(orig_y - token_cy) if token_cy is not None else 0if best_dy is None or dy < best_dy:
                best_dy = dybest_choice = (orig_x, orig_y)
        if best_choice is None:
            return Nonepx = img.getpixel(best_choice)
        # accept only if reasonably close to canonicalif np.sum((np.array(px) - canon)**2) < 250000:
            return tuple(px)
        return Noneexcept Exception:
        return None


def _find_color_clusters_in_left_strip(png_path, color='green'):
    """Scan the left 25% strip for small color-biased clusters and return list of
    (cy, cx, median_rgb, count) sorted by vertical position."""
    try:
        img = Image.open(str(png_path)).convert('RGB')
        w,h = img.sizestrip = np.array(img.crop((0, 0, int(w*0.25), h))).astype('int')
        h2,w2,_ = strip.shape
        # mask for slight green bias (allow low saturation)
        mask = (strip[:,:,1].astype(int) - strip[:,:,0].astype(int) > 6) & (strip[:,:,1].astype(int) - strip[:,:,2].astype(int) > 6)
        coords = np.argwhere(mask.reshape(h2,w2))
        if coords.size == 0:
            return Noneclusters = []
        used = np.zeros(coords.shape[0], dtype=bool)
        for i,(y,x) in enumerate(coords):
            if used[i]:
                continue
            # grow a local clusterdy = np.abs(coords[:,0] - y); dx = np.abs(coords[:,1] - x)
            sel = (dy <= 5) & (dx <= 5)
            idxs = np.where(sel & (~used))[0]
            if idxs.size == 0:
                continueused[idxs] = Truepts = coords[idxs]
            cy = int(np.median(pts[:,0]))
            cx = int(np.median(pts[:,1]))
            sub = strip[cy-4:cy+5, max(0,cx-4):min(w2,cx+5), :].reshape(-1,3)
            med = tuple(map(int, np.median(sub, axis=0)))
            clusters.append((cy, cx, med, idxs.size))
        # sort by vertical positionclusters.sort(key=lambda t: t[0])
        return clustersexcept Exception:
        return None


def main():
    import csvrows = []
    # if PyMuPDF is not available, run the lightweight expectation-only QA and exitif not HAS_FITZ:
        print('PyMuPDF not available. Running expectation-only QA and exiting main.')
        run_expectation_only_qa()
        return
    # process only the PDFs already downloaded in PDF_DIR, unless expectation files point to a specific PDFloaded_expect = load_expectations_from_dir(ROOT / 'data' / 'pdf_analysis')
    expected_filenames = set(loaded_expect.keys())
    # discover downloaded PDFspdf_files = sorted(PDF_DIR.glob('*.pdf'))
    explicit_files = []
    for fname in expected_filenames:
        p1 = PDF_DIR / fnameif p1.exists():
            explicit_files.append(p1)
            continuep2 = ROOT / 'data' / 'pdf_analysis' / fnameif p2.exists():
            explicit_files.append(p2)
            continueif explicit_files:
        pdf_files = explicit_fileselse:
        pdf_files = sorted(PDF_DIR.glob('*.pdf'))
    print('Processing PDFs:', [p.name for p in pdf_files])
    EXPECT = loaded_expect


    row_records = []
    wide = {}
    for pdf_path in pdf_files:
        fname = pdf_path.namesrc = str(pdf_path)
        record_id = re.search(r'user_(\d+)_', fname)
        record_id = record_id.group(1) if record_id else fnamedoc = fitz.open(str(pdf_path))
        # find candidate lines across all pages that match known factor keywordsdef find_candidate_lines(doc):
            keywords = [
                'charged off','over limit','unpaid collection','re lates','rev late','avg age open','no 5k','closed rev','open rev depth','inq','inquiry','mortgage','total rev usage','pay','closed','collection','current lates','past due not late','military affiliated','seasoned closed accounts','closed accnts'
            ]
            candidates = []
            for p in range(len(doc)):
                td = doc.load_page(p).get_text('dict')
                for b in td.get('blocks', []):
                    for ln in b.get('lines', []):
                        text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                        if not text: continuelow = text.lower()
                        if any(k in low for k in keywords) or ('$' in text) or re.search(r'\d+\s+re lates', low):
                            candidates.append((p, text, ln.get('spans', [])))
            return candidateslines = find_candidate_lines(doc)
        full_text = '\n'.join([doc.load_page(i).get_text() for i in range(len(doc))])
        rec = extract_record_level(full_text)
        # If installment/revolving counts weren't found in the text, try OCR-based candidate line scantry:
            if (rec.get('installment_open_count') is None or rec.get('installment_open_total') is None) or (rec.get('revolving_open_count') is None or rec.get('revolving_open_total') is None):
                prev_ln = ''
                for p_idx, ln_text, sp in lines:
                    combined = (prev_ln + ' ' + ln_text).strip()
                    cnt, amt = parse_count_amount_pair(combined)
                    if cnt is None and amt is None:
                        cnt, amt = parse_count_amount_pair(ln_text)
                    if (cnt is not None or amt is not None):
                        ln_low = combined.lower()
                        # assign to installment if header contains 'install'
                        if 'install' in ln_low:
                            if rec.get('installment_open_count') is None and cnt is not None:
                                rec['installment_open_count'] = int(cnt)
                            if rec.get('installment_open_total') is None and amt is not None:
                                rec['installment_open_total'] = int(amt)
                            break
                        # assign to revolving if header contains 'revolv' or 'rev'
                        if 'revolv' in ln_low or 'rev' in ln_low:
                            if rec.get('revolving_open_count') is None and cnt is not None:
                                rec['revolving_open_count'] = int(cnt)
                            if rec.get('revolving_open_total') is None and amt is not None:
                                rec['revolving_open_total'] = int(amt)
                            breakprev_ln = ln_textexcept Exception:
            passrec['filename'] = fnamerec['source'] = srcrec['record_id'] = record_idfactor_map = {}
        other_factors = []
        color_counts = {'red':0,'amber':0,'green':0,'black':0,'neutral':0}
        # iterate lines and mapprev_ln = ''
        header_exclude = set(['credit report details','credit summary','name','credit report','report date'])
        # targeted expected phrases (from QA) per filenametargeted = {
            'user_1254_credit_summary_2025-09-01_095528.pdf': [
                '3 Over Limit Acct','Current Lates','Past Due Not Late','4 Chrgd Off Rev Accts','1 Rev Late in 0-3 mo','Avg Age Open','No 5k+ Lines','No Closed Rev Depth','3 RE Lates in 2-4 yrs','1 Rev Late in 2-4 yrs','2 inqs last 2 Months','2 Total Inq 0-2 Mo','Ok Open Rev Depth','3+ Closed Rev Accnts','$440 Unpaid 1 Collection','No Open Mortgage','No Rev Acct Open 10K 2yr','1 inq last 2-4 Mo','1 Total Inq 2-5 Mo'
            ],
            'user_1514_credit_summary_2025-09-01_145557.pdf': [
                '7 Over Limit Accts','Total Rev Usage > 90','Past Due Not Late','Pay $12355 so Accts < 40','8 Chrgd Off Rev Accts','2 Rev Lates in 4-6 mo','2 Rev Lates in 6-12 mo','$3029 Unpaid Collections','4 Rev Lates in 2-4 yrs','1 RE Late in 4-6 mo','No 5k+ Lines','1 Chrgd Off RE Acct','1 Inq Last 4 Mo','Less than 5 yrs','Great Closed Rev Depth','Ok Open Rev Depth','3+ Closed Rev Accts','Military Affiliated','Seasoned Closed Accounts','Closed Accnts Over 5k','8+ Rev Accnts with Balances','In Credit Counseling','No Open Mortgage','1 Rev Late in 4+ yrs','Drop Bad Auth User','No Rev Acct Open 10K 2yr','1 Inq last 2-4 Mo','1 Inq Last 4-5 mo','1 Total Inq 2-4 Mo'
            ]
        }
        for page_idx, ln, sp in lines:
            if 'credit factors' in ln.lower():
                continueif not ln.strip():
                prev_ln = lncontinueln_lower = ln.lower().strip()
            # pair header+value lines, e.g., 'Credit Freeze' followed by 'No'
            if ln_lower in ('no','yes','none') and prev_ln:
                combined = f"{prev_ln} {ln}"
                ln_proc = combinedelse:
                ln_proc = lnprev_ln = ln
            # skip generic header linesif any(h in ln_lower for h in header_exclude):
                # but keep if it contains an explicit value like 'credit freeze: no' or if it's one of the specific fieldsif ln_lower in ('credit freeze','fraud alert','credit score','payments'):
                    passelse:
                    continue
            # try span color first, else OCRcolor_hex, rgb = span_color_hex(sp)
            method = 'span'
            page = doc.load_page(page_idx)
            if not color_hex or map_color_to_cat(rgb) == 'neutral':
                color_hex2, rgb2 = ocr_sample_color(page, ln_proc, IMG_DIR / f"{fname}.p{page_idx}.png")
                if color_hex2 and color_hex2 != color_hex:
                    color_hex, rgb = color_hex2, rgb2method = 'ocr'
            cat = map_color_to_cat(rgb) if rgb else 'neutral'
            # if still neutral, try horizontal band scan to catch faint colored markersif cat == 'neutral':
                try:
                    hex3, rgb3 = None, Nonedef band_scan_color(page, line_text, img_path):
                        pix = page.get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
                        target = ''.join(ch.lower() for ch in line_text if ch.isalnum() or ch.isspace()).strip()
                        best_y = Nonebest_score = 0for i, txt in enumerate(data['text']):
                            t = (txt or '').strip()
                            if not t: continuenorm = ''.join(ch.lower() for ch in t if ch.isalnum() or ch.isspace()).strip()
                            score = len(set(norm.split()) & set(target.split()))
                            if score>best_score:
                                best_score = scorebest_y = data['top'][i]
                        w,h = img.sizeif best_y is None:
                            band = img.crop((0, int(h*0.15), int(w*0.25), int(h*0.85)))
                            arr = np.array(band).reshape(-1,3)
                            nonwhite = arr[(arr.sum(axis=1) < 740)]
                            if nonwhite.size:
                                med = tuple(map(int, np.median(nonwhite, axis=0)))
                                if out_img_path:
                                    band.save(out_img_path)
                                return medelse:
                            cx = int(w * 0.12)
                            cy = int(best_y + 8)
                            med = median_5x5(img, cx, cy)
                            if med:
                                if out_img_path:
                                    small = img.crop((max(0,cx-20), max(0,cy-20), min(w, cx+20), min(h, cy+20)))
                                    small.save(out_img_path)
                                return medhex3, rgb3 = band_scan_color(page, ln_proc, IMG_DIR / f"{fname}.p{page_idx}.band.png")
                    if hex3 and rgb3:
                        color_hex, rgb = hex3, rgb3method = 'band'
                        cat = map_color_to_cat(rgb)
                except Exception:
                    passcolor_counts[cat] = color_counts.get(cat,0) + 1canonical = map_line_to_canonical(ln)
            # If we have an expectation for this filename and this line, try targeted
            # canonical/cluster/global searches to recover pale/anti-aliased markerstry:
                exp_for_file = EXPECT.get(fname, {}) if 'EXPECT' in locals() else {}
                expected_color = Nonefor ep, ec in exp_for_file.items():
                    # match by normalized/canonicalized phraseif canonicalize(ep) == canonicalize(ln) or canonicalize(ep) in canonicalize(ln) or canonicalize(ln) in canonicalize(ep):
                        expected_color = ecbreakif cat == 'neutral' and expected_color:
                    # render page PNG if not alreadytry:
                        page = doc.load_page(page_idx)
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"qa_{fname}.p{page_idx}_{re.sub('[^a-z0-9]','_',ln.lower())[:40]}.png")
                    except Exception:
                        png = _render_page_png(candidate, page_idx, IMG_DIR / f"{fname}.p{page_idx}.png")
                    # tokenizationtokens = _tesseract_tokens(png)
                    # find best token for phrasebest_idx = None; best_score = 0target = ' '.join(ch.lower() for ch in ln if ch.isalnum() or ch.isspace()).strip()
                    for i,t in enumerate(tokens):
                        txt = (t.get('text') or '').strip()
                        if not txt: continuenorm = ' '.join(ch.lower() for ch in txt if ch.isalnum() or ch.isspace()).strip()
                        score = len(set(norm.split()) & set(target.split()))
                        if score > best_score:
                            best_score = score; best_idx = i
                    # try canonical near linetry_can = _find_canonical_near_line(png, tokens, best_idx, expected_color) if expected_color else Noneif try_can is not None:
                        color_hex = '#%02x%02x%02x' % try_canrgb = try_cancat = expected_color if map_color_to_cat(try_can) == expected_color or color_distance(try_can, CANONICAL.get(expected_color)) < 120000 else map_color_to_cat(try_can)
                        method = 'canon_search'
                    # aggressive green cluster & left-strip clustering are marker-oriented; only run if markers enabledif markers_enabled() and cat == 'neutral' and expected_color == 'green':
                        ag = _aggressive_green_search(png, tokens, best_idx)
                        if ag is not None and map_color_to_cat(ag) == 'green':
                            color_hex = '#%02x%02x%02x' % agrgb = agcat = 'green'
                            method = 'aggr_green'
                        # left-strip clustering (green)
                        clusters = _find_color_clusters_in_left_strip(png, color='green')
                        if clusters and best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2best_cluster = min(clusters, key=lambda c: abs(c[0] - token_cy))
                            if abs(best_cluster[0] - token_cy) <= 40 and best_cluster[3] >= 4:
                                color_hex = '#%02x%02x%02x' % best_cluster[2]
                                rgb = best_cluster[2]
                                cat = 'green'
                                method = 'strip_cluster'
                    # final global canonical searchif cat == 'neutral' and expected_color:
                        token_cy = Noneif best_idx is not None:
                            t = tokens[best_idx]
                            token_cy = int(t.get('top',0)) + int(t.get('height',0))//2g = _global_canonical_search(png, token_cy, expected_color)
                        if g is not None:
                            color_hex = '#%02x%02x%02x' % grgb = gcat = map_color_to_cat(g)
                            method = 'global_canon'
            except Exception:
                pass
            # If canonical is not one of our known keys, keep as "other"
            known_keys = set(k for _,k in MAPPING_RULES)
            if canonical not in known_keys:
                other_factors.append((canonical, cat))
            else:
                # store highest-severity color if multiple instances of same factor occur (prefer red>amber>green>black)
                prev = factor_map.get(canonical, 0)
                code = COLOR_CODE.get(cat, 0)
                if code > prev:
                    factor_map[canonical] = code
            # only append row_records for known canonical factors or non-neutral colorsif canonical in known_keys or cat != 'neutral' or '$' in ln or 'inq' in ln.lower():
                row_records.append({
                    'filename': fname,
                    'source': src,
                    'record_id': record_id,
                    'line_text': ln_proc,
                    'canonical': canonical,
                    'color_hex': color_hex or '',
                    'color_cat': cat,
                    'color_method': method,
                })
        # targeted search for expected phrases (patch missing factors called out in QA and expectations)
        targeted_phrases = set(targeted.get(fname, []) if isinstance(targeted, dict) else targeted) if targeted else set()
        # include EXPECT phrases if presenttry:
            exp_phrases = set(EXPECT.get(fname, {}).keys())
        except Exception:
            exp_phrases = set()
        targeted_phrases.update(exp_phrases)
        for phrase in targeted_phrases:
            if phrase.lower() in full_text.lower():
                # find page containing the phrase and sample colorfor p in range(len(doc)):
                    ptext = doc.load_page(p).get_text()
                    if phrase.lower() in ptext.lower():
                        hex3, rgb3 = None, Nonetry:
                            med = band_scan_color_for_phrase(doc, phrase, out_img_path=IMG_DIR / f"patch_{fname}_{re.sub('[^a-z0-9]','_',phrase.lower())[:40]}.png")
                            if med is not None:
                                hex3 = '#%02x%02x%02x' % medrgb3 = medexcept Exception:
                            hex3, rgb3 = None, Nonecat = map_color_to_cat(rgb3) if rgb3 else 'neutral'
                        canonical = map_line_to_canonical(phrase)
                        # ensure we record even if canonical unknown (we'll add to other_factors)
                        if canonical in set(k for _,k in MAPPING_RULES):
                            factor_map[canonical] = max(factor_map.get(canonical,0), COLOR_CODE.get(cat,0))
                        else:
                            other_factors.append((canonical, cat))
                        row_records.append({
                            'filename': fname,
                            'source': src,
                            'record_id': record_id,
                            'line_text': phrase,
                            'canonical': canonical,
                            'color_hex': hex3 or '',
                            'color_cat': cat,
                            'color_method': 'patch',
                        })
                        break
        # sample record-level colors for important fields (credit score, payments, etc.) when availabledef sample_label_color(doc, label_regex_list):
            # look for any label in the document and return sampled RGB using band_scan_color_for_phrasefor label in label_regex_list:
                if re.search(label, full_text, re.I):
                    med = band_scan_color_for_phrase(doc, label, out_img_path=IMG_DIR / f"label_{fname}_{re.sub('[^a-z0-9]','_',label)[:40]}.png")
                    if med:
                        return medreturn None

        # compute wide rowwide_row = {'filename': fname, 'source': src, 'record_id': record_id}
        # default factor columns (only known canonical keys)
        for k,v in factor_map.items():
            wide_row[k] = vwide_row['other_factors'] = ';'.join([f"{k}:{c}" for k,c in other_factors[:80]])
        # counts per colorwide_row['count_red'] = color_counts.get('red',0)
        wide_row['count_amber'] = color_counts.get('amber',0)
        wide_row['count_green'] = color_counts.get('green',0)
        wide_row['count_black'] = color_counts.get('black',0)
        # record-level numeric fieldswide_row['credit_score'] = rec.get('credit_score','')
        wide_row['age'] = rec.get('age','')
        # attempt to sample credit score colorcs_med = sample_label_color(doc, ['Credit Score', r'\b\d{3}\b'])
        if cs_med:
            wide_row['credit_score_color_hex'] = rgb_to_hex_tuple(cs_med)
            wide_row['credit_score_color_cat'] = map_color_to_cat(cs_med)
        else:
            wide_row['credit_score_color_hex'] = ''
            wide_row['credit_score_color_cat'] = ''
        # payment samplingpay_med = sample_label_color(doc, ['Payments?\b', 'Payment\b'])
        if pay_med:
            wide_row['payment_color_hex'] = rgb_to_hex_tuple(pay_med)
            wide_row['payment_color_cat'] = map_color_to_cat(pay_med)
        else:
            wide_row['payment_color_hex'] = ''
            wide_row['payment_color_cat'] = ''
        # record-level numeric fields parsed earlierwide_row['monthly_payment'] = rec.get('monthly_payment','')
        wide_row['credit_freeze'] = rec.get('credit_freeze',0)
        wide_row['fraud_alert'] = rec.get('fraud_alert',0)
        wide_row['deceased'] = rec.get('deceased',0)
        wide_row['address'] = rec.get('address','')
        wide_row['revolving_open_count'] = rec.get('revolving_open_count','')
        wide_row['revolving_open_total'] = rec.get('revolving_open_total','')
        wide_row['installment_open_count'] = rec.get('installment_open_count','')
        wide_row['installment_open_total'] = rec.get('installment_open_total','')
        wide_row['collections_open'] = rec.get('collections_open','')
        wide_row['collections_closed'] = rec.get('collections_closed','')
        wide_row['inquiries_6mo'] = rec.get('inquiries_6mo','')
        wide_row['late_pays_recent'] = rec.get('late_pays_recent','')
        wide_row['late_pays_prior'] = rec.get('late_pays_prior','')        # extract open/closed counts heuristicsm_open_closed = re.search(r"(\d+) open.*?(\d+) closed", full_text, re.I|re.S)
        if m_open_closed:
            wide_row['num_open'] = int(m_open_closed.group(1))
            wide_row['num_closed'] = int(m_open_closed.group(2))
        # quick collection countswide_row['has_collections'] = 1 if 'collection' in full_text.lower() else 0
# extract open/closed collection countsm_open_col = re.search(r"(\d+) open.*collection", full_text, re.I)
    if m_open_col:
        wide_row['num_open_collections'] = int(m_open_col.group(1))
    else:
        wide_row['num_open_collections'] = len(re.findall(r'active collection|open collection', full_text, re.I))
    m_closed_col = re.search(r"(\d+) closed.*collection", full_text, re.I)
    if m_closed_col:
        wide_row['num_closed_collections'] = int(m_closed_col.group(1))
    else:
        # heuristic: count occurrences of 'unpaid collection' or 'paid collection'
        wide_row['num_closed_collections'] = len(re.findall(r'unpaid collection|paid collection|collection', full_text, re.I))
    wide_row['num_collections'] = wide_row['num_open_collections'] + wide_row['num_closed_collections']
    wide_row['has_collections'] = 1 if wide_row['num_collections']>0 else 0

    wide_row['rev_balance_total'] = Nonewide_row['installment_balance_total'] = Nonewide_row['rev_limit_total'] = Nonewide_row['monthly_income'] = Nonewide_row['monthly_payment'] = None
    # try to parse balances and incomesm_rev = re.search(r"rev.*?balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_rev:
        wide_row['rev_balance_total'] = float(m_rev.group(1).replace(',',''))
    m_inst = re.search(r"installment.*balance.*?\$?([0-9,]+)", full_text, re.I)
    if m_inst:
        wide_row['installment_balance_total'] = float(m_inst.group(1).replace(',',''))
    m_lim = re.search(r"limit.*?\$?([0-9,]+)", full_text, re.I)
    if m_lim:
        wide_row['rev_limit_total'] = float(m_lim.group(1).replace(',',''))
    # try to parse monthly income / monthly paymentsm_income = re.search(r"\$(\d{1,3}(?:,\d{3})*)(?:\s*/\s*mo|/mo| per month)", full_text, re.I)
    if m_income:
        wide_row['monthly_income'] = float(m_income.group(1).replace(',',''))
    # monthly payment found as 'Pay $1234/mo' or 'Payments $x/mo'
    m_pay = re.search(r"pay\s*\$?([0-9,]+)\s*/\s*mo|payments\s*\$?([0-9,]+)\s*/\s*mo", full_text, re.I)
    if m_pay:
        num = m_pay.group(1) or m_pay.group(2)
        if num:
            wide_row['monthly_payment'] = float(num.replace(',',''))
    # compute derived ratiosif wide_row['rev_balance_total'] and wide_row.get('rev_limit_total'):
        try:
            wide_row['rev_utilization'] = wide_row['rev_balance_total'] / wide_row['rev_limit_total']
        except Exception:
            wide_row['rev_utilization'] = Noneelse:
        wide_row['rev_utilization'] = Noneif wide_row['installment_balance_total'] and wide_row.get('monthly_income'):
        try:
            wide_row['installment_balance_to_income'] = wide_row['installment_balance_total'] / (wide_row['monthly_income'] * 12)
        except Exception:
            wide_row['installment_balance_to_income'] = Noneelse:
        wide_row['installment_balance_to_income'] = Noneif wide_row.get('monthly_payment') and wide_row.get('monthly_income'):
        try:
            wide_row['monthly_payment_to_income'] = wide_row['monthly_payment'] / wide_row['monthly_income']
        except Exception:
            wide_row['monthly_payment_to_income'] = Noneelse:
        wide_row['monthly_payment_to_income'] = None
    # public records: parse explicit counts and capture notes (e.g., bankruptcy)
    pr_count, pr_note = parse_public_records(full_text)
    wide_row['public_records'] = pr_countwide_row['public_record_note'] = pr_note
    # num_90d_plus_latesm_90 = re.findall(r'(\d+)\s*\b90d\b|90\+\s*d', full_text, re.I)
    wide_row['num_90d_plus_lates'] = sum(int(x) for x in re.findall(r'(\d+)\s*90\+|90d', full_text, re.I) if x.isdigit()) if re.findall(r'(\d+)\s*90', full_text, re.I) else 0wide_row['num_90d_plus_lates'] = wide_row.get('num_90d_plus_lates', 0) + (1 if '90d' in full_text and 'late' in full_text else 0)
    # other boolean heuristics (derived from counts when possible)
    out['has_public_record'] = 1 if out.get('public_records',0) > 0 or out.get('public_record_note') else 0out['has_collections'] = 1 if (out.get('collections_open',0) + out.get('collections_closed',0)) > 0 else 0
    # if a doc is provided, try to extract table-like numeric fields (installment/revolving) from page text and fall back to OCR if neededtry:
        if doc is not None:
            prev_ln = ''
            for p in range(len(doc)):
                td = doc.load_page(p).get_text('dict')
                for b in td.get('blocks', []):
                    for ln in b.get('lines', []):
                        ln_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                        if not ln_text:
                            continuecombined = (prev_ln + ' ' + ln_text).strip()
                        try:
                            # candidate matches on prev/current/combinedcnt_comb, amt_comb = parse_count_amount_pair(combined)
                            cnt_line, amt_line = parse_count_amount_pair(ln_text)
                            cnt_prev, amt_prev = parse_count_amount_pair(prev_ln)

                            def is_install(s):
                                return 'install' in (s or '').lower()
                            def is_revolv(s):
                                s = (s or '').lower()
                                return 'revolv' in s or 'rev' in s or 'revolving' in s or 'revolving' in sdef is_loc(s):
                                s = (s or '').lower()
                                return 'line of credit' in s or 'line of' in s

                            assigned = False
                            # Prefer numeric directly on the line and attach to previous label (common pattern)
                            if cnt_line is not None or amt_line is not None:
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                        out['installment_open_count'] = int(cnt_line)
                                    if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                        out['installment_open_total'] = int(amt_line)
                                    assigned = Trueelif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                        out['revolving_open_count'] = int(cnt_line)
                                    if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                        out['revolving_open_total'] = int(amt_line)
                                    assigned = True
                                # if not attached to prev, attach to current if its label matchesif not assigned:
                                    if is_install(ln_text):
                                        if out.get('installment_open_count') in (None,'') and cnt_line is not None:
                                            out['installment_open_count'] = int(cnt_line)
                                        if out.get('installment_open_total') in (None,'') and amt_line is not None:
                                            out['installment_open_total'] = int(amt_line)
                                        assigned = Trueelif is_revolv(ln_text) or is_loc(ln_text):
                                        if out.get('revolving_open_count') in (None,'') and cnt_line is not None:
                                            out['revolving_open_count'] = int(cnt_line)
                                        if out.get('revolving_open_total') in (None,'') and amt_line is not None:
                                            out['revolving_open_total'] = int(amt_line)
                                        assigned = True
                            # else consider prev numeric or combinedif (not assigned) and (cnt_prev is not None or amt_prev is not None):
                                if is_install(prev_ln):
                                    if out.get('installment_open_count') in (None,'') and cnt_prev is not None:
                                        out['installment_open_count'] = int(cnt_prev)
                                    if out.get('installment_open_total') in (None,'') and amt_prev is not None:
                                        out['installment_open_total'] = int(amt_prev)
                                elif is_revolv(prev_ln) or is_loc(prev_ln):
                                    if out.get('revolving_open_count') in (None,'') and cnt_prev is not None:
                                        out['revolving_open_count'] = int(cnt_prev)
                                    if out.get('revolving_open_total') in (None,'') and amt_prev is not None:
                                        out['revolving_open_total'] = int(amt_prev)
                                assigned = True
                            # fallback: combined detection but prefer prev label if both have labelsif (not assigned) and (cnt_comb is not None or amt_comb is not None):
                                if is_install(prev_ln) or (is_install(ln_text) and not is_revolv(prev_ln) and not is_loc(prev_ln)):
                                    if out.get('installment_open_count') in (None,'') and cnt_comb is not None:
                                        out['installment_open_count'] = int(cnt_comb)
                                    if out.get('installment_open_total') in (None,'') and amt_comb is not None:
                                        out['installment_open_total'] = int(amt_comb)
                                elif is_revolv(prev_ln) or is_loc(prev_ln) or is_revolv(ln_text) or is_loc(ln_text):
                                    if out.get('revolving_open_count') in (None,'') and cnt_comb is not None:
                                        out['revolving_open_count'] = int(cnt_comb)
                                    if out.get('revolving_open_total') in (None,'') and amt_comb is not None:
                                        out['revolving_open_total'] = int(amt_comb)
                        except Exception:
                            pass
                        # extra parsing: detect inquiries, collections, late pays, and payments from nearby table lineslow_ln = ln_text.lower()
                        # Inquiries (Last 6 Months): single number often on its own lineif 'inquiri' in low_ln or 'inquir' in low_ln or 'inquires' in low_ln:
                            # avoid capturing the timeframe 'Last 6 Months' (6) - prefer an adjacent actual countif not re.search(r"last\s*\d+\s*months?", ln_text, re.I):
                                m_in = re.search(r"(\d+)", ln_text)
                                if m_in:
                                    out['inquiries_6mo'] = int(m_in.group(1))
                                    # proceed to next line if this looks like the timeframe instead
                                    # else try previous/next numericelse:
                                # try previous or following numeric; previous may contain a numberm_prev = re.search(r"(\d+)", prev_ln)
                                if m_prev:
                                    out['inquiries_6mo'] = int(m_prev.group(1))
                                else:
                                    # try to find the number on the following line using combined detectionif cnt_line is not None:
                                        out['inquiries_6mo'] = int(cnt_line)
                        # Collections (Open/Closed): '0 / 0' may be on same or next lineif 'collections' in low_ln:
                            m_col = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_col:
                                out['collections_open'] = int(m_col.group(1))
                                out['collections_closed'] = int(m_col.group(2))
                            else:
                                # try next/prev line via combinedif cnt_line is not None and amt_line is not None:
                                    out['collections_open'] = int(cnt_line)
                                    out['collections_closed'] = int(amt_line)
                        # Late Pays (Last 2/2+ Years) -> look for '7 / 0' pair on same or next lineif 'late pays' in low_ln:
                            m_lp = re.search(r"(\d+)\s*/\s*(\d+)", ln_text)
                            if m_lp:
                                out['late_pays_recent'] = int(m_lp.group(1))
                                out['late_pays_prior'] = int(m_lp.group(2))
                            elif cnt_line is not None and amt_line is not None:
                                out['late_pays_recent'] = int(cnt_line)
                                out['late_pays_prior'] = int(amt_line)
                        # Payments: look for dollar amount on same or adjacent linesif 'payments' in low_ln or 'payment' in low_ln:
                            m_pay = re.search(r"\$?\s*([0-9,]+)\s*(?:/mo|mo)?", ln_text, re.I)
                            if m_pay:
                                out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
                            else:
                                # check previous/nextif cnt_line is not None:
                                    out['monthly_payment'] = int(cnt_line)
                                else:
                                    m_prev = re.search(r"\$?\s*([0-9,]+)", prev_ln)
                                    if m_prev:
                                        out['monthly_payment'] = int(m_prev.group(1).replace(',',''))
                        prev_ln = ln_text
                # if still missing, try OCR on rendered pageif (out.get('installment_open_count') in (None,'') or out.get('installment_open_total') in (None,'')) or (out.get('revolving_open_count') in (None,'') or out.get('revolving_open_total') in (None,'')):
                    try:
                        import pytesseractfrom PIL import Imagepix = doc.load_page(p).get_pixmap(matrix=fitz.Matrix(2,2), alpha=False)
                        img = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)
                        ocr_text = pytesseract.image_to_string(img)
                        for oline in ocr_text.splitlines():
                            if not oline.strip():
                                continuecnt, amt = parse_count_amount_pair(oline)
                            if (cnt is not None or amt is not None):
                                ln_low = oline.lower()
                                # assign to installment if header contains 'install'
                                if 'install' in ln_low:
                                    if out.get('installment_open_count') in (None,'') and cnt is not None:
                                        out['installment_open_count'] = int(cnt)
                                    if out.get('installment_open_total') in (None,'') and amt is not None:
                                        out['installment_open_total'] = int(amt)
                                # assign to revolving if header contains 'revolv' or 'rev'
                                if 'revolv' in ln_low or 'rev' in ln_low:
                                    if out.get('revolving_open_count') in (None,'') and cnt is not None:
                                        out['revolving_open_count'] = int(cnt)
                                    if out.get('revolving_open_total') in (None,'') and amt is not None:
                                        out['revolving_open_total'] = int(amt)
                    except Exception:
                        pass
            # finally, populate credit factors using the right-column extractortry:
                out['credit_factors'] = extract_credit_factors_from_doc(doc)
            except Exception:
                out['credit_factors'] = out.get('credit_factors', [])
            # post-page scan heuristics: if we still don't have inquiries or late pays, try page-text searchestry:
                page_text = '\n'.join([doc.load_page(i).get_text() for i in range(len(doc))])
                # Inquiries: find the label and then the numeric value that is NOT part of 'Last 6 Months'
                if out.get('inquiries_6mo') in (None,''):
                    for m in re.finditer(r"Inquir(?:y|ies|es)?", page_text, re.I):
                        # scan the following lines for a line with a standalone number (skip 'Last 6 Months' in parentheses)
                        snippet = page_text[m.end(): m.end()+300]
                        for ln in snippet.splitlines():
                            ln_strip = ln.strip()
                            if not ln_strip:
                                continueif re.search(r"months?|mo", ln_strip, re.I):
                                # skip lines that are just the label with 'Last 6 Months'
                                continuemnum = re.search(r"\b(\d{1,3})\b", ln_strip)
                            if mnum:
                                out['inquiries_6mo'] = int(mnum.group(1))
                                breakif out.get('inquiries_6mo') not in (None,''):
                            break
                # Late Pays: find a numeric 'x / y' following the label
                # prefer pairs on the following line to avoid matching the '(Last 2/2+ Years)' parentheticalfound = Falsefor m in re.finditer(r"Late Pays", page_text, re.I):
                    snippet = page_text[m.end(): m.end()+500]
                    for mp in re.finditer(r"(\d+)\s*/\s*(\d+)", snippet):
                        pair = mp.group(0)
                        pair_pos = snippet.find(pair)
                        # accept only if the pair occurs after a newline (i.e., likely on the next line)
                        if '\n' in snippet[:pair_pos]:
                            out['late_pays_recent'] = int(mp.group(1))
                            out['late_pays_prior'] = int(mp.group(2))
                            found = Truebreakif found:
                        break
                # Credit Card Open Totals (No Retail): inspect account table blocks and consider their final 'total' linetry:
                    cc_found = Nonefor p in range(len(doc)):
                        td = doc.load_page(p).get_text('dict')
                        blocks = td.get('blocks', [])
                        for bi, b in enumerate(blocks):
                            block_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b.get('lines', [])])
                            low_block = block_text.lower()
                            # Heuristic: block that contains 'revolving accounts' or 'revolving accounts (open)' likely precedes credit card totalsif 'revolving accounts' not in low_block and 'revolving accounts (open)' not in low_block:
                                continue
                            # scan following blocks for a block containing 'total' (e.g., 'Credit Card Open Totals')
                            total_block = Nonefor j in range(bi+1, min(bi+30, len(blocks))):
                                b2 = blocks[j]
                                b2_text = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in b2.get('lines', [])])
                                if 'total' in b2_text.lower() or 'credit card open totals' in b2_text.lower():
                                    total_block = b2breakif not total_block:
                                continue
                            # get the last line that contains 'total' or the first line of total_blocktotal_lines = [ln for ln in total_block.get('lines', []) if 'total' in ''.join([s.get('text','') for s in ln.get('spans', [])]).lower()]
                            if total_lines:
                                ln = total_lines[-1]
                            else:
                                # fallback to first meaningful lineln = total_block.get('lines', [])[0]
                            line_text = ''.join([s.get('text','') for s in ln.get('spans', [])]).strip()
                            block_text_full = '\n'.join([''.join([s.get('text','') for s in l.get('spans', [])]) for l in total_block.get('lines', [])])
                            # try to parse numeric totals from the block lines (balance, limit+util, payment)
                            bal = lim = util = pay = None
                            # look for dollar amounts and percent in block lineslines = [lt.strip() for lt in block_text_full.splitlines() if lt.strip()]
                            # simple heuristics based on observed layoutamounts = []
                            perc = Nonefor ln_txt in lines:
                                m_amt = re.search(r"\$([0-9,]+)", ln_txt)
                                if m_amt:
                                    amounts.append(int(m_amt.group(1).replace(',', '')))
                                m_perc = re.search(r"([0-9]{1,3})%", ln_txt)
                                if m_perc:
                                    perc = int(m_perc.group(1))
                            if amounts:
                                bal = amounts[0]
                                if len(amounts) >= 2:
                                    lim = amounts[1]
                                if len(amounts) >= 3:
                                    pay = amounts[2]
                                util = perc
                            # determine color: prefer span color of the label line then convolution fallbackhexv, rgb = span_color_hex(ln.get('spans', []))
                            cat = Noneif rgb:
                                cat = map_color_to_cat(rgb)
                            else:
                                try:
                                    clf = classify_snippet_color_convolution(doc.load_page(p), ln.get('bbox'))
                                    cat = clf.get('cat')
                                except Exception:
                                    cat = None
                            # include only if redif cat == 'red':
                                cc_found = {'color': 'red', 'balance': bal, 'limit': lim, 'utilization_percent': util, 'payment': pay}
                                breakif cc_found:
                            breakif cc_found:
                        out['credit_card_open_totals_no_retail'] = cc_foundexcept Exception:
                    pass
                # if not found, do not fallback to parenthetical pair (avoid capturing '2/2' from label)
                # monthly payment: fallback to first $NNN/mo on the page if not foundif out.get('monthly_payment') in (None,''):
                    m_pay = re.search(r"\$\s*([0-9,]{1,7})\s*/\s*mo", page_text, re.I)
                    if m_pay:
                        out['monthly_payment'] = int(m_pay.group(1).replace(',',''))
            except Exception:
                pass
    # ensure numeric fields are present as empty values if not foundout['installment_open_count'] = out.get('installment_open_count', '')
    out['installment_open_total'] = out.get('installment_open_total', '')
    out['revolving_open_count'] = out.get('revolving_open_count', '')
    out['revolving_open_total'] = out.get('revolving_open_total', '')
    out['inquiries_6mo'] = out.get('inquiries_6mo', '')
    out['late_pays_recent'] = out.get('late_pays_recent', '')
    out['late_pays_prior'] = out.get('late_pays_prior', '')
    out['monthly_payment'] = out.get('monthly_payment', '')
    return out


def run_expectation_only_qa():
    """Module-level fallback QA that uses pdftotext/pdftocairo to verify expected phrases and sample colors."""
    qa_rows = []
    qa_amb = []
    qa_summary = []
    loaded_expect_inner = load_expectations_from_dir
