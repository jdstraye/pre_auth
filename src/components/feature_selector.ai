@debug_pipeline_step("FeatureSelector_Transform")
def transform(self, X: Union[pd.DataFrame, np.ndarray, pd.Series, spmatrix]) -> Union[pd.DataFrame, pd.Series, np.ndarray, spmatrix]:
    """
    Reduce X to the selected features using the fitted selector.
    Applies the feature selection determined during fit().

    While fit() sets which features to keep, transform() applies that selection. 
    Therefore, it uses the attributes set during fit():
    SK/IMBLearn required:
    - feature_names_in_
    - n_features_in_
    - support_
    Private:
    - _feature_names_in
    - _n_features_in
    - _support
    - _selected_features 
    to generate a new X with only the selected features.

    Since pd.DataFrame format has more information than a NumPy array, this method
    preserves the DataFrame structure, including column names and index.

    Parameters
    ----------
    X : {array, sparse matrix} of shape (n_samples, n_features_in_)
        Data to transform. Must have the same number of features as seen during fit().
    
    Returns
    -------
    X_transformed : {DataFrame, Series, array, sparse matrix}
        Transformed data with only selected features. Type matches input type:
        - DataFrame input → DataFrame output (with selected column names)
        - Array input → Array output
        - Sparse input → Sparse output (avoids densification)
        - Series input → Series output (if single feature selected)
    
    Raises
    ------
    NotFittedError
        If transform is called before fit.
    ValueError
        If X has wrong number of features.
        If transformed output doesn't match expected shape.
    """
    lg = self._get_logger()
    if __debug__:
        dict_before = {k: id(v) for k, v in self.__dict__.items()}
        lg.debug(f"DBGTransform04a: {dict_before = }")

    # Check if fitting set everything necessary. sklearn's check_is_fitted performs 
    # other tests as well on the quality of the fit.
    self._check_fit_quality()

    X_array: Union[np.ndarray, spmatrix] = np.array([])  # Placeholder for validated X
    X_selected: Union[np.ndarray, spmatrix] = np.array([])  # Placeholder for selected X
    X_df: Optional[pd.DataFrame] = None  # Placeholder for DataFrame version of input X
    X_df_selected: Optional[pd.DataFrame] = None  # Placeholder for DataFrame version of output X_selected

    # For sparse matrices, avoid densification
    if issparse(X):
        # Validate sparse matrix
        X_array = check_array(X, accept_sparse=True, ensure_2d=True)

        # Validate feature count
        if X_array.shape[1] != self.n_features_in_:
            raise ValueError(
                f"Sparse matrix has {X_array.shape[1] = } features, but FeatureSelector "
                f"expected {self.n_features_in_ = } features"
            )

        # Apply selection
        X_selected = self._transform(X_array)

    else:
        # Input Validation
        X_array, _ = self._validate_and_record_input(
            X, 
            None, 
            require_y = False,
            record_metadata=False,
            feature_names=None
        )

        # Per SKLearn compliance, immediately return empty input
        if isinstance(X, pd.DataFrame) and (X.empty or X.shape[1] == 0):
            return X
        elif isinstance(X_array, np.ndarray) and (X_array.size == 0 or X_array.shape[1] == 0):
            return self._generate_output(X_array)
        elif isinstance(X, spmatrix) and (X.shape[0] == 0 or X.shape[1] == 0):
            return self._generate_output(X)

        # Apply Selection
        lg.debug(f"DBGTransform05: {self._support = }, {X_array.shape = }")
        X_selected = self._transform(X_array)

    # Determine the index to use for validation DFs. This handles the case where the number of samples in transform differs
    # from fit (common in sklearn compliance tests like check_methods_subset_invariance, where full data is transformed, then
    # single-sample batches are transformed separately). If lengths match, reuse the original index (preserves custom indices
    # if any). If not, create a fresh RangeIndex matching the current X rows to avoid shape mismatches in pd.DataFrame creation.
    validation_index = pd.RangeIndex(start=0, stop=X_array.shape[0], step=1)
    if hasattr(self, '_X_index') and self._X_index is not None:
        if len(self._X_index) == X_array.shape[0]:
            validation_index = self._X_index

    # Create input DF for validation (using full columns from fit)
    if hasattr(self, 'feature_names_in_') and self.feature_names_in_ is not None:
        X_df = pd.DataFrame(
            data=X_array,
            columns=self.feature_names_in_,
            index=validation_index
        )
        # Optionally restore dtypes if available
        if hasattr(self, '_X_dtypes') and self._X_dtypes is not None:
            for col in X_df.columns:
                if col in self._X_dtypes:
                    try:
                        X_df[col] = X_df[col].astype(self._X_dtypes[col])
                    except Exception as e:
                        lg.error(f"Error applying dtype to input column {col}: {e}")
        self._validator.validate_frame(X_df, "feature_selector_transform_X_input")
    else:
        lg.debug("Skipping input validation DF: feature_names_in_ not available")

    # Create output DF for validation (using selected columns and selected data)
    if self._selected_features is not None:
        X_df_selected = pd.DataFrame(
            data=X_selected,
            columns=self._selected_features,
            index=validation_index
        )
        # Optionally restore dtypes for selected columns
        if hasattr(self, '_X_dtypes') and self._X_dtypes is not None:
            for col in X_df_selected.columns:
                if col in self._X_dtypes:
                    try:
                        X_df_selected[col] = X_df_selected[col].astype(self._X_dtypes[col])
                    except Exception as e:
                        lg.error(f"Error applying dtype to output column {col}: {e}")
        self._validator.validate_frame(X_df_selected, "feature_selector_transform_X_output")
        if X_df is not None:
            # Compare should verify that the selected columns in input match the output (no corruption)
            self._validator.compare_frames(X_df, X_df_selected, "feature_selector_transform_X_in_v_out")
    else:
        lg.debug("Skipping output validation DF: _selected_features not available")

    dict_after = {k: id(v) for k, v in self.__dict__.items()}
    changed = {k: (dict_before[k], dict_after[k])
               for k in dict_before
               if k in dict_after and dict_before[k] != dict_after[k]}
    if changed:
        lg.error(f"DBGTransform04b: FeatureSelector mutated attributes in transform: {changed}")

    # Generate output in the original input format
    X_res = self._generate_output(X_selected)

    lg.debug(f"Feature selection: {X_array.shape if hasattr(X_array, 'shape') else 'sparse'} -> {X_res.shape if hasattr(X_res, 'shape') else 'sparse'}")

    return X_res